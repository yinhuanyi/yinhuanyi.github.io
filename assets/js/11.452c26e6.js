(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{1223:function(s,n,a){s.exports=a.p+"assets/img/2019-06-222.44.18.af6d5137.png"},1224:function(s,n,a){s.exports=a.p+"assets/img/2019-06-222.52.14.e6d4bf71.png"},1225:function(s,n,a){s.exports=a.p+"assets/img/des.dcf6b2ca.png"},1226:function(s,n,a){s.exports=a.p+"assets/img/sd.0f5f9041.png"},1227:function(s,n,a){s.exports=a.p+"assets/img/er.c93beb60.png"},1228:function(s,n,a){s.exports=a.p+"assets/img/2019-06-225.57.59.911cd3ba.png"},1229:function(s,n,a){s.exports=a.p+"assets/img/2019-06-226.24.13.9c603ca1.png"},1230:function(s,n,a){s.exports=a.p+"assets/img/2019-06-226.31.29.2bb90c22.png"},1231:function(s,n){s.exports="data:image/png;base64,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"},1232:function(s,n,a){s.exports=a.p+"assets/img/has.1cb08fce.png"},1233:function(s,n,a){s.exports=a.p+"assets/img/hs12.7a0467b3.png"},1234:function(s,n,a){s.exports=a.p+"assets/img/zz.c8f4dac4.png"},1235:function(s,n,a){s.exports=a.p+"assets/img/as.6898b398.png"},1236:function(s,n,a){s.exports=a.p+"assets/img/sdf.e7dcabba.png"},1237:function(s,n,a){s.exports=a.p+"assets/img/ass.3f6ddbaf.png"},1238:function(s,n,a){s.exports=a.p+"assets/img/asdf.5a10747a.png"},1239:function(s,n,a){s.exports=a.p+"assets/img/aaa.f62f8335.png"},1240:function(s,n,a){s.exports=a.p+"assets/img/sdd.4d5fa1bc.png"},1241:function(s,n,a){s.exports=a.p+"assets/img/2019-06-254.56.56.6f95b167.png"},1242:function(s,n,a){s.exports=a.p+"assets/img/2019-06-255.03.37.1f60fa24.png"},1243:function(s,n,a){s.exports=a.p+"assets/img/2019-06-255.04.03.7a7310a4.png"},1244:function(s,n,a){s.exports=a.p+"assets/img/2019-06-255.06.41.55135767.png"},1245:function(s,n,a){s.exports=a.p+"assets/img/2019-06-259.25.50.04916a2c.png"},2427:function(s,n,a){"use strict";a.r(n);var t=a(9),e=Object(t.a)({},(function(){var s=this,n=s.$createElement,t=s._self._c||n;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("h3",{attrs:{id:"一-梯度下降算法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#一-梯度下降算法"}},[s._v("#")]),s._v(" "),t("code",[s._v("(一)梯度下降算法")])]),s._v(" "),t("ul",[t("li",[s._v("梯度下降算法概念")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("①：它不是一个机器学习算法\n②：是一种基于搜索的最优化方法\n③：作用：最小化一个损失函数\n④：梯度上升法：最大化一个效用函数\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("ul",[t("li",[s._v("定义一个损失函数")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1223),alt:"Alt text"}})]),s._v(" "),t("p",[t("img",{attrs:{src:a(1224),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("那么θ值递降的速度决定于η的大小和dJ的大小，计算公式如下：左边的θ就是每次迭代递降后的新θ值")])]),s._v(" "),t("p",[s._v("$$θ = θ - η * dJ$$")]),s._v(" "),t("h3",{attrs:{id:"二-梯度下降算法解决一元二次方程最小值"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#二-梯度下降算法解决一元二次方程最小值"}},[s._v("#")]),s._v(" "),t("code",[s._v("(二)梯度下降算法解决一元二次方程最小值")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("import numpy as np \nimport matplotlib.pyplot as plt\n\n# 初始化一个140个等间距元素的向量\nplot_x = np.linspace(-1, 6, 141)\n# 一元二次方程\nplot_y = (plot_x - 2.5)**2 - 1\n\n# 绘图\nplt.plot(plot_x, plot_y)\nplt.show()\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br")])]),t("p",[t("img",{attrs:{src:a(1225),alt:"Alt text"}})]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 对二次方程的theta求导, plot_x就是theta\ndef dJ(theta):\n    return 2*(theta - 2.5)\n\n# 损失函数值\ndef J(theta):\n    return (theta-2.5)**2 -1\n\n# 设置theta, eta, epsilon初始值\ntheta = 0\neta=0.1\nepsilon = 1e-8\n\nwhile True:\n    # 获取斜率\n    gradient = dJ(theta)\n    # 保留之前一次的theta值\n    last_theta = theta\n    # 获取新的theta值\n    theta = theta - eta*gradient\n    # 这是循环退出的条件，比较最近两次损失函数的值小于给定的epsilon，就退出循环\n    if abs(J(theta) - J(last_theta)) < epsilon:\n        break\n\nprint(theta)\nprint(J(theta))\n输出结果：\n2.499891109642585\n-0.99999998814289\n# 小结：当theta = 2.499891109642585时候，损失值最小\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br")])]),t("ul",[t("li",[s._v("画图描述")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("theta = 0\neta=0.1\nepsilon = 1e-8\n\n# 记录theta的取值过程\ntheta_history = [theta]\n\nwhile True:\n    # 获取斜率\n    gradient = dJ(theta)  # -5\n    # 保留之前一次的theta值\n    last_theta = theta  # 0\n    # 获取新的theta值\n    theta = theta - eta*gradient # 0.5 gradient是负数，减去一个负数得正数\n    theta_history.append(theta)\n    \n    # 这是循环退出的条件，比较最近两次损失函数的值小于给定的epsilon，就退出循环\n    if abs(J(theta) - J(last_theta)) < epsilon:\n        break\n\n# 看看theta_history每次的取值走向图\n\n# 先画出原一元二次方程图\nplt.plot(plot_x, J(plot_x), color='g')\n# 再通过theta_history的取值，查看对应的J损失函数的值\nplt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='x')\nplt.show()\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br")])]),t("p",[t("img",{attrs:{src:a(1226),alt:"Alt text"}})]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 查看theta取了多少次\nlen(theta_history)\n输出结果：\n46\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("ul",[t("li",[s._v("将代码进行封装")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 将上面的计算封装为函数\n# 给梯度下降设置梯度下降的次数\ndef gradient_descent(initial_theta, eta, n_iters=1e4,epsilon=1e-8):\n\n    theta = initial_theta\n    theta_history.append(initial_theta)\n    i_iter = 0\n    while i_iter < n_iters:\n        gradient = dJ(theta)\n        last_theta = theta\n        theta = theta - eta*gradient\n        theta_history.append(theta)\n    \n\t    # 当y的差值足够小了就认为是到了极致点，类似于斜率为0的点\n        if abs(J(theta) - J(last_theta)) < epsilon:\n            break\n        \n        i_iter += 1\n        \n        \ndef plot_theta_history():\n    plt.plot(plot_x, J(plot_x), color='g')\n    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='x')\n    plt.show()\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br")])]),t("ul",[t("li",[s._v("修改eta的值")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("eta = 0.01\ninitial_theta = 0\ntheta_history = []\n\ngradient_descent(initial_theta, eta)\nplot_theta_history()\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br")])]),t("p",[t("img",{attrs:{src:a(1227),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("查看theta_history记录的值的长度")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("len(theta_history)\n输出结果：\n424\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("ul",[t("li",[s._v("给损失函数添加异常捕获")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("def J(theta):\n    try:\n        return (theta-2.5) ** 2 -1\n    except Exception as e:\n        return float('inf')\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br")])]),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 即使eta是一个比较大的值，也不会进入死循环， 因为有异常捕获\neta = 1.1\ninitial_theta = 0\ntheta_history = []\n\ngradient_descent(initial_theta, eta)\nlen(theta_history)\n输出结果：\n10001\n\n# 获取theta_history最后一个值, 一定是nan， 这是无穷大-无穷大\ntheta_history[-1]\n输出结果：\nnan\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br")])]),t("h3",{attrs:{id:"二-多元线性回归中使用梯度下降法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#二-多元线性回归中使用梯度下降法"}},[s._v("#")]),s._v(" "),t("code",[s._v("（二）多元线性回归中使用梯度下降法")])]),s._v(" "),t("ul",[t("li",[s._v("问题转换为：")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1228),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[t("p",[s._v("此时 : J(θ) = $\\sum\\limits_{i=1}^m (y^i - θ_0X_0^i$ + $θ_1X_1^i$ + $θ_2X_2^i$ + $θ_3X_3^i$ + ... + $θ_nX_n^i)^2 $")])]),s._v(" "),t("li",[t("p",[t("strong",[s._v("对J(θ)求θ的偏导数")])])])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1229),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("但是这里有个问题，当m的值越大，那么▽J(θ)中元素的值也就越大，这不符合实际需要，因此将损失函数除以m")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(522),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("那么问题再次转换为下面这个式子尽可能小")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1230),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("模拟一组训练数据，让损失函数的值最小（这里的X矩阵和y真值是训练数据, 3是斜率，4为截距， 为了模拟，线性回归方程末尾添加了正态分布噪音）")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(666)\nx = 2 * np.random.random(size=100)\n# 这里加了一个正态分布的噪音\ny = x * 3. + 4. + np.random.normal(size=100)\nX = x.reshape(-1, 1)\nprint(X.shape)\nprint(y.shape)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br")])]),t("ul",[t("li",[s._v("画出散点图")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("plt.scatter(x, y)\nplt.show()\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br")])]),t("p",[t("img",{attrs:{src:a(1231),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("梯度下降核心思想：找到合适的θ向量，使得J(θ)尽可能的小，也就是找到合适的θ向量，使得▽J(θ) = 0，但是斜率等于0基本上很难找到，于是将斜率等于0，转换为最后一次求得的J(θ)与上一次J(θ)的差值尽可能小，即：$J(θ) - J_l(θ)$ ， 那么这个θ向量就是被找到的向量，θ[0]元素就是截距，θ[1: ]就是系数")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(522),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("计算"),t("strong",[s._v("损失函数")]),s._v("、"),t("strong",[s._v("损失函数倒数")]),s._v("、"),t("strong",[s._v("梯度下降函数")])])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 计算损失函数\n# theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量\ndef J(theta, X_b, y):\n    try:\n        return np.sum((y - X_b.dot(theta))**2) / X_b.shape[0]\n    except Exception as e:\n        return float('inf')\n\n# theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量\ndef dJ(theta, X_b, y):\n    res = np.empty(len(theta))\n    # 这是对J求偏导后第一个元素\n    # 第一个元素有些特殊，得到的是一个向量，\n    # 这个向量应该与1的列向量做点积运算， 也可以将向量元素求和，这里选择求和\n    res[0] = np.sum(X_b.dot(theta) -y)\n    for i in range(1, len(theta)):\n        # 从第二项开始，前面的累加符号由于点积运算不需要理会\n        res[i] = (X_b.dot(theta) -y).dot(X_b[:, i])\n    return res * 2 / X_b.shape[0]\n\n# 这里的initial_theta是一个向量，不在是一个值了, 函数最终返回theta向量\ndef gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):\n    \n    theta = initial_theta\n    # theta_history.append(initial_theta)\n    i_iter = 0\n    while i_iter < n_iters:\n        gradient = dJ(theta, X_b, y)\n        last_theta = theta\n        theta = theta - eta*gradient\n        # theta_history.append(theta)\n    \n        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:\n            break\n        \n        i_iter += 1\n    return theta\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br"),t("span",{staticClass:"line-number"},[s._v("34")]),t("br"),t("span",{staticClass:"line-number"},[s._v("35")]),t("br"),t("span",{staticClass:"line-number"},[s._v("36")]),t("br"),t("span",{staticClass:"line-number"},[s._v("37")]),t("br")])]),t("ul",[t("li",[s._v("调用函数，计算θ")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 拼凑出X_b矩阵, 由于训练的X数据集是只有一列的矩阵，因此合并后是两列的矩阵\nX_b = np.hstack([\n    np.ones((X.shape[0], 1)), \n    X])\n\n# 让初始化的theta值从0开始, initial_theta只有两个元素\ninitial_theta = np.zeros(X_b.shape[1])\neta = 0.01\n\ntheta = gradient_descent(X_b, y, initial_theta, eta)\ntheta\n输出结果：\narray([4.02145786, 3.00706277])\n# 小结：预测结果与期望结果类似，斜率为3，截距为4\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br")])]),t("ul",[t("li",[s._v("修改之前的linearRegression.py模块的LinearRegression类, 添加梯度下降的拟合方法")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("import numpy as np\nfrom metrics import r2_score\n\nclass LinearRegression:\n    def __init__(self):\n        # 这是系数\n        self.coef_ = None\n        # 这个是截距\n        self.interception_ = None\n        # 这个就是具体计算出来的θ列向量\n        self._theta = None\n\n    # 这里的拟合是使用正规方程求解\n    def fit_normal(self, X_train, y_train):\n        assert X_train.shape[0] == y_train.shape[0], \\\n        '训练数据集必须与预测训练一个维度'\n\n        # 构建特征矩阵， np.ones((X_train.shape[0], 1)), 为元素都为1的矩阵，且只有1列\n        X_b = np.hstack([\n                         np.ones((X_train.shape[0], 1)),\n                         X_train,\n                        ])\n\n        # 使用θ表达式，计算出θ的值\n        self._theta = np.linalg.inv(X_b.T.dot(X_b))\\\n                      .dot(X_b.T)\\\n                      .dot(y_train)\n\n        # 那么截距就是第一个元素\n        self.interception_ = self._theta[0]\n        # 系数就是后面的元素\n        self.coef_ = self._theta[1:]\n\n        return self\n\n    # 这里的拟合使用梯度下降方法\n    def fit_gd(self, X_train, y_train, eta=0.01, n_iters=1e4):\n        assert X_train.shape[0] == y_train.shape[0], \\\n        '必须同一维度'\n\n        # 计算损失函数\n        # theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量\n        def J(theta, X_b, y):\n            try:\n                return np.sum((y - X_b.dot(theta)) ** 2) / X_b.shape[0]\n            except Exception as e:\n                return float('inf')\n\n        # theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量\n        def dJ(theta, X_b, y):\n            res = np.empty(len(theta))\n            # 这是对J求偏导后第一个元素\n            # 第一个元素有些特殊，得到的是一个向量，\n            # 这个向量应该与1的列向量做点积运算， 也可以将向量元素求和，这里选择求和\n            res[0] = np.sum(X_b.dot(theta) - y)\n            for i in range(1, len(theta)):\n                # 从第二项开始，前面的累加符号由于点积运算不需要理会\n                res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])\n            return res * 2 / X_b.shape[0]\n\n        # 这里的initial_theta是一个向量，不在是一个值了, 函数最终返回theta向量\n        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):\n\n            theta = initial_theta\n            i_iter = 0\n            while i_iter < n_iters:\n                gradient = dJ(theta, X_b, y)\n                last_theta = theta\n                theta = theta - eta * gradient\n\n                if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:\n                    break\n\n                i_iter += 1\n            return theta\n\n        X_b = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n        initial_theta = np.zeros(X_b.shape[1])\n\n        # 最终拿到theta， theta中包括截距和系数\n        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters=n_iters, epsilon=1e-8)\n\n        self.interception_ = self._theta[0]\n        self.coef_ = self._theta[1:]\n        return self\n\n    # 预测\n    def predict(self, X_predict):\n\n        assert self.interception_ is not None and self.coef_ is not None, \\\n        '必须先调用fit_normal方法'\n\n        assert X_predict.shape[1] == len(self.coef_), \\\n        '被预测矩阵的列数必须与系数向量的长度相等'\n\n        # 通过传递进来的X_predict计算新的矩阵X_b\n        X_b = np.hstack([\n            np.ones((X_predict.shape[0], 1)),\n            X_predict,\n        ])\n\n        return X_b.dot(self._theta)\n\n    # 使用R Squared 评测算法，评测算法的准确度\n    def score(self, X_test, y_test):\n        y_predict = self.predict(X_test)\n        return r2_score(y_test, y_predict)\n\n    def __str__(self):\n        return \"LinearRegression(coef_={}, \" \\\n               \"interception_={}, \" \\\n               \"_theta={})\".format(self.coef_, self.interception_, self._theta)\n    \n    __repr__ = __str__\n\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br"),t("span",{staticClass:"line-number"},[s._v("34")]),t("br"),t("span",{staticClass:"line-number"},[s._v("35")]),t("br"),t("span",{staticClass:"line-number"},[s._v("36")]),t("br"),t("span",{staticClass:"line-number"},[s._v("37")]),t("br"),t("span",{staticClass:"line-number"},[s._v("38")]),t("br"),t("span",{staticClass:"line-number"},[s._v("39")]),t("br"),t("span",{staticClass:"line-number"},[s._v("40")]),t("br"),t("span",{staticClass:"line-number"},[s._v("41")]),t("br"),t("span",{staticClass:"line-number"},[s._v("42")]),t("br"),t("span",{staticClass:"line-number"},[s._v("43")]),t("br"),t("span",{staticClass:"line-number"},[s._v("44")]),t("br"),t("span",{staticClass:"line-number"},[s._v("45")]),t("br"),t("span",{staticClass:"line-number"},[s._v("46")]),t("br"),t("span",{staticClass:"line-number"},[s._v("47")]),t("br"),t("span",{staticClass:"line-number"},[s._v("48")]),t("br"),t("span",{staticClass:"line-number"},[s._v("49")]),t("br"),t("span",{staticClass:"line-number"},[s._v("50")]),t("br"),t("span",{staticClass:"line-number"},[s._v("51")]),t("br"),t("span",{staticClass:"line-number"},[s._v("52")]),t("br"),t("span",{staticClass:"line-number"},[s._v("53")]),t("br"),t("span",{staticClass:"line-number"},[s._v("54")]),t("br"),t("span",{staticClass:"line-number"},[s._v("55")]),t("br"),t("span",{staticClass:"line-number"},[s._v("56")]),t("br"),t("span",{staticClass:"line-number"},[s._v("57")]),t("br"),t("span",{staticClass:"line-number"},[s._v("58")]),t("br"),t("span",{staticClass:"line-number"},[s._v("59")]),t("br"),t("span",{staticClass:"line-number"},[s._v("60")]),t("br"),t("span",{staticClass:"line-number"},[s._v("61")]),t("br"),t("span",{staticClass:"line-number"},[s._v("62")]),t("br"),t("span",{staticClass:"line-number"},[s._v("63")]),t("br"),t("span",{staticClass:"line-number"},[s._v("64")]),t("br"),t("span",{staticClass:"line-number"},[s._v("65")]),t("br"),t("span",{staticClass:"line-number"},[s._v("66")]),t("br"),t("span",{staticClass:"line-number"},[s._v("67")]),t("br"),t("span",{staticClass:"line-number"},[s._v("68")]),t("br"),t("span",{staticClass:"line-number"},[s._v("69")]),t("br"),t("span",{staticClass:"line-number"},[s._v("70")]),t("br"),t("span",{staticClass:"line-number"},[s._v("71")]),t("br"),t("span",{staticClass:"line-number"},[s._v("72")]),t("br"),t("span",{staticClass:"line-number"},[s._v("73")]),t("br"),t("span",{staticClass:"line-number"},[s._v("74")]),t("br"),t("span",{staticClass:"line-number"},[s._v("75")]),t("br"),t("span",{staticClass:"line-number"},[s._v("76")]),t("br"),t("span",{staticClass:"line-number"},[s._v("77")]),t("br"),t("span",{staticClass:"line-number"},[s._v("78")]),t("br"),t("span",{staticClass:"line-number"},[s._v("79")]),t("br"),t("span",{staticClass:"line-number"},[s._v("80")]),t("br"),t("span",{staticClass:"line-number"},[s._v("81")]),t("br"),t("span",{staticClass:"line-number"},[s._v("82")]),t("br"),t("span",{staticClass:"line-number"},[s._v("83")]),t("br"),t("span",{staticClass:"line-number"},[s._v("84")]),t("br"),t("span",{staticClass:"line-number"},[s._v("85")]),t("br"),t("span",{staticClass:"line-number"},[s._v("86")]),t("br"),t("span",{staticClass:"line-number"},[s._v("87")]),t("br"),t("span",{staticClass:"line-number"},[s._v("88")]),t("br"),t("span",{staticClass:"line-number"},[s._v("89")]),t("br"),t("span",{staticClass:"line-number"},[s._v("90")]),t("br"),t("span",{staticClass:"line-number"},[s._v("91")]),t("br"),t("span",{staticClass:"line-number"},[s._v("92")]),t("br"),t("span",{staticClass:"line-number"},[s._v("93")]),t("br"),t("span",{staticClass:"line-number"},[s._v("94")]),t("br"),t("span",{staticClass:"line-number"},[s._v("95")]),t("br"),t("span",{staticClass:"line-number"},[s._v("96")]),t("br"),t("span",{staticClass:"line-number"},[s._v("97")]),t("br"),t("span",{staticClass:"line-number"},[s._v("98")]),t("br"),t("span",{staticClass:"line-number"},[s._v("99")]),t("br"),t("span",{staticClass:"line-number"},[s._v("100")]),t("br"),t("span",{staticClass:"line-number"},[s._v("101")]),t("br"),t("span",{staticClass:"line-number"},[s._v("102")]),t("br"),t("span",{staticClass:"line-number"},[s._v("103")]),t("br"),t("span",{staticClass:"line-number"},[s._v("104")]),t("br"),t("span",{staticClass:"line-number"},[s._v("105")]),t("br"),t("span",{staticClass:"line-number"},[s._v("106")]),t("br"),t("span",{staticClass:"line-number"},[s._v("107")]),t("br"),t("span",{staticClass:"line-number"},[s._v("108")]),t("br"),t("span",{staticClass:"line-number"},[s._v("109")]),t("br"),t("span",{staticClass:"line-number"},[s._v("110")]),t("br"),t("span",{staticClass:"line-number"},[s._v("111")]),t("br"),t("span",{staticClass:"line-number"},[s._v("112")]),t("br"),t("span",{staticClass:"line-number"},[s._v("113")]),t("br"),t("span",{staticClass:"line-number"},[s._v("114")]),t("br"),t("span",{staticClass:"line-number"},[s._v("115")]),t("br")])]),t("ul",[t("li",[s._v("调用自实现类，计算θ")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("from linearRegression import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit_gd(X_train=X, y_train=y, eta=0.01, n_iters=1e4)\n输出结果：\nLinearRegression(coef_=[3.00706277], interception_=4.021457858204859, _theta=[4.02145786 3.00706277])\n\n小结：同样的系数是3.00706277， 截距是4.021457858204859，符合预期结果\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br")])]),t("ul",[t("li",[s._v("总结：目前学习了两种算法，处理损失函数的最小值\n"),t("ul",[t("li",[s._v("①：正规方程求解，求解出θ的表达式(一般很难求解得到)")]),s._v(" "),t("li",[s._v("②：梯度下降，找到损失函数J(θ)的极值点对应的θ，就是预期的θ，从而获得损失函数J(θ)的系数和截距，最终通过求得的θ向量，得到多元线性回归方程来预测趋势数据")])])])]),s._v(" "),t("h3",{attrs:{id:"二-线性回归使用梯度下降法向量化训练真实数据集"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#二-线性回归使用梯度下降法向量化训练真实数据集"}},[s._v("#")]),s._v(" "),t("code",[s._v("(二)线性回归使用梯度下降法向量化训练真实数据集")])]),s._v(" "),t("ul",[t("li",[t("p",[s._v("之前的▽J(θ)是一项一项通过循环的方式将列向量每一行元素计算出来求解的，速度比较慢，下面通过矩阵运算的方式进行优化")])]),s._v(" "),t("li",[t("p",[s._v("将▽J(θ)进行化简变形 $X_0^i$表示第i行第0列的值为1， 由于需要拼凑成X_b，第0列的元素都是1")])])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1232),alt:"Alt text"}})]),s._v(" "),t("p",[t("img",{attrs:{src:a(1233),alt:"Alt text"}})]),s._v(" "),t("p",[t("img",{attrs:{src:a(1234),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("最后将梯度的表达式化简为 矩阵运算，结果如下")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1235),alt:"Alt text"}})]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 在\n# 修改梯度导数函数\ndef dJ_matrix(theta, X_b, y):\n    return 2. / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br")])]),t("ul",[t("li",[t("strong",[s._v("引入scikit-learn的波士顿房价真实数据进行算法测评(第一个是正规方程求解的方式)")])])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("import numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n# 加载波士顿房产数据\nboston = datasets.load_boston()\n\nX = boston.data\ny = boston.target\n\n# 这里使用的是fancing indexing的方式过滤数据\nX = X[y < 50.0]\ny = y[y < 50.0]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)\n\nfrom linearRegression import LinearRegression\nlin_reg = LinearRegression()\n# 使用线性回归方程的方式求解θ\n%time lin_reg.fit_normal(X_train, y_train)\nlin_reg.score(X_test, y_test)\n\n输出结果：\nCPU times: user 3.25 ms, sys: 492 µs, total: 3.74 ms\nWall time: 2.74 ms\n0.8009390227580956\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br")])]),t("ul",[t("li",[s._v("使用梯度下降的方式(▽J(θ)计算已经通过矩阵计算的方式进行了优化)")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("lin_reg2 = LinearRegression()\n# 使用梯度下降的方式求解θ, eta的取值要非常小，保证移动的步长是收敛的\n%time lin_reg.fit_gd(X_train, y_train, eta=0.0000001, n_iters=1e6)\nlin_reg.score(X_test, y_test)\n输出结果：\nCPU times: user 1min 6s, sys: 403 ms, total: 1min 7s\nWall time: 34.5 s\n0.5116716158630092\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br")])]),t("ul",[t("li",[s._v("小结： "),t("strong",[s._v("上面的运行结果表明，eta、n_iters这两个值都需要调整，因为这两个值决定了测试结果的准确度，但是，这两个值一般都是事先指定好的，解决的方法可以使用数据归一化，将特征数据和真值数据都统一到同一个维度")])])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1236),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("将特征数据X矩阵和真值数据y列向量归一化处理")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 将初始值进行数据归一化处理\nfrom sklearn.preprocessing import StandardScaler\n\nstandardScaler = StandardScaler()\n# 调用fit方法，目的是先计算出均值和方差\nstandardScaler.fit(X_train)\n# 通过fit方法计算出来的举止和方差将X_train、X_test进行方差均值归一化\nX_train_standard = standardScaler.transform(X_train)\nX_test_standard = standardScaler.transform(X_test)\n\nlin_reg3 = LinearRegression()\n%time lin_reg3.fit_gd(X_train_standard, y_train)\n\n输出结果：\nCPU times: user 317 ms, sys: 3.19 ms, total: 320 ms\nWall time: 167 ms\n\nlin_reg3.score(X_test_standard, y_test)\n输出结果：\n0.800927010538664\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br")])]),t("ul",[t("li",[s._v("小结：通过上述结果可以得知：正规方程的运算准确度与梯度下降的运算准确度一样, 梯度下降法寻找最优的θ向量的时间比正规矩阵的方法耗时更加长， 但是这里是有原因的, 看下面这个例子")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 初始化样本数\nm = 1000\n# 初始化特征数\nn = 5000\n# 初始化符合正态分布的X特征矩阵\nbig_X = np.random.normal(size=(m, n))\n# 从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high\ntrue_theta = np.random.uniform(0.0, 100.0, size=n+1)\n# 生成y真值,生成的公式就是y = X*θ + θ[0] + 噪音(m ✖️1的矩阵)\nbig_y = big_X.dot(true_theta[1:]) + true_theta[0] + np.random.normal(0.0,10.0,size=m)\n# 使用正规矩阵的方式，训练的结果耗时6s\nbig_reg1 = LinearRegression()\n%time big_reg1.fit_normal(big_X, big_y)\n输出结果：\nCPU times: user 11 s, sys: 396 ms, total: 11.4 s\nWall time: 5.86 s\n# 使用梯度下降的方式耗时\nbig_reg2 = LinearRegression()\n%time big_reg2.fit_gd(big_X, big_y)\n输出结果：\nCPU times: user 3.86 s, sys: 32.4 ms, total: 3.9 s\nWall time: 2.14 s\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br")])]),t("ul",[t("li",[s._v("说明：此时，梯度下降算法的时间比正规矩阵算法的时间少， 因为正规方程处理的是一个m x n 的矩阵运算，而梯度下降是根据样本数量规模影响时间的， 这种梯度下降法是批量梯度下降法，那么当样本量大的化可以使用随机梯度下降法来优化")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1237),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("这种批量梯度下降法随着m样本数越大，计算的时间就长")])]),s._v(" "),t("h3",{attrs:{id:"三-随机梯度下降法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#三-随机梯度下降法"}},[s._v("#")]),s._v(" "),t("code",[s._v("(三)随机梯度下降法")])]),s._v(" "),t("ul",[t("li",[s._v("随机梯度下降法就是将▽J(θ)向量只取一个样本，不再要前面的求和，例如i=3\n"),t("ul",[t("li",[s._v("这种极小值搜索的曲线将不是沿着一个方向下降，可能会因为随机的不同，递降的方向发生变化，为了避免递降的速度在初始的时候更快，η的去取值会被a、b常数调整。这就是模拟退火的思想")])])])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1238),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("随机梯度下降法从公式本身来看，就是对▽J(θ)进行了改造，批量梯度下降法是考虑了特征矩阵的所有行，在随机梯度下降法中只考虑一行即可，因此没有了前面的求和符号和除以m")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1239),alt:"Alt text"}})]),s._v(" "),t("p",[t("img",{attrs:{src:a(1240),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[t("p",[s._v("举例：一个一维特征矩阵的线性回归方程，通过X_train、y_train进行训练，计算出θ，并且比较批量梯度下降方法与随机梯度下降方法的耗时时长")])]),s._v(" "),t("li",[t("p",[t("strong",[s._v("①：初始化特征矩阵")])])])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 特征矩阵X， 真值y\nimport numpy as np \nimport matplotlib as plt\n\n# 初始化样本个数\nm = 10000\n# 初始化样本特征个数，这里特征个数为1, 且保证了特征数据为正态分布的数据\nx = np.random.normal(size=m)\nX = x.reshape(-1, 1)\n# 这里模拟的是一个特征的线性回归方程, 注意，这里是小x，如果是X矩阵，那么y的shape就是10000 x 10000\ny = 4. * x + 3. + np.random.normal(0, 3, size=m)\nprint(X.shape)\nprint(y.shape)\n输出结果：\n(10000, 1)\n(10000,)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br")])]),t("ul",[t("li",[s._v("②：使用批量梯度下降方法计算θ值")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("def J(theta, X_b, y):\n    try:\n        return np.sum((y - X_b.dot(theta)) ** 2) / X_b.shape[0]\n    except Exception as e:\n        return float('inf')\n\ndef dJ_matrix(theta, X_b, y):\n    return 2. / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n    \n# 这里的initial_theta是一个向量，不在是一个值了, 函数最终返回theta向量\ndef gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):\n    theta = initial_theta\n    i_iter = 0\n    \n    while i_iter < n_iters:\n        gradient = dJ_matrix(theta, X_b, y)\n        last_theta = theta\n        theta = theta - eta * gradient\n        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:\n            break\n        i_iter += 1\n    return theta\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br")])]),t("ul",[t("li",[s._v("③：使用批量梯度下降方法计算θ值且获取耗时时长")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("%%time\nX_b = np.hstack([np.ones((len(X), 1)), X])\ninitial_theta = np.zeros(X_b.shape[1])\neta = 0.01\ntheta = gradient_descent(X_b, y, initial_theta, eta)\nprint(theta)\n输出结果：\n[3.04923175 3.98437447]\n耗时：103ms\nCPU times: user 170 ms, sys: 4.44 ms, total: 175 ms\nWall time: 103 ms\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br")])]),t("ul",[t("li",[s._v("④：使用随机梯度下降法计算θ值且获取耗时时长(由于在随机梯度下降法中，判断循环退出的条件，不在按照批量梯度下降方法计算J(θ)的差值，判断循环退出的条件只由n_iters循环迭代次数决定")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 这里传递进来的特征矩阵，不再是整个的X_b，而是X_b中的一行特征，相当于一个特征行向量\n# y不再是列向量，而是一个值\ndef dJ_sgd(theta, X_b_i, y_i):\n    # 根据随机梯度下降的公式可知\n    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.\n\n# 计算θ值\n# \ndef stochastic_gradient_descent(X_b, y, initial_theta, n_iters):\n    t0 = 5\n    t1 = 50\n    \n    # 这个就是退火的学习率\n    def learning_rate(t):\n        return t0 / (t + t1)\n    \n    theta = initial_theta\n    \n    # 循环获取迭代次数, 这里每次迭代就是为了计算学习率,这里看看是否可以改为while循环\n    for cur_iter in range(n_iters):\n        # 获取一个数据样本的索引，这里是在样本总长度中随机选择一个样本\n        rand_i = np.random.randint(len(X_b))\n        # 获取梯度值\n        gradient = dJ_sgd(theta, X_b[rand_i], y[rand_i])\n        # 获取最新的θ值，这里最新的θ值计算的公式为θ沿导数递降的变化率\n        theta = theta - learning_rate(cur_iter) * gradient\n        return theta\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br")])]),t("ul",[t("li",[s._v("在pycharm中封装自己的sgd随机梯度下降算法")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("    def fit_sgd(self, X_train, y_train, n_iters, t0=5, t1=50):\n        assert X_train.shape[0] == y_train.shape[0], \\\n            '样本数必须等于真值的列向量行数'\n\n        assert n_iters >= 1, \\\n            '迭代所有样本的次数必须大于1'\n\n        def dJ_sgd(theta, X_b_i, y_i):\n            # 这里去掉了转置和点乘\n            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.\n\n        # 计算θ值\n        # 这里的n_iters的意义发生了变化，n_iters指的是在循环迭代过程中，需要把所有的样本计算n_iters遍\n        def stochastic_gradient_descent(X_b, y, initial_theta, n_iters, t0=5, t1=50):\n\n            # 这个就是退火的学习率\n            def learning_rate(t):\n                return t0 / (t + t1)\n\n            theta = initial_theta\n            # 获取样本数\n            m = len(X_b)\n\n            # 循环获取迭代次数, 迭代次数为样本的n_iters倍\n            for cur_iter in range(n_iters):\n                # 获取随机排列的索引\n                indexes = np.random.permutation(m)\n                # 让之前的X_b、y_new的所有样本随机排列,\n                X_b_new = X_b[indexes]\n                y_new = y[indexes]\n\n                # 对每一个样本都进行遍历\n                for i in range(m):\n                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])\n                    theta = theta - learning_rate(cur_iter * m + i) * gradient\n\n            return theta\n\n        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])\n        initial_theta = np.zeros(X_b.shape[1])\n        # 使用随机梯度下降计算theta,\n        self._theta = stochastic_gradient_descent(X_b, y_train, initial_theta, n_iters=n_iters)\n        self.interception_ = self._theta[0]\n        self.coef_ = self._theta[1:]\n        return self\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br"),t("span",{staticClass:"line-number"},[s._v("34")]),t("br"),t("span",{staticClass:"line-number"},[s._v("35")]),t("br"),t("span",{staticClass:"line-number"},[s._v("36")]),t("br"),t("span",{staticClass:"line-number"},[s._v("37")]),t("br"),t("span",{staticClass:"line-number"},[s._v("38")]),t("br"),t("span",{staticClass:"line-number"},[s._v("39")]),t("br"),t("span",{staticClass:"line-number"},[s._v("40")]),t("br"),t("span",{staticClass:"line-number"},[s._v("41")]),t("br"),t("span",{staticClass:"line-number"},[s._v("42")]),t("br"),t("span",{staticClass:"line-number"},[s._v("43")]),t("br"),t("span",{staticClass:"line-number"},[s._v("44")]),t("br"),t("span",{staticClass:"line-number"},[s._v("45")]),t("br")])]),t("ul",[t("li",[s._v("使用自己的SGD计算θ值")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 模拟数据\nimport numpy as np \nimport matplotlib as plt\n\n# 初始化样本个数\nm = 100000\n# 初始化样本特征个数，这里特征个数为1, 且保证了特征数据为正态分布的数据\nx = np.random.normal(size=m)\nX = x.reshape(-1, 1)\n# 这里模拟的是一个特征的线性回归方程, 注意，这里是小x，如果是X矩阵，那么y的shape就是10000 x 10000\ny = 4. * x + 3. + np.random.normal(0, 3, size=m)\nprint(X.shape)\nprint(y.shape)\n输出结果：\n(100000, 1)\n(100000,)\n\nfrom linearRegression import LinearRegression\n\nlin_reg4 = LinearRegression()\nlin_reg4.fit_sgd(X, y, n_iters=10)\n输出结果：\nLinearRegression(coef_=[3.99974133], interception_=3.007942096244411, _theta=[3.0079421  3.99974133])\n\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br")])]),t("ul",[t("li",[s._v("使用波士顿房价数据验证算法的准确度")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("import numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n# 加载波士顿房产数据\nboston = datasets.load_boston()\n\nX = boston.data\ny = boston.target\n\n# 这里使用的是fancing indexing的方式过滤数据\nX = X[y < 50.0]\ny = y[y < 50.0]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)\n\n# 将初始值进行数据归一化处理\nfrom sklearn.preprocessing import StandardScaler\n\nstandardScaler = StandardScaler()\nstandardScaler.fit(X_train)\nX_train_standard = standardScaler.transform(X_train)\nX_test_standard = standardScaler.transform(X_test)\n\nfrom linearRegression import LinearRegression\nlin_reg = LinearRegression()\n\n%time lin_reg.fit_sgd(X_train_standard, y_train, n_iters=1000)\nlin_reg.score(X_test_standard, y_test)\n输出结果：\nCPU times: user 1.97 s, sys: 11.3 ms, total: 1.99 s\nWall time: 2.02 s\n0.8009992300659649\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br"),t("span",{staticClass:"line-number"},[s._v("34")]),t("br"),t("span",{staticClass:"line-number"},[s._v("35")]),t("br")])]),t("ul",[t("li",[s._v("使用scikit-learn中的SGD随机梯度下降方法验证准确度")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 这个类只能解决线性模型\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(n_iter=1000)\n%time sgd_reg.fit(X_train_standard, y_train)\nsgd_reg.score(X_test_standard, y_test)\n输出结果：\nCPU times: user 35.8 ms, sys: 1.54 ms, total: 37.3 ms\nWall time: 36.5 ms\n0.8010109472897677\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br")])]),t("ul",[t("li",[s._v("小结：从自实现的算法和scikit-learn算法对比，scikit-learn耗时少")])]),s._v(" "),t("h3",{attrs:{id:"四-梯度下降法的调试方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#四-梯度下降法的调试方法"}},[s._v("#")]),s._v(" "),t("code",[s._v("(四)梯度下降法的调试方法")])]),s._v(" "),t("ul",[t("li",[t("p",[s._v("对于推导出来的算法公式，如何证明是正确且准确的呢？这就需要梯度下降的调试方法了")])]),s._v(" "),t("li",[t("p",[s._v("当θ是一个值的时候，此时是一维的场景， 那么在θ这点的导数就两个蓝色点的斜率")])])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1241),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("当θ是一个向量的时候，此时是二维的场景， 那么在θ这点的导数就等于对各个分量求导")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1242),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("那么对$θ_0$这个点的导数可以表示如下：")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1243),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[s._v("那么对$θ_1$这个点的导数可以表示如下：")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1244),alt:"Alt text"}})]),s._v(" "),t("ul",[t("li",[t("strong",[s._v("线性回归方程初始化")])])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(666)\n# 特征矩阵\nX = np.random.random(size=(1000, 10))\n# theta值， 11个元素\ntrue_theta = np.arange(1,12,dtype=float)\n# 拼凑出X_b, 11列\nX_b = np.hstack([np.ones((len(X), 1)), X])\n# 这个根据数学上的点乘计算，true_theta为列向量，\n# 左边一个矩阵.列向量=列向量， 再加上一个列向量，还是等于列向量\ny = X_b.dot(true_theta) + np.random.normal(size=1000)\nprint(X.shape)\nprint(y.shape)\nprint(true_theta)\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br")])]),t("ul",[t("li",[s._v("准备损失函数、数学梯度函数、dubug梯度函数、梯度下降函数")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("# 损失函数\ndef J(theta, X_b, y):\n    try:\n        return np.sum((y - X_b.dot(theta)) ** 2) / X_b.shape[0]\n    except Exception as e:\n        return float('inf')\n\n# 损失函数导数，数学推导出来的\ndef dJ_matrix(theta, X_b, y):\n    return 2. / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n\n# 根据之前的公式，求出debug模式下的导数\ndef dJ_debug(theta, X_b, y, epsilon=0.01):\n    # 初始化一个与theta等长度的向量\n    res = np.empty(len(theta))\n    for i in range(len(theta)):\n        theta_1 = theta.copy()\n        theta_1[i] += epsilon\n        theta_2 = theta.copy()\n        theta_2[i] -= epsilon\n        # 计算每个维度的偏导数\n        res[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y)) / (2 * epsilon)\n    return res\n\n# 这里多加了一个参数，dJ_matrix，这个函数定义了如何求解梯度\ndef gradient_descent(dJ_matrix, X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):\n    theta = initial_theta\n    i_iter = 0\n    while i_iter < n_iters:\n        gradient = dJ_matrix(theta, X_b, y)\n        last_theta = theta\n        theta = theta - eta * gradient\n        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:\n            break\n        i_iter += 1\n    return theta     \n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br"),t("span",{staticClass:"line-number"},[s._v("8")]),t("br"),t("span",{staticClass:"line-number"},[s._v("9")]),t("br"),t("span",{staticClass:"line-number"},[s._v("10")]),t("br"),t("span",{staticClass:"line-number"},[s._v("11")]),t("br"),t("span",{staticClass:"line-number"},[s._v("12")]),t("br"),t("span",{staticClass:"line-number"},[s._v("13")]),t("br"),t("span",{staticClass:"line-number"},[s._v("14")]),t("br"),t("span",{staticClass:"line-number"},[s._v("15")]),t("br"),t("span",{staticClass:"line-number"},[s._v("16")]),t("br"),t("span",{staticClass:"line-number"},[s._v("17")]),t("br"),t("span",{staticClass:"line-number"},[s._v("18")]),t("br"),t("span",{staticClass:"line-number"},[s._v("19")]),t("br"),t("span",{staticClass:"line-number"},[s._v("20")]),t("br"),t("span",{staticClass:"line-number"},[s._v("21")]),t("br"),t("span",{staticClass:"line-number"},[s._v("22")]),t("br"),t("span",{staticClass:"line-number"},[s._v("23")]),t("br"),t("span",{staticClass:"line-number"},[s._v("24")]),t("br"),t("span",{staticClass:"line-number"},[s._v("25")]),t("br"),t("span",{staticClass:"line-number"},[s._v("26")]),t("br"),t("span",{staticClass:"line-number"},[s._v("27")]),t("br"),t("span",{staticClass:"line-number"},[s._v("28")]),t("br"),t("span",{staticClass:"line-number"},[s._v("29")]),t("br"),t("span",{staticClass:"line-number"},[s._v("30")]),t("br"),t("span",{staticClass:"line-number"},[s._v("31")]),t("br"),t("span",{staticClass:"line-number"},[s._v("32")]),t("br"),t("span",{staticClass:"line-number"},[s._v("33")]),t("br"),t("span",{staticClass:"line-number"},[s._v("34")]),t("br"),t("span",{staticClass:"line-number"},[s._v("35")]),t("br"),t("span",{staticClass:"line-number"},[s._v("36")]),t("br")])]),t("ul",[t("li",[s._v("准备X_b、initial_theta、eta")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("X_b = np.hstack([np.ones((len(X), 1)), X])\ninitial_theta = np.zeros(X_b.shape[1])\neta = 0.01\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("ul",[t("li",[s._v("使用debug方式计算θ值")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("%time theta = gradient_descent(dJ_debug, X_b, y, initial_theta, eta)\nprint(theta)\n输出结果：\nCPU times: user 7.32 s, sys: 34.1 ms, total: 7.36 s\nWall time: 3.8 s\n[ 1.1251597   2.05312521  2.91522497  4.11895968  5.05002117  5.90494046\n  6.97383745  8.00088367  8.86213468  9.98608331 10.90529198]\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br")])]),t("ul",[t("li",[s._v("使用数学推导的方式计算θ值")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("%time theta = gradient_descent(dJ_matrix, X_b, y, initial_theta, eta)\nprint(theta)\n输出结果：\nCPU times: user 918 ms, sys: 3.64 ms, total: 922 ms\nWall time: 479 ms\n[ 1.1251597   2.05312521  2.91522497  4.11895968  5.05002117  5.90494046\n  6.97383745  8.00088367  8.86213468  9.98608331 10.90529198]\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br"),t("span",{staticClass:"line-number"},[s._v("4")]),t("br"),t("span",{staticClass:"line-number"},[s._v("5")]),t("br"),t("span",{staticClass:"line-number"},[s._v("6")]),t("br"),t("span",{staticClass:"line-number"},[s._v("7")]),t("br")])]),t("ul",[t("li",[s._v("小结")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("\t这个就是使用了使用debug方式计算θ值与数学推导出来的公式进行对比，最后两者的结果一样，表明数学推导正确, dJ_matrix函数只适用线性的梯度下降问题求解梯度, 这个dJ_debug函数，可以用到所有的梯度下降方法求解梯度的运算中， 可以将dJ_debug函数加入到机器学习的工具集中\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("h3",{attrs:{id:"五-梯度下降方法总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#五-梯度下降方法总结"}},[s._v("#")]),s._v(" "),t("code",[s._v("(五)梯度下降方法总结")])]),s._v(" "),t("ul",[t("li",[s._v("分类")])]),s._v(" "),t("p",[t("img",{attrs:{src:a(1245),alt:"Alt text"}})]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("1：批量梯度下降法：稳定，但是速度慢\n2：随机批量下降法：在批量梯度下降法的基础上，计算1行样本，速度快，但是不稳定\n3：小批量梯度下降法：在批量梯度下降法的基础上，计算k行样本\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br"),t("span",{staticClass:"line-number"},[s._v("2")]),t("br"),t("span",{staticClass:"line-number"},[s._v("3")]),t("br")])]),t("ul",[t("li",[t("strong",[s._v("自己实现小批量梯度下降法")])])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("待实现内容\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("ul",[t("li",[s._v("梯度下降法")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("不是机器学习的一种算法，而是一种解决搜索中的最优化的搜索方法\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])]),t("ul",[t("li",[s._v("梯度上升方法")])]),s._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[s._v("前讲的都是对损失函数求最小值，但是有时候也有对效用函数求最大值，那么就需要使用梯度上升方法\n")])]),s._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[s._v("1")]),t("br")])])])}),[],!1,null,null,null);n.default=e.exports},522:function(s,n,a){s.exports=a.p+"assets/img/2019-06-226.27.36.0d70fdd4.png"}}]);