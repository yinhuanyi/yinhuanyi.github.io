(window.webpackJsonp=window.webpackJsonp||[]).push([[301],{1577:function(a,s,n){a.exports=n.p+"assets/img/2019-08-286.36.09.30de2e4d.png"},2594:function(a,s,n){"use strict";n.r(s);var e=n(9),t=Object(e.a)({},(function(){var a=this,s=a.$createElement,e=a._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h2",{attrs:{id:"一-介绍"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-介绍"}},[a._v("#")]),a._v(" 一：介绍")]),a._v(" "),e("h3",{attrs:{id:"一-概念"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-概念"}},[a._v("#")]),a._v(" (一)概念")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("RDD、DataFrame与Dataset是spark的弹性分布式数据集，可以方便的处理数据\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('DataFrame可以对列进行重新命名\nval dateSet = spark.read.textFile(filePath)\n            .flatMap(x=>x.split(" "))\n            .map(x=>(x,1))\n            .groupBy("_1")\n            .count()\n            // 给两个列取名\n            .toDF("key", "value")\n\ndateSet.printSchema()\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br")])]),e("h2",{attrs:{id:"二-rdd概念"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二-rdd概念"}},[a._v("#")]),a._v(" 二：RDD概念")]),a._v(" "),e("h3",{attrs:{id:"一-创建方式"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-创建方式"}},[a._v("#")]),a._v(" (一)创建方式")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('1： 基于内部数据结构平行创建rdd\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\n\nval conf = new SparkConf().setAppName("HDFSTest").setMaster("local[2]")\nval sc = new SparkContext(conf)\nval data = Array(1,2,3,4)\nval rdd = sc.parallelize(data)\nprintln(rdd.count())\n\n\n2: 基于外部文件创建rdd\nval rdd = spark.sparkContext.textFile(filePath)\n\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br")])]),e("h3",{attrs:{id:"二-5中特性"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二-5中特性"}},[a._v("#")]),a._v(" (二)5中特性")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("1：有一个分片列表，就是能被切分，和Hadoop一样，能够切分的数据才能并行计算。\n　　一组分片（partition），即数据集的基本组成单位，对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。每个分配的存储是由BlockManager实现的，每个分区都会被逻辑映射成BlockManager的一个Block，而这个Block会被一个Task负责计算。\n2：由一个函数计算每一个分片，这里指的是下面会提到的compute函数。\n\tSpark中的RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。\n3：对其他RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。\n\tRDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。\n4可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的paritioner接口，控制Key分到哪个reduce。\n\t一个列表，存储存取每个Partition的优先位置（preferred\n\t一个partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个基于范围的RangePartitioner。只有对于key-value的RDD，才会有Partitioner，非key-value的RDD的Partitioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。\n\n5可选：每一分片的优先计算位置，比如HDFS的block的所在位置应该是优先计算的位置。\n 　　一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br")])]),e("h2",{attrs:{id:"三-rdd操作方式-主要是transformation、action、persist"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#三-rdd操作方式-主要是transformation、action、persist"}},[a._v("#")]),a._v(" 三：RDD操作方式，主要是Transformation、Action、Persist")]),a._v(" "),e("h3",{attrs:{id:"一-基本操作"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-基本操作"}},[a._v("#")]),a._v(" (一)基本操作")]),a._v(" "),e("ul",[e("li",[a._v("Transformation和Action")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('// 获取rdd\nval lines = sc.textFile("data.txt")\n// rdd的transformation\nval lineLengths = lines.map(s =### s.length)\n// rdd的action\nval totalLength = lineLengths.reduce((a, b) =### a + b)\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br")])]),e("ul",[e("li",[a._v("将rdd持久化")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("val rdd = sc.parallelize(data)\nrdd.persist(StorageLevel.DISK_ONLY)\nrdd.count\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("p",[e("img",{attrs:{src:n(1577),alt:"Alt text"}})]),a._v(" "),e("ul",[e("li",[a._v("Transformation操作")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("map、flatMap、mapPartitions、glom、filter、distinct、cartesian、union、mapValues、subtract、sample、takeSample、groupByKey、partitionBy、cogroup、combineByKey、reduceByKey、join、leftOuterJoin、rightOuterJoin\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("ul",[e("li",[a._v("Actions操作")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("first、count、reduce、take、top、takeOrdered、aggregate、fold、lookup、countByKey、foreach、foreachPartition、sortBy、saveAsTextFile、saveAsSequenceFile、saveAsObjectFile、saveAsHadoopFile、saveAsHadoopDataset、saveAsNewAPIHadoopFile、saveAsNewAPIHadoopDataset、\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("ul",[e("li",[a._v("Persistence操作")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("persist\ncache\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br")])]),e("ul",[e("li",[a._v("小练习1：将两种数据合并，统计词频，倒叙排列，过滤词频大于8的数据，写入hdfs文件系统")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('import org.apache.spark.{SparkConf, SparkContext}\nobject Test {\n    def main(args: Array[String]): Unit = {\n        val conf = new SparkConf().setAppName("HDFSTest").setMaster("local[2]")\n        val sc = new SparkContext(conf)\n        val rdd1 = sc.textFile("/Users/yinhuanyi/Scala_Project/sparktest01/src/main/resources/worldcount.txt")\n        val rdd2 = sc.textFile("/Users/yinhuanyi/Scala_Project/sparktest01/src/main/resources/worldcount.txt")\n        // 纵向合并\n        val rddUnion = rdd1.union(rdd2)\n        // flatMap(x=>x.split(" "))方法先映射之后在压平\n        // collect()方法可以将rdd转换为数组\n        // map(x=>(x,1))方法是将字符串映射为元组\n        // reduceByKey()方法将RDD[K,V]中每个K对应的V值根据函数法则运算，_+_ 表示相同的K对应的V值累加，也可以reduceByKey((a,b)=>(a+b))\n        // filter(x=>(x._2>8)) 过滤V值大于8的数据\n        // map(x=>(x._2, x._1)) 将K和V调换位置\n        // sortByKey(ascending = false) 基于key排倒叙\n        val lines = rddUnion.flatMap(x=>x.split(" "))\n            .map(x=>(x,1))\n            .reduceByKey(_+_)\n            .filter(x=>(x._2>8))\n            .map(x=>(x._2, x._1))\n            .sortByKey(ascending = false)\n            .map(x=>(x._2, x._1))\n            .saveAsTextFile("hdfs://node1:9000/user/root/data/tongji.txt")\n    }\n}\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br"),e("span",{staticClass:"line-number"},[a._v("24")]),e("br"),e("span",{staticClass:"line-number"},[a._v("25")]),e("br"),e("span",{staticClass:"line-number"},[a._v("26")]),e("br")])]),e("ul",[e("li",[a._v("小练习2：将rdd重新设置分区数，且结果遍历打印")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('        val lines = rddUnion.flatMap(x=>x.split(" "))\n            .map(x=>(x,1))\n            .reduceByKey(_+_)\n            .filter(x=>(x._2>8))\n            .map(x=>(x._2, x._1))\n            .sortByKey(ascending = false)\n            .map(x=>(x._2, x._1))\n\n        // 重新设置rdd的分区数\n        var newLines = lines.repartition(1)\n        // 变量打印元素\n        newLines.foreach(println)\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br")])]),e("h2",{attrs:{id:"四-dataframe概念"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#四-dataframe概念"}},[a._v("#")]),a._v(" 四：DataFrame概念")]),a._v(" "),e("h3",{attrs:{id:"一-概念-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-概念-2"}},[a._v("#")]),a._v(" (一)概念")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("1: DataFrame是一种基于RDD的分布式数据集，类似于数据库的表，DataFrame的计算基于Spark SQL\n2：DataFrame对比RDD，DataFrame有schema元信息，即DataFrame的每一列都有名称和类型\n3：DataFrame可以基于spark SQL操作\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("h2",{attrs:{id:"五-dataframe操作"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#五-dataframe操作"}},[a._v("#")]),a._v(" 五：DataFrame操作")]),a._v(" "),e("h3",{attrs:{id:"一-操作1"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-操作1"}},[a._v("#")]),a._v(" (一)操作1")]),a._v(" "),e("ul",[e("li",[a._v("toDF( )方法可以直接将RDD转换为dataFrame, dataFrame再转换为表，再通过spark对象查询表即可")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('object Test {\n    def main(args: Array[String]): Unit = {\n        val spark = SparkSession\n            .builder\n            .master("local[2]")\n            .appName("HDFSTest")\n            .getOrCreate()\n\n        val filePath = "/Users/yinhuanyi/Scala_Project/sparktest01/src/main/resources/worldcount.txt"\n        import spark.implicits._\n\n        val dataFrame = spark.sparkContext.textFile(filePath)\n            .flatMap(x=>x.split(" "))\n            .map(x=>(x,1))\n            .reduceByKey(_+_)\n            .toDF("key", "value")\n\n        // 将dataFrame创建一张表\n        dataFrame.createOrReplaceTempView("table1")\n        // 查询表\n        val ret = spark.sql("select * from table1 where value ### 4")\n        ret.show()\n    }\n}\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br"),e("span",{staticClass:"line-number"},[a._v("24")]),e("br")])]),e("h3",{attrs:{id:"二-操作2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二-操作2"}},[a._v("#")]),a._v(" (二)操作2")]),a._v(" "),e("ul",[e("li",[a._v("dataSet转换为dataFrame")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('object Test {\n    def main(args: Array[String]): Unit = {\n        val spark = SparkSession\n            .builder\n            .master("local[2]")\n            .appName("HDFSTest")\n            .getOrCreate()\n\n        val filePath = "/Users/yinhuanyi/Scala_Project/sparktest01/src/main/resources/worldcount.txt"\n        import spark.implicits._\n\n        val dataSet = spark.read.textFile(filePath)\n            .flatMap(x=>x.split(" "))\n            .map(x=>(x,1))\n            .toDF("key", "value")\n            \n        // 将dataSet创建一张表\n        dataSet.createOrReplaceTempView("table1")\n        // 查询表, 这里不用where也可以，因为在group by之前不需要进行条件过滤\n        val ret = spark.sql("select key,sum(value) as sum from table1 where 1=1 group by key having sum>4 order by sum desc")\n        ret.show()\n    }\n}\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br")])]),e("ul",[e("li",[a._v("其实就是完成sql操作, sql效果如下(这个是一个比较经典的SQL)")])]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("+------+-----+\n|   key|value|\n+------+-----+\n|  java|    1|\n|python|    1|\n| scala|    1|\n|  java|    1|\n| scala|    1|\n|python|    1|\n| scala|    1|\n| scala|    1|\n|   C++|    1|\n|   C++|    1|\n|   C++|    1|\n|python|    1|\n|python|    1|\n|python|    1|\n|   C++|    1|\n|  java|    1|\n|  java|    1|\n|   C++|    1|\n|python|    1|\n+------+-----+\n\nSQL: select key,sum(value) as sum from table1 where 1=1 group by key having sum>4 order by sum desc\n\n+------+---+\n|   key|sum|\n+------+---+\n|python|  6|\n|   C++|  5|\n+------+---+\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br"),e("span",{staticClass:"line-number"},[a._v("24")]),e("br"),e("span",{staticClass:"line-number"},[a._v("25")]),e("br"),e("span",{staticClass:"line-number"},[a._v("26")]),e("br"),e("span",{staticClass:"line-number"},[a._v("27")]),e("br"),e("span",{staticClass:"line-number"},[a._v("28")]),e("br"),e("span",{staticClass:"line-number"},[a._v("29")]),e("br"),e("span",{staticClass:"line-number"},[a._v("30")]),e("br"),e("span",{staticClass:"line-number"},[a._v("31")]),e("br"),e("span",{staticClass:"line-number"},[a._v("32")]),e("br")])]),e("h3",{attrs:{id:"三-操作3-直接使用dataset操作数据"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#三-操作3-直接使用dataset操作数据"}},[a._v("#")]),a._v(" (三)操作3 直接使用dataSet操作数据")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('object Test {\n    def main(args: Array[String]): Unit = {\n        val spark = SparkSession\n            .builder\n            .master("local[2]")\n            .appName("HDFSTest")\n            .getOrCreate()\n\n        val filePath = "/Users/yinhuanyi/Scala_Project/sparktest01/src/main/resources/worldcount.txt"\n        import spark.implicits._\n\n        val dataSet = spark.read.textFile(filePath)\n            .flatMap(x=>x.split(" "))\n            .map(x=>(x,1))\n            .groupBy("_1")\n            .count()\n            // 这里由于使用了count方法，那么第二列的列名改为了count，不再是_2\n            .filter($"count" ### 4)\n            .sort($"count".desc)\n            .toDF("key", "value")\n        dataSet.show()\n    }\n}\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br")])]),e("h3",{attrs:{id:"四-小结"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#四-小结"}},[a._v("#")]),a._v(" (四)小结")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('RDD和dataSet在转换为dataFrame的时候，区别在于：\t\n\t1：RDD可以进行reduceByKey()操作，将数据进行聚合之后再使用toDF()转换为dataFrame\n\t2：dataSet虽然可以使用.groupBy("_1")进行聚合，但是聚合只有就无法使用toDF()方法转换为dataFrame，因此dataSet在转换为dataFrame时候，不聚合\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("h2",{attrs:{id:"六-dataset操作"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#六-dataset操作"}},[a._v("#")]),a._v(" 六：DataSet操作")]),a._v(" "),e("h3",{attrs:{id:"一-创建方法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-创建方法"}},[a._v("#")]),a._v(" (一)创建方法")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('1：（只能在spark shell中运行，在idea中会报错）基于外部数据创建dataSet，将数据写入case class中\nval filePath = "file:///opt/datas/worldcount.txt"\nval dataSet1 = spark.read.textFile(filePath).flatMap(x=>x.split(" ")).map(x=>(x,1))\ncase class Person(username: String, age: Int)\ndataSet1.map(x=>Person(x._1, x._2)).show\n\n// 输出结果\n+--------+---+\n|username|age|\n+--------+---+\n|    java|  1|\n|  python|  1|\n|   scala|  1|\n|    java|  1|\n|   scala|  1|\n|  python|  1|\n|   scala|  1|\n|   scala|  1|\n|     C++|  1|\n|     C++|  1|\n|     C++|  1|\n|  python|  1|\n|  python|  1|\n|  python|  1|\n|     C++|  1|\n|    java|  1|\n|    java|  1|\n|     C++|  1|\n|  python|  1|\n+--------+---+\n\n2: 也可以将dataFrame转换为dataSet\nval filePath = "file:///opt/datas/worldcount.txt"\nval dataFrame = spark.sparkContext.textFile(filePath).flatMap(x=>x.split(" ")).map(x=>(x,1)).reduceByKey(_+_).toDF("username", "age")\ncase class Person(username: String, age: Int)\nval dataSet = dataFrame.as[Person]\ndataSet.show()\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br"),e("span",{staticClass:"line-number"},[a._v("10")]),e("br"),e("span",{staticClass:"line-number"},[a._v("11")]),e("br"),e("span",{staticClass:"line-number"},[a._v("12")]),e("br"),e("span",{staticClass:"line-number"},[a._v("13")]),e("br"),e("span",{staticClass:"line-number"},[a._v("14")]),e("br"),e("span",{staticClass:"line-number"},[a._v("15")]),e("br"),e("span",{staticClass:"line-number"},[a._v("16")]),e("br"),e("span",{staticClass:"line-number"},[a._v("17")]),e("br"),e("span",{staticClass:"line-number"},[a._v("18")]),e("br"),e("span",{staticClass:"line-number"},[a._v("19")]),e("br"),e("span",{staticClass:"line-number"},[a._v("20")]),e("br"),e("span",{staticClass:"line-number"},[a._v("21")]),e("br"),e("span",{staticClass:"line-number"},[a._v("22")]),e("br"),e("span",{staticClass:"line-number"},[a._v("23")]),e("br"),e("span",{staticClass:"line-number"},[a._v("24")]),e("br"),e("span",{staticClass:"line-number"},[a._v("25")]),e("br"),e("span",{staticClass:"line-number"},[a._v("26")]),e("br"),e("span",{staticClass:"line-number"},[a._v("27")]),e("br"),e("span",{staticClass:"line-number"},[a._v("28")]),e("br"),e("span",{staticClass:"line-number"},[a._v("29")]),e("br"),e("span",{staticClass:"line-number"},[a._v("30")]),e("br"),e("span",{staticClass:"line-number"},[a._v("31")]),e("br"),e("span",{staticClass:"line-number"},[a._v("32")]),e("br"),e("span",{staticClass:"line-number"},[a._v("33")]),e("br"),e("span",{staticClass:"line-number"},[a._v("34")]),e("br"),e("span",{staticClass:"line-number"},[a._v("35")]),e("br"),e("span",{staticClass:"line-number"},[a._v("36")]),e("br"),e("span",{staticClass:"line-number"},[a._v("37")]),e("br")])]),e("h3",{attrs:{id:"二-dataset的优势"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二-dataset的优势"}},[a._v("#")]),a._v(" (二)DataSet的优势")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("1：DataSet不需要基于Spark SQL，可以直接使用类似于RDD的算子函数处理数据\n\n2：DataSet具有与DataFrame一样的有点，可以将数据赋予对应的数据类型\n\n3：DataSet的创建一般将有列名的DataFrame基于一个case class转换，使得这个DataSet能够像实例对象一样被操作\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br")])])])}),[],!1,null,null,null);s.default=t.exports}}]);