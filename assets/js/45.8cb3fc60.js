(window.webpackJsonp=window.webpackJsonp||[]).push([[45],{1295:function(s,e,t){s.exports=t.p+"assets/img/2018-10-0911.52.18.b624e545.png"},1296:function(s,e,t){s.exports=t.p+"assets/img/ScreenShot_20181010002347.19a70d15.png"},1297:function(s,e,t){s.exports=t.p+"assets/img/2018-10-101.15.35.f83f94ed.png"},1298:function(s,e,t){s.exports=t.p+"assets/img/2018-10-101.57.59.86da7efe.png"},1299:function(s,e,t){s.exports=t.p+"assets/img/ScreenShot_20181010020824.56f1f202.png"},1300:function(s,e,t){s.exports=t.p+"assets/img/ScreenShot_20181010021009.4fe50f82.png"},1301:function(s,e,t){s.exports=t.p+"assets/img/ScreenShot_20181010021039.50681d18.png"},1302:function(s,e,t){s.exports=t.p+"assets/img/ScreenShot_20181010025253.1955e5a7.png"},1303:function(s,e,t){s.exports=t.p+"assets/img/ScreenShot_20181010025336.73c751dd.png"},1304:function(s,e,t){s.exports=t.p+"assets/img/ScreenShot_20181010025447.c1035800.png"},2436:function(s,e,t){"use strict";t.r(e);var a=t(9),n=Object(a.a)({},(function(){var s=this,e=s.$createElement,a=s._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h3",{attrs:{id:"_1-深入理解unicode编码和utf-8编码"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-深入理解unicode编码和utf-8编码"}},[s._v("#")]),s._v(" 1：深入理解Unicode编码和utf-8编码")]),s._v(" "),a("ul",[a("li",[a("strong",[a("code",[s._v("ASCII编码表")])]),s._v(" "),a("img",{attrs:{src:t(1295),alt:"Alt text"}})])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("\t1：计算机只能处理0和1，那么计算机如果需要识别字符，就需要有一套编码机制，ASCII编码由此诞生了。8个bit为一个byte, 11111111 = 255， 那么一个字节最大能够表示255（十进制）。\n\t\n\t2：对于英文字母来说，A~Z,a~z 一共才 26 * 2 = 52个，因此，一个字节表示所有的英文字符是满足需要的。但是对于中文来说，是远远不够的。那么GB2312因此诞生了，11111111 11111111 = 65535， 那么按照GB2312就可以表示65535个汉字字符，基本可以满足现代汉字\n\t\n\t3：不同国家有不同的字符编码表，例如，在中国，假如 10101011 11010110 表示 “中”， 如果你在日本，使用日本的编码表，对应的10101011 11010110，可能表示的就是“日”。那么，中文写好的文字，按照日文编码表编码，就会出现乱码，这就是为什么，有时候打开某些页面的时候，显示的是乱码，是因为编码表引入错误，没有引入正确的编码表\n\t\n\t4：unicode编码的出现，解决了所有语言编码的问题。 但是使用Unicode编码英文字母，本来只需要一个字节的存储空间，变为了两个字节，那么在数据传输的过程中，消耗了更多的带宽\n\t\n\t5：utf-8变长编码的出现，解决了这个存储过多的问题，将英文字符使用一个字节编码，中文字符使用三个字节编码。\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br")])]),a("ul",[a("li",[a("strong",[s._v("内存编码和文件编码")]),s._v(" "),a("img",{attrs:{src:t(1296),alt:"Alt text"}})])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("当文件的字符读入内存的时候，需要指定文件的编码，读入到内存内存时候，会变成Unicode编码\n当内存的字符保存到文件的之后，需要指定编码\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("h3",{attrs:{id:"_2-scrapy框架"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-scrapy框架"}},[s._v("#")]),s._v(" 2：Scrapy框架")]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("\tScrapy，Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。\n\tScrapy吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改。它也提供了多种类型爬虫的基类，如BaseSpider、sitemap爬虫等，最新版本又提供了web2.0爬虫的支持。\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("ul",[a("li",[a("strong",[a("code",[s._v("创建scrapy项目")])])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("1：选择Python3.6虚拟环境， 绑定到Artical目录中，在Artical目录中， \npip install scrapy\n\n2：创建scrapy项目模板\nscrapy startproject ArticleSpider\n\n3：在pycharm中打开 ArticleSpider 这个项目模板\n\n4：在ArticleSpider目录下面创建一个名称为jobbole的spider， 网站的url是：blog.jobbole.com\nscrapy genspider jobbole blog.jobbole.com\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br")])]),a("p",[a("img",{attrs:{src:t(1297),alt:"Alt text"}})]),s._v(" "),a("ul",[a("li",[a("strong",[a("code",[s._v("打断点调试scrapy")])])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("由于scrapy在pycharm中没有工程模板，因此无法直接调试，需要在ArticleSpider项目中，写一个main.py文件来调用命令行\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("ul",[a("li",[a("strong",[s._v("main.py文件")])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('import sys\nimport os\nfrom scrapy.cmdline import execute\n\n\n# 将项目的base路径导入到Python的搜索路径中\nabs_path = os.path.abspath(__file__)\nbase_path = os.path.dirname(abs_path)\nsys.path.append(base_path)\n\n\n# 等价于在命令行输入 scrapy crawl jobbole\nexecute(["scrapy", "crawl", "jobbole"])\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br")])]),a("ul",[a("li",[a("strong",[s._v("在settings文件中，设置ROBOTSTXT_OBEY为False，避免过滤robot爬虫过滤url")])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("ROBOTSTXT_OBEY = False\n\nrobots.txt 是遵循 Robot协议 的一个文件，它保存在网站的服务器中，它的作用是，告诉搜索引擎爬虫，本网站哪些目录下的网页 不希望 你进行爬取收录。在Scrapy启动后，会在第一时间访问网站的 robots.txt 文件，然后决定该网站的爬取范围。但是在某些情况下我们想要获取的内容恰恰是被 robots.txt 所禁止访问的。所以，某些时候，我们就要将此配置项设置为 False ，拒绝遵守 Robot协议 ！\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br")])]),a("ul",[a("li",[a("strong",[s._v("以debug模式运行main.py文件")]),s._v(" "),a("img",{attrs:{src:t(1298),alt:"Alt text"}})])]),s._v(" "),a("h3",{attrs:{id:"_3-xpath介绍"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-xpath介绍"}},[s._v("#")]),s._v(" 3：xpath介绍")]),s._v(" "),a("p",[a("img",{attrs:{src:t(1299),alt:"Alt text"}}),s._v(" "),a("img",{attrs:{src:t(1300),alt:"Alt text"}}),s._v(" "),a("img",{attrs:{src:t(1301),alt:"Alt text"}})]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("ret_selector = response.xpath('//*[@id=\"post-114432\"]/div[1]/h1/text()')\nprint(ret_selector.extract()[0])\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("h3",{attrs:{id:"_4-css选择器介绍"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-css选择器介绍"}},[s._v("#")]),s._v(" 4：css选择器介绍")]),s._v(" "),a("p",[a("img",{attrs:{src:t(1302),alt:"Alt text"}}),s._v(" "),a("img",{attrs:{src:t(1303),alt:"Alt text"}}),s._v(" "),a("img",{attrs:{src:t(1304),alt:"Alt text"}})]),s._v(" "),a("h3",{attrs:{id:"_5-在一个页面中-获取所有文章的url-且解析字段"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-在一个页面中-获取所有文章的url-且解析字段"}},[s._v("#")]),s._v(" 5：在一个页面中，获取所有文章的url，且解析字段")]),s._v(" "),a("ul",[a("li",[a("strong",[s._v("如何从response中获取到标签和标签的值")])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("1: 如果解析一个标签\nret_selector = response.xpath('//*[@id=\"post-114432\"]/div[1]/h1') # 获取h1标签\nret_selector = response.xpath('//*[@id=\"post-114432\"]/div[1]/h1/text()') # 获取h1标签的内部的文本\nprint(ret_selector.extract()) # 返回一个列表，拿到h1标签\nprint(ret_selector.extract()[0]) # 返回h1标签 \n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br")])]),a("ul",[a("li",[a("strong",[s._v("获取文字列表页中所有的文字的url，且解析文章的字段信息")])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("import scrapy\nfrom scrapy.http import Request\nfrom urllib import parse\n\nclass JobboleSpider(scrapy.Spider):\n    name = 'jobbole'\n    allowed_domains = ['blog.jobbole.com']\n    # 爬取的url列表\n    # start_urls = ['http://blog.jobbole.com/114432/']\n    start_urls = ['http://blog.jobbole.com/all-posts/']\n\n    def parse(self, response):\n        # 获取主页中所有文字的url\n\n        # 找到类名为.floated-thumb元素下面的类名为post-thumb的元素下面的a标签，提取a标签的href的url的值\n        post_urls = response.css('#archive .floated-thumb .post-thumb a::attr(href)').extract()\n        for post_url in post_urls:\n            # yield就会将Request返回的实例给scrapy下载，下载好的页面，会给parse_detail回调函数处理, parse.urljoin函数实现url自能拼接\n            yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)\n\n        # 获取下一页的url，.next.page-numbers 使用两个类名确定一个标签， extract_first()方法等价于extract()[0], 如果列表为空，不会抛出异常\n        next_url = response.css('.next.page-numbers::attr(href)').extract_first(\"\")\n        if next_url:\n            # 下一页的url返回的结果应该被重新解析，所以回调函数是parse\n            yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse)\n\n\n    # 这个方法里面，是解析具体文字的字段信息\n    def parse_detail(self, response):\n        pass\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br")])]),a("h3",{attrs:{id:"_6-在获取到文字列表页的每一篇文字的a标签的时候-不做extract-生成列表-而是保留selectorlist对象-且不获取a标签的href属性值-只获取a标签-因为不仅需要获取文章详情页的url-也需要获取图片的url。那么parse函数改下如下"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-在获取到文字列表页的每一篇文字的a标签的时候-不做extract-生成列表-而是保留selectorlist对象-且不获取a标签的href属性值-只获取a标签-因为不仅需要获取文章详情页的url-也需要获取图片的url。那么parse函数改下如下"}},[s._v("#")]),s._v(" 6：在获取到文字列表页的每一篇文字的a标签的时候，不做extract()生成列表，而是保留selectorlist对象，且不获取a标签的href属性值，只获取a标签， 因为不仅需要获取文章详情页的url，也需要获取图片的url。那么parse函数改下如下")]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 获取主页中所有文章的url，已经url中的文字图片")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 找到类名为.floated-thumb元素下面的类名为post-thumb的元素下面的a标签，提取a标签的href的url的值")]),s._v("\n        post_nodes "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("css"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'#archive .floated-thumb .post-thumb a'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" post_node "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" post_nodes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v('# 获取图片的url, 这里的extract_first("")默认值是空，如果获取不到的话')]),s._v("\n            image_url "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" post_node"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("css"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'img::attr(src)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('""')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            post_url "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" post_node"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("css"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'::attr(href)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('""')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# yield就会将Request返回的实例给scrapy下载，下载好的页面，会给parse_detail回调函数处理, parse.urljoin函数实现url自能拼接")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("yield")]),s._v(" Request"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("url"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("parse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("urljoin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" post_url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                          meta"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"front_image_url"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" parse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("urljoin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" image_url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                          callback"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("parse_detail"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 获取下一页的url，.next.page-numbers 使用两个类名确定一个标签， extract_first()方法等价于extract()[0], 如果列表为空，不会抛出异常")]),s._v("\n        next_url "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("css"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'.next.page-numbers::attr(href)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("extract_first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('""')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("if")]),s._v(" next_url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 下一页的url返回的结果应该被重新解析，所以回调函数是parse")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("yield")]),s._v(" Request"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("url"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("parse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("urljoin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("response"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" next_url"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n                          callback"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("parse"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br")])]),a("h3",{attrs:{id:"_7-根据爬取到的页面详情页的字段和图片的url字段构建item-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-根据爬取到的页面详情页的字段和图片的url字段构建item-model"}},[s._v("#")]),s._v(" 7：根据爬取到的页面详情页的字段和图片的url字段构建item model")]),s._v(" "),a("ul",[a("li",[a("strong",[s._v("在items.py文件中开始构建一个JobBoleArticleItem的model，来存储字段")])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("class JobBoleArticleItem(scrapy.Item):\n\n    title = scrapy.Field()\n    create_date = scrapy.Field()\n    # 这里的url是变长的，可以使用MD5来存储，使用url_object_id来标记这个url的MD5值\n    url = scrapy.Field()\n    url_object_id = scrapy.Field()\n    front_image_url = scrapy.Field()\n    front_image_path = scrapy.Field()\n    praise_nums = scrapy.Field()\n    comment_nums = scrapy.Field()\n    fav_nums = scrapy.Field()\n    tags = scrapy.Field()\n    content = scrapy.Field()\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br")])]),a("ul",[a("li",[a("strong",[s._v("那么parse_detail拿到文字详情页面的response之后，开始解析页面，且将字段写入model中")])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('    def parse_detail(self, response):\n\t    # 实例化item\n        article_item = JobBoleArticleItem()\n        \n\t\t# 开始解析字段\n        # 文章的图片不是从文字详情页中获取的，是从Request中的meta字典中传递过来的\n        front_image_url = response.meta.get("front_image_url", "")\n        title = response.css(".entry-header h1::text").extract()[0]\n        create_date = response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·","").strip()\n        praise_nums = response.css(".vote-post-up h10::text").extract()[0]\n        fav_nums = response.css(".bookmark-btn::text").extract()[0]\n        # 如果需要使用到正则匹配的话，最好是先编译正则，在匹配，提高效率\n        match_re = re.match(".*?(\\d+).*", fav_nums)\n        if match_re:\n            fav_nums = int(match_re.group(1))\n        else:\n            fav_nums = 0\n        comment_nums = response.css("a[href=\'#article-comment\'] span::text").extract()[0]\n        match_re = re.match(".*?(\\d+).*", comment_nums)\n        if match_re:\n            comment_nums = int(match_re.group(1))\n        else:\n            comment_nums = 0\n        content = response.css("div.entry").extract()[0]\n        tag_list = response.css("p.entry-meta-hide-on-mobile a::text").extract()\n        tag_list = [element for element in tag_list if not element.strip().endswith("评论")]\n        tags = ",".join(tag_list)\n\n        # 开始存储item\n        # article_item["url_object_id"] = get_md5(response.url)\n        article_item["title"] = title\n        article_item["url"] = response.url\n        try:\n            create_date = datetime.datetime.strptime(create_date, "%Y/%m/%d").date()\n        except Exception as e:\n            create_date = datetime.datetime.now().date()\n        article_item["create_date"] = create_date\n        # todo: 这里需要写成列表的形式，因为使用ImagesPipeline下载图片的时候，将article_item["front_image_url"]作为列表迭代下载的\n        article_item["front_image_url"] = [front_image_url]\n        article_item["praise_nums"] = praise_nums\n        article_item["comment_nums"] = comment_nums\n        article_item["fav_nums"] = fav_nums\n        article_item["tags"] = tags\n        article_item["content"] = content\n\n        # 将model返回给pipeline, 会被settings文件中指定的pipeline处理，也就是pipeline文件中的ArticlespiderPipeline\n        yield article_item\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br"),a("span",{staticClass:"line-number"},[s._v("42")]),a("br"),a("span",{staticClass:"line-number"},[s._v("43")]),a("br"),a("span",{staticClass:"line-number"},[s._v("44")]),a("br"),a("span",{staticClass:"line-number"},[s._v("45")]),a("br"),a("span",{staticClass:"line-number"},[s._v("46")]),a("br"),a("span",{staticClass:"line-number"},[s._v("47")]),a("br")])]),a("h3",{attrs:{id:"_8-修改settings文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8-修改settings文件"}},[s._v("#")]),s._v(" 8：修改settings文件")]),s._v(" "),a("ul",[a("li",[a("strong",[s._v("settings文件中添加了项目根路径，添加了一个下载图片的ImagesPipeline，自定了下载图片的url列表，指定了图片下载存储的路径")])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("# TODO: 添加项目路径 BASE_DIR\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n# todo: 指定item经过的 pipeline， 第一个是item经过ArticlespiderPipeline这个pipeline, 300表示优先级，数字越小，优先级越高, 这里的scrapy.pipelines.images.ImagesPipeline为1，所以item最先经过ImagesPipeline，再经过ArticlespiderPipeline\nITEM_PIPELINES = {\n    'ArticleSpider.pipelines.ArticlespiderPipeline': 300,\n    'scrapy.pipelines.images.ImagesPipeline' : 1,\n\n}\n# todo: 这里是告诉ImagesPipeline去下载这个front_image_url的图片，但是这里front_image_url是一个列表，会迭代，所以item存储front_image_url的value也必须是列表\nIMAGES_URLS_FIELD = \"front_image_url\"\n# todo: 指定图片存储路径\nIMAGES_STORE = os.path.join(BASE_DIR, 'images')\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br")])]),a("h3",{attrs:{id:"_9-创建一个新的pipeline-articleimagepipeline-继承imagespipeline-重写父类的方法-来设置图片的存储地址和保存路径到model中"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_9-创建一个新的pipeline-articleimagepipeline-继承imagespipeline-重写父类的方法-来设置图片的存储地址和保存路径到model中"}},[s._v("#")]),s._v(" 9：创建一个新的pipeline，ArticleImagePipeline，继承ImagesPipeline， 重写父类的方法，来设置图片的存储地址和保存路径到model中")]),s._v(" "),a("ul",[a("li",[s._v("在pipelines.py文件中，添加ArticleImagePipeline")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("# 继承ImagesPipeline, 对下载的图片进行操作， 保存图片的地址\nclass ArticleImagePipeline(ImagesPipeline):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    # 重写图片保存路径\n    def file_path(self, request, response=None, info=None):\n        ## start of deprecation warning block (can be removed in the future)\n        def _warn():\n            from scrapy.exceptions import ScrapyDeprecationWarning\n            import warnings\n            warnings.warn('ImagesPipeline.image_key(url) and file_key(url) methods are deprecated, '\n                          'please use file_path(request, response=None, info=None) instead',\n                          category=ScrapyDeprecationWarning, stacklevel=1)\n\n        # check if called from image_key or file_key with url as first argument\n        if not isinstance(request, Request):\n            _warn()\n            url = request\n        else:\n            url = request.url\n\n        # detect if file_key() or image_key() methods have been overridden\n        if not hasattr(self.file_key, '_base'):\n            _warn()\n            return self.file_key(url)\n        elif not hasattr(self.image_key, '_base'):\n            _warn()\n            return self.image_key(url)\n        ## end of deprecation warning block\n\n        image_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n        # 修改了保存的位置\n        return '{year}/{image_name}.jpg'.format(image_name=image_guid, year=datetime.now().year)\n\n    # 获取图片路径，给item的front_image_path赋值\n    def item_completed(self, results, item, info):\n        values = [value['path'] for ok, value in results if ok]\n        item['front_image_path'] = values.pop() if values else 'abcd.jpg'\n        return item\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br"),a("span",{staticClass:"line-number"},[s._v("30")]),a("br"),a("span",{staticClass:"line-number"},[s._v("31")]),a("br"),a("span",{staticClass:"line-number"},[s._v("32")]),a("br"),a("span",{staticClass:"line-number"},[s._v("33")]),a("br"),a("span",{staticClass:"line-number"},[s._v("34")]),a("br"),a("span",{staticClass:"line-number"},[s._v("35")]),a("br"),a("span",{staticClass:"line-number"},[s._v("36")]),a("br"),a("span",{staticClass:"line-number"},[s._v("37")]),a("br"),a("span",{staticClass:"line-number"},[s._v("38")]),a("br"),a("span",{staticClass:"line-number"},[s._v("39")]),a("br"),a("span",{staticClass:"line-number"},[s._v("40")]),a("br"),a("span",{staticClass:"line-number"},[s._v("41")]),a("br")])]),a("ul",[a("li",[s._v("在settings.py文件中，添加ArticleImagePipeline")])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("ITEM_PIPELINES = {\n    'ArticleSpider.pipelines.ArticlespiderPipeline': 300,\n    'ArticleSpider.pipelines.ArticleImagePipeline': 1,\n}\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br")])]),a("h3",{attrs:{id:"_10-将文章的url与md5映射"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_10-将文章的url与md5映射"}},[s._v("#")]),s._v(" 10：将文章的url与MD5映射")]),s._v(" "),a("ul",[a("li",[a("strong",[s._v("在ArticleSpider下创建一个包，utils包，创建一个common文件， 实现url转MD5")])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("from hashlib import md5\ndef get_md5(url):\n    if isinstance(url, str):\n        url = url.encode()\n    md5_obj = md5()\n    md5_obj.update(url)\n    return md5_obj.hexdigest()\n\nif __name__ == '__main__':\n    # Python3中，内存中的字符串都死Unicode编码的，所以需要再次保存为utf-8编码，才能做md5\n    m = get_md5('www.baidu.com'.encode())\n    print(m)\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br")])]),a("ul",[a("li",[a("strong",[s._v("在jobbole.py文件中，保存这个MD5的值到model(item)中")])])]),s._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v('# 给url映射为MD5\narticle_item["url_object_id"] = get_md5(response.url)\n')])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br")])]),a("ul",[a("li",[a("strong",[a("code",[s._v("到此为止，item中的所有的字段就已经封装好了")])])])])])}),[],!1,null,null,null);e.default=n.exports}}]);