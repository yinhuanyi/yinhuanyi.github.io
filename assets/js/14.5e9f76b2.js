(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{1192:function(s,n,a){s.exports=a.p+"assets/img/2019-07-063.10.18.53f7f3ec.png"},1193:function(s,n,a){s.exports=a.p+"assets/img/2019-07-063.13.46.70e910f6.png"},1194:function(s,n,a){s.exports=a.p+"assets/img/2019-07-063.18.53.5453b3d4.png"},1195:function(s,n,a){s.exports=a.p+"assets/img/2019-07-063.20.48.1b37c95e.png"},1196:function(s,n,a){s.exports=a.p+"assets/img/2019-07-063.42.26.1773b858.png"},1197:function(s,n,a){s.exports=a.p+"assets/img/2019-07-063.50.01.7d814349.png"},1198:function(s,n,a){s.exports=a.p+"assets/img/2019-07-063.55.04.1d0ae829.png"},1199:function(s,n,a){s.exports=a.p+"assets/img/2019-07-063.57.05.65d90dde.png"},1200:function(s,n,a){s.exports=a.p+"assets/img/2019-07-064.03.19.92b2f60d.png"},1201:function(s,n,a){s.exports=a.p+"assets/img/2019-07-064.08.36.346a3855.png"},1202:function(s,n,a){s.exports=a.p+"assets/img/2019-07-071.10.15.e8d66f5e.png"},1203:function(s,n,a){s.exports=a.p+"assets/img/2019-07-064.39.37.45a7e3c4.png"},1204:function(s,n,a){s.exports=a.p+"assets/img/2019-07-071.32.03.c2d2e56d.png"},1205:function(s,n,a){s.exports=a.p+"assets/img/2019-07-071.37.30.9b261e28.png"},1206:function(s,n,a){s.exports=a.p+"assets/img/2019-07-071.41.57.03bd269e.png"},1207:function(s,n,a){s.exports=a.p+"assets/img/2019-07-071.57.39.c86a7a2e.png"},1208:function(s,n,a){s.exports=a.p+"assets/img/2019-07-072.55.48.2cd37cf1.png"},1209:function(s,n,a){s.exports=a.p+"assets/img/12.bd0901d2.png"},1210:function(s,n,a){s.exports=a.p+"assets/img/2.165e6549.png"},1211:function(s,n,a){s.exports=a.p+"assets/img/3.98062d18.png"},2423:function(s,n,a){"use strict";a.r(n);var e=a(9),t=Object(e.a)({},(function(){var s=this,n=s.$createElement,e=s._self._c||n;return e("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[e("h3",{attrs:{id:"一-逻辑回归算法基本概念"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-逻辑回归算法基本概念"}},[s._v("#")]),s._v(" "),e("code",[s._v("(一)逻辑回归算法基本概念")])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1192),alt:"Alt text"}})]),s._v(" "),e("p",[e("img",{attrs:{src:a(1193),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("机器学习的算法本质上就是将一个样本输入到f(x)中，输出一个预测值y_hat\n但是在逻辑回归中，将一个样本输入到f(x)中，输出的值是p_hat来表示输出的概率, 然后对这个概率进行分类，如果这个p_hat>=0.5那么就认为y_hat的值是1，如果p_hat<=0.5那么就认为y_hat的值是0\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br")])]),e("p",[e("img",{attrs:{src:a(1194),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("那么在线性回归中，y_hat就等于 θ_T·x_b，由于x_b是样本最前面加了1，那么θ_0 乘以 1就是截距，那么y_hat的值域为[+无穷， -无穷], 但是概率的值域必须是[0，1], 因此将y_hat的值再作为样本，输入到σ(x)，将值域从[+无穷， -无穷]变为[0，1]， σ(x)就叫做p_hat\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[e("strong",[s._v("Sigmoid 函数")])])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1195),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("# sigmoid将数值转换为概率问题\nimport numpy as np \nimport matplotlib.pyplot as plt\n\ndef sigmoid(t):\n    return 1 / (1 + np.exp(-t))\n\nx = np.linspace(-10, 10, 500)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br")])]),e("p",[e("img",{attrs:{src:a(1196),alt:"Alt text"}})]),s._v(" "),e("ul",[e("li",[s._v("那么回归模型的概率表达式和需要解决的问题为：")])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1197),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("如果p_hat >= 0.5 那么y_hat == 1, 如果p_hat <=0.5 那么y_hat == 0\n\n那么这个问题就转换为：通过给定的数据集X和y，找到参数θ，使得用这样的方式，训练出一个模型，当有新的X样本之后，获得对应的y\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br")])]),e("h3",{attrs:{id:"二-逻辑回归的损失函数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二-逻辑回归的损失函数"}},[s._v("#")]),s._v(" "),e("code",[s._v("(二)逻辑回归的损失函数")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("之前的线性回归的损失函数模型比较简单：\n\tf(x) = 均方误差 MSE\n让MSE尽可能的小，使用梯度下降法寻找θ向量的值即可\n\n但是在逻辑线性回归中，损失函数就没有这么简单了\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br")])]),e("ul",[e("li",[s._v("cost就是损失")])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1198),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t通过分类的结果，对cost进行描述， 这里描述的趋势是通过极端值模拟进行描述的\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("p",[e("img",{attrs:{src:a(1199),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t通过分析cost损失的趋势，就可以通过数学上的函数模型对损失进行模拟，这里可以作为一个改进的损失函数创新点\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("p",[e("img",{attrs:{src:a(1200),alt:"Alt text"}})]),s._v(" "),e("ul",[e("li",[s._v("最后通过推导求得最终损失函数")])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1201),alt:"Alt text"}})]),s._v(" "),e("h3",{attrs:{id:"三-逻辑回归算法实现"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#三-逻辑回归算法实现"}},[s._v("#")]),s._v(" "),e("code",[s._v("(三)逻辑回归算法实现")])]),s._v(" "),e("ul",[e("li",[e("strong",[s._v("损失函数的计算公式")])])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1202),alt:"Alt text"}})]),s._v(" "),e("ul",[e("li",[e("strong",[s._v("损失函数梯度的计算公式")])])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1203),alt:"Alt text"}})]),s._v(" "),e("ul",[e("li",[e("strong",[s._v("实现自己的逻辑回归算法")])])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import numpy as np\nfrom metrics import accuracy_score\n\nclass LogisticRegression:\n    def __init__(self):\n        # 这是系数\n        self.coef_ = None\n        # 这个是截距\n        self.interception_ = None\n        # 这个就是具体计算出来的θ列向量\n        self._theta = None\n\n    # 计算sigmoid的值\n    def _sigmoid(self, t):\n        return 1. / (1. + np.exp(-t))\n\n    # 这里的拟合使用梯度下降方法\n    def fit(self, X_train, y_train, eta=0.01, n_iters=1e4):\n        assert X_train.shape[0] == y_train.shape[0], \\\n        '必须同一维度'\n\n        # todo: 计算损失函数\n        def J(theta, X_b, y):\n            # 计算那个p_hat\n            y_hat = self._sigmoid(X_b.dot(theta))\n\n            try:\n                return np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) / len(y)\n            except Exception as e:\n                return float('inf')\n\n        # todo: 梯度的计算\n        def dJ(theta, X_b, y):\n            return X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) / len(X_b)\n\n        # todo: 梯度下降法\n        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):\n\n            theta = initial_theta\n            i_iter = 0\n            while i_iter < n_iters:\n                gradient = dJ(theta, X_b, y)\n                last_theta = theta\n                theta = theta - eta * gradient\n                if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon:\n                    break\n                i_iter += 1\n            return theta\n\n        X_b = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n        initial_theta = np.zeros(X_b.shape[1])\n        # 最终拿到theta， theta中包括截距和系数\n        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters=n_iters, epsilon=1e-8)\n\n        self.interception_ = self._theta[0]\n        self.coef_ = self._theta[1:]\n        return self\n\n    # 返回结果概率向量\n    def predict_proba(self, X_predict):\n\n        assert self.interception_ is not None and self.coef_ is not None, \\\n        '必须先调用fit_normal方法'\n\n        assert X_predict.shape[1] == len(self.coef_), \\\n        '被预测矩阵的列数必须与系数向量的长度相等'\n\n        # 通过传递进来的X_predict计算新的矩阵X_b\n        X_b = np.hstack([\n            np.ones((X_predict.shape[0], 1)),\n            X_predict,\n        ])\n\n        return self._sigmoid(X_b.dot(self._theta))\n\n\n    # 返回结果分类结果向量\n    def predict(self, X_predict):\n\n        assert self.interception_ is not None and self.coef_ is not None, \\\n        '必须先调用fit_normal方法'\n\n        assert X_predict.shape[1] == len(self.coef_), \\\n        '被预测矩阵的列数必须与系数向量的长度相等'\n\n        # 通过传递进来的X_predict计算新的矩阵X_b\n        proba = self.predict_proba(X_predict)\n\n        # 判断proba是否大于等于0.5， 大于0.5就认为是1\n        # 返回只有0和1组成的向量\n        return np.array(proba >= 0.5, dtype=int)\n\n    # todo: 分类问题使用accuracy_score评测算法的准确度\n    def score(self, X_test, y_test):\n        y_predict = self.predict(X_test)\n        return accuracy_score(y_test, y_predict)\n\n    def __str__(self):\n        return \"LogisticRegression(coef_={}, \" \\\n               \"interception_={}, \" \\\n               \"_theta={})\".format(self.coef_, self.interception_, self._theta)\n\n    __repr__ = __str__\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br"),e("span",{staticClass:"line-number"},[s._v("29")]),e("br"),e("span",{staticClass:"line-number"},[s._v("30")]),e("br"),e("span",{staticClass:"line-number"},[s._v("31")]),e("br"),e("span",{staticClass:"line-number"},[s._v("32")]),e("br"),e("span",{staticClass:"line-number"},[s._v("33")]),e("br"),e("span",{staticClass:"line-number"},[s._v("34")]),e("br"),e("span",{staticClass:"line-number"},[s._v("35")]),e("br"),e("span",{staticClass:"line-number"},[s._v("36")]),e("br"),e("span",{staticClass:"line-number"},[s._v("37")]),e("br"),e("span",{staticClass:"line-number"},[s._v("38")]),e("br"),e("span",{staticClass:"line-number"},[s._v("39")]),e("br"),e("span",{staticClass:"line-number"},[s._v("40")]),e("br"),e("span",{staticClass:"line-number"},[s._v("41")]),e("br"),e("span",{staticClass:"line-number"},[s._v("42")]),e("br"),e("span",{staticClass:"line-number"},[s._v("43")]),e("br"),e("span",{staticClass:"line-number"},[s._v("44")]),e("br"),e("span",{staticClass:"line-number"},[s._v("45")]),e("br"),e("span",{staticClass:"line-number"},[s._v("46")]),e("br"),e("span",{staticClass:"line-number"},[s._v("47")]),e("br"),e("span",{staticClass:"line-number"},[s._v("48")]),e("br"),e("span",{staticClass:"line-number"},[s._v("49")]),e("br"),e("span",{staticClass:"line-number"},[s._v("50")]),e("br"),e("span",{staticClass:"line-number"},[s._v("51")]),e("br"),e("span",{staticClass:"line-number"},[s._v("52")]),e("br"),e("span",{staticClass:"line-number"},[s._v("53")]),e("br"),e("span",{staticClass:"line-number"},[s._v("54")]),e("br"),e("span",{staticClass:"line-number"},[s._v("55")]),e("br"),e("span",{staticClass:"line-number"},[s._v("56")]),e("br"),e("span",{staticClass:"line-number"},[s._v("57")]),e("br"),e("span",{staticClass:"line-number"},[s._v("58")]),e("br"),e("span",{staticClass:"line-number"},[s._v("59")]),e("br"),e("span",{staticClass:"line-number"},[s._v("60")]),e("br"),e("span",{staticClass:"line-number"},[s._v("61")]),e("br"),e("span",{staticClass:"line-number"},[s._v("62")]),e("br"),e("span",{staticClass:"line-number"},[s._v("63")]),e("br"),e("span",{staticClass:"line-number"},[s._v("64")]),e("br"),e("span",{staticClass:"line-number"},[s._v("65")]),e("br"),e("span",{staticClass:"line-number"},[s._v("66")]),e("br"),e("span",{staticClass:"line-number"},[s._v("67")]),e("br"),e("span",{staticClass:"line-number"},[s._v("68")]),e("br"),e("span",{staticClass:"line-number"},[s._v("69")]),e("br"),e("span",{staticClass:"line-number"},[s._v("70")]),e("br"),e("span",{staticClass:"line-number"},[s._v("71")]),e("br"),e("span",{staticClass:"line-number"},[s._v("72")]),e("br"),e("span",{staticClass:"line-number"},[s._v("73")]),e("br"),e("span",{staticClass:"line-number"},[s._v("74")]),e("br"),e("span",{staticClass:"line-number"},[s._v("75")]),e("br"),e("span",{staticClass:"line-number"},[s._v("76")]),e("br"),e("span",{staticClass:"line-number"},[s._v("77")]),e("br"),e("span",{staticClass:"line-number"},[s._v("78")]),e("br"),e("span",{staticClass:"line-number"},[s._v("79")]),e("br"),e("span",{staticClass:"line-number"},[s._v("80")]),e("br"),e("span",{staticClass:"line-number"},[s._v("81")]),e("br"),e("span",{staticClass:"line-number"},[s._v("82")]),e("br"),e("span",{staticClass:"line-number"},[s._v("83")]),e("br"),e("span",{staticClass:"line-number"},[s._v("84")]),e("br"),e("span",{staticClass:"line-number"},[s._v("85")]),e("br"),e("span",{staticClass:"line-number"},[s._v("86")]),e("br"),e("span",{staticClass:"line-number"},[s._v("87")]),e("br"),e("span",{staticClass:"line-number"},[s._v("88")]),e("br"),e("span",{staticClass:"line-number"},[s._v("89")]),e("br"),e("span",{staticClass:"line-number"},[s._v("90")]),e("br"),e("span",{staticClass:"line-number"},[s._v("91")]),e("br"),e("span",{staticClass:"line-number"},[s._v("92")]),e("br"),e("span",{staticClass:"line-number"},[s._v("93")]),e("br"),e("span",{staticClass:"line-number"},[s._v("94")]),e("br"),e("span",{staticClass:"line-number"},[s._v("95")]),e("br"),e("span",{staticClass:"line-number"},[s._v("96")]),e("br"),e("span",{staticClass:"line-number"},[s._v("97")]),e("br"),e("span",{staticClass:"line-number"},[s._v("98")]),e("br"),e("span",{staticClass:"line-number"},[s._v("99")]),e("br"),e("span",{staticClass:"line-number"},[s._v("100")]),e("br"),e("span",{staticClass:"line-number"},[s._v("101")]),e("br"),e("span",{staticClass:"line-number"},[s._v("102")]),e("br"),e("span",{staticClass:"line-number"},[s._v("103")]),e("br")])]),e("ul",[e("li",[e("strong",[s._v("使用iris数据集进行预测")])])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("# 导入数据集\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\niris = datasets.load_iris()\n\n# 获取特征矩阵和特征输出\nX = iris.data\ny = iris.target\n\n#  由于逻辑回归算法解决的是一个二分类的问题，因此只取0，1类别的X样本，且为了后续可视化方便，每个样本的特征也只取2个\nX = X[y<2,:2]\ny = y[y<2]\n\n# 将每一类样本的两个特征绘制为散点图， 可以看出：通过这两个特征其实就很清楚样本的类别了，这个数据集不具有强说服力\nplt.scatter(X[y==0,0], X[y==0, 1], color='r')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b')\nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br")])]),e("p",[s._v("这里是一个分类问题，这里的横纵坐标都是特征，点表示分类的输出结果")]),s._v(" "),e("p",[e("img",{attrs:{src:a(521),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("# 分离数据集\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)\n\n# 调用自己实现了逻辑回归算法\nimport os,sys\nsys.path.insert(0,os.path.join('/Users/yinhuanyi', 'Machine_Learning_Project'))\n\nfrom logisticRegression import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n输出结果：\nLogisticRegression(coef_=[ 3.01749692 -5.03046934], interception_=-0.6827383698993109, _theta=[-0.68273837  3.01749692 -5.03046934])\n\n# 预测结果的准确度\nlog_reg.score(X_test, y_test)\n输出结果：\n100.0\n\n# 查看分类计算的分类结果\nprint(log_reg.predict_proba(X_test))\nprint(log_reg.predict(X_test))\nprint(y_test)\n输出结果：\n[0.93292947 0.98717455 0.15541379 0.01786837 0.03909442 0.01972689\n 0.05214631 0.99683149 0.98092348 0.75469962 0.0473811  0.00362352\n 0.27122595 0.03909442 0.84902103 0.80627393 0.83574223 0.33477608\n 0.06921637 0.21582553 0.0240109  0.1836441  0.98092348 0.98947619\n 0.08342411]\n[1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0]\n[1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0]\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br"),e("span",{staticClass:"line-number"},[s._v("29")]),e("br"),e("span",{staticClass:"line-number"},[s._v("30")]),e("br"),e("span",{staticClass:"line-number"},[s._v("31")]),e("br")])]),e("h3",{attrs:{id:"四-决策边界"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#四-决策边界"}},[s._v("#")]),s._v(" "),e("code",[s._v("(四)决策边界")])]),s._v(" "),e("ul",[e("li",[e("strong",[s._v("回顾逻辑回归分类的原理")])])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1204),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t通过训练的方式求解了一个θ向量，如果样本有N个特征的话，那么θ就有N+1个元素，因为需要拼凑一个截距。每当新来一个样本的时候，计算p_hat的值，如果p_hat>=0.5，那么就是1这个类别，反之为0这个类别\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("p",[e("img",{attrs:{src:a(1205),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("    对于sigmoid函数来说，当t>0时p>0.5, t<0时，p<0.5\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[s._v("综合上述两点来看")])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1206),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t样本的属于1类别还是属于0类别，取决于决策边界θ_T·x_b的值\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[s._v("对于这个**"),e("code",[s._v("决策边界")]),s._v("**来说，如果假设X只有两个特征，那么将决策边界展开，有如下结果，可以将x_1特征来表示x_2特征的值")])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1207),alt:"Alt text"}})]),s._v(" "),e("ul",[e("li",[s._v("通过训练数据，绘制决策边界")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("def x2(x1):\n    return (-log_reg.coef_[0] * x1 - log_reg.interception_) / log_reg.coef_[1]\n\nx1_plot = np.linspace(4, 8, 1000)\nx2_plot = x2(x1_plot)\n\nplt.scatter(X[y==0,0], X[y==0, 1], color='r')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b')\nplt.plot(x1_plot, x2_plot)\nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br")])]),e("p",[e("img",{attrs:{src:a(521),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t从决策边界可以看出来，这里的决策边界是一条直线，直线上面是红色的类别，下面是蓝色类别\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("h3",{attrs:{id:"五-在逻辑回归中使用多项式特征"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#五-在逻辑回归中使用多项式特征"}},[s._v("#")]),s._v(" "),e("code",[s._v("(五)在逻辑回归中使用多项式特征")])]),s._v(" "),e("p",[e("img",{attrs:{src:a(1208),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t例如：决策边界如上图所示，可以使用一个圆形来描述样本的决策边界\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[s._v("使用模拟数据")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(666)\nX = np.random.normal(0, 1, size=(200, 2))\ny = np.array(X[:, 0]**2 + X[:, 1]**2 < 1.5, dtype='int')\nplt.scatter(X[y==0,0], X[y==0, 1], color='y')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b') \nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br")])]),e("p",[e("img",{attrs:{src:a(1209),alt:"Alt text"}})]),s._v(" "),e("ul",[e("li",[s._v("使用逻辑回归进行分类")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("from logisticRegression import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\nlog_reg.score(X, y)\n输出结果：分类的准确率只有60.5%\n60.5\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br")])]),e("ul",[e("li",[s._v("查看逻辑回归的决策边界")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("# 根据算法模型，绘制决策边界\ndef plot_decision_boundary(model, axis):\n    \n    x0, x1 = np.meshgrid(\n        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(), x1.ravel()]\n    \n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    \n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    \n    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)\n\n\nplot_decision_boundary(log_reg, axis=[-4, 4, -4, 4])\nplt.scatter(X[y==0,0], X[y==0, 1], color='y')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b') \nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br")])]),e("p",[e("img",{attrs:{src:a(1210),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t从图可以看到，逻辑回归是一条直线对样本空间进行划分的，显然不对，导致准确率只有60%\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[s._v("在逻辑回归的基础上添加多项式回归模型(这里LogisticRegression这个类是自己实现的，只要自己实现的类满足sklearn的标准，就能够无缝结合)")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\n\n\n# 这里LogisticRegression这个类是自己实现的，只要自己实现的类满足sklearn的标准，就能够无缝结合\ndef PolynormialLogisticRegression(degree):\n    return Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('std_scaler', StandardScaler()),\n        ('log_reg', LogisticRegression()),\n    ])\n\npoly_log_reg = PolynormialLogisticRegression(degree=2)\npoly_log_reg.fit(X, y)\npoly_log_reg.score(X, y)\n输出结果：\n95.0\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br")])]),e("ul",[e("li",[s._v("绘制有多项式算法的逻辑回归的决策边界")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])\nplt.scatter(X[y==0,0], X[y==0, 1], color='y')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b') \nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br")])]),e("p",[e("img",{attrs:{src:a(1211),alt:"Alt text"}})]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t在引入多项式的逻辑回归算法中，决策边界变成了一个圆，准确率为95%\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("h3",{attrs:{id:"六-在逻辑回归中使用正则化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#六-在逻辑回归中使用正则化"}},[s._v("#")]),s._v(" "),e("code",[s._v("(六)在逻辑回归中使用正则化")])]),s._v(" "),e("ul",[e("li",[e("strong",[s._v("多项式模型正则化")])])]),s._v(" "),e("div",{staticClass:"language-python line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[s._v("\t多项式模型容易产生过拟合现象，那么为了解决过拟合问题，一个通常的手段就是使用模型正则化。\n\t模型正则化：对损失函数添加一个新的项，这个新的项可以是L2正则项或L1正则项，最后将这个新的函数作为损失函数进行优化。L正则项包含所有的θ值，a用来调整两边的重要程度\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br")])]),e("ul",[e("li",[e("p",[e("strong",[s._v("逻辑回归模型正则化")])])]),s._v(" "),e("li",[e("p",[s._v("下面看看在sklearn中的逻辑回归如何使用")])])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import numpy as np\nimport matplotlib.pyplot as plt\n\n# 模拟一个二次的逻辑回归数据集\nnp.random.seed(666)\nX = np.random.normal(0, 1, size=(200, 2))\ny = np.array(X[:, 0]**2 + X[:, 1] < 1.5, dtype='int')\n\n# 随机的给这个分类向量中的元素设置1, 添加噪音\nfor _ in range(20):\n    y[np.random.randint(200)] = 1\n\n\nplt.scatter(X[y==0,0], X[y==0, 1], color='y')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b') \nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br")])]),e("ul",[e("li",[s._v("1:分离数据集，且使用sklearn中的逻辑回归进行分类预测")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=666)\n\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\nlog_reg.score(X_train, y_train)\n输出结果：训练数据集的分类准确度\n0.7933333333333333\n\nlog_reg.score(X_test, y_test)\n输出结果：测试数据集的分类结果\n0.86\n\n\n# 使用决策边界画图\ndef plot_decision_boundary(model, axis):\n    \n    x0, x1 = np.meshgrid(\n        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(), x1.ravel()]\n    \n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    \n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    \n    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)\n\n\n# 这条直线就是sklearn中逻辑回归的决策边界\nplot_decision_boundary(log_reg, axis=[-4,4, -4,4])\nplt.scatter(X[y==0,0], X[y==0, 1], color='y')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b') \nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br"),e("span",{staticClass:"line-number"},[s._v("29")]),e("br"),e("span",{staticClass:"line-number"},[s._v("30")]),e("br"),e("span",{staticClass:"line-number"},[s._v("31")]),e("br"),e("span",{staticClass:"line-number"},[s._v("32")]),e("br"),e("span",{staticClass:"line-number"},[s._v("33")]),e("br"),e("span",{staticClass:"line-number"},[s._v("34")]),e("br"),e("span",{staticClass:"line-number"},[s._v("35")]),e("br"),e("span",{staticClass:"line-number"},[s._v("36")]),e("br"),e("span",{staticClass:"line-number"},[s._v("37")]),e("br"),e("span",{staticClass:"line-number"},[s._v("38")]),e("br")])]),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t从决策边界可以看出，分类效果不好，分类边界是一条直线\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[s._v("2：考虑添加多项式模型")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\n\ndef PolynormialLogisticRegression(degree):\n    return Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('std_scaler', StandardScaler()),\n        ('log_reg', LogisticRegression()),\n    ])\n\npoly_log_reg = PolynormialLogisticRegression(degree=2)\npoly_log_reg.fit(X_train, y_train)\npoly_log_reg.score(X_train, y_train)\n输出结果：分类效果较好，原因是父类模型本来就是2次多项式的\n0.9133333333333333\npoly_log_reg.score(X_test, y_test)\n输出结果：分类效果较好，原因是父类模型本来就是2次多项式的\n0.94\n\n# 绘制决策边界\nplot_decision_boundary(poly_log_reg, axis=[-4,4, -4,4])\nplt.scatter(X[y==0,0], X[y==0, 1], color='y')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b') \nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br")])]),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t从决策边界的图形看，基本符合\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[s._v("3：模拟过拟合，增大degree的值")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("poly_log_reg2 = PolynormialLogisticRegression(degree=20)\npoly_log_reg2.fit(X_train, y_train)\npoly_log_reg2.score(X_train, y_train)\n输出结果：\n0.94\npoly_log_reg2.score(X_test, y_test)\n输出结果：测试数据的分类效果降低，原因是因为degree=20，模型过拟合\n0.92\n\n# 绘制过拟合的决策边界\nplot_decision_boundary(poly_log_reg2, axis=[-4,4, -4,4])\nplt.scatter(X[y==0,0], X[y==0, 1], color='y')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b') \nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br")])]),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("这是过拟合的决策边界\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[s._v("4：对逻辑回归模型正则化")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("# 加了一个正则化的超参数C\ndef PolynormialLogisticRegression(degree, C):\n    return Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('std_scaler', StandardScaler()),\n        ('log_reg', LogisticRegression(C=C)),\n    ])\n\n# C为0.1表示让分类准确度的损失函数比重小一些，让正则化对模型的影响多一些\npoly_log_reg3 = PolynormialLogisticRegression(degree=20,C=0.1)\npoly_log_reg3.fit(X_train, y_train)\npoly_log_reg3.score(X_train, y_train)\n输出结果：\n0.8533333333333334\npoly_log_reg3.score(X_test, y_test)\n输出结果：测试数据集的准确度不变\n0.92\n\n# 绘制 过拟合+正则化 的决策边界\nplot_decision_boundary(poly_log_reg3, axis=[-4,4, -4,4])\nplt.scatter(X[y==0,0], X[y==0, 1], color='y')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b') \nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br")])]),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("加入模型正则化，明显比之前的决策边界好\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("ul",[e("li",[s._v("5: 选择正则项为L1,  在sklearn中，默认的正则项为L2")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("def PolynormialLogisticRegression(degree, C, penalty='l2'):\n    return Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('std_scaler', StandardScaler()),\n        ('log_reg', LogisticRegression(C=C, penalty=penalty)),\n    ])\n\n# 这里使用l1进行正则化\npoly_log_reg4 = PolynormialLogisticRegression(degree=20,C=0.1, penalty='l1')\npoly_log_reg4.fit(X_train, y_train)\npoly_log_reg4.score(X_train, y_train)\n输出结果：\n0.8266666666666667\npoly_log_reg4.score(X_test, y_test)\n输出结果：\n0.9\n\n# L1正则项可以基本上将过拟合的影响消除， 查看决策边界\nplot_decision_boundary(poly_log_reg4, axis=[-4,4, -4,4])\nplt.scatter(X[y==0,0], X[y==0, 1], color='y')\nplt.scatter(X[y==1,0], X[y==1, 1], color='b') \nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br")])]),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t因为我们在模拟数据的时候，就知道了多项式的最高次是2。但是对于实际情况下， degree, C, penalty这三个超参数必须使用网格搜索的方式，找到最好的degree, C, penalty超参数\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("h3",{attrs:{id:"七-将二分类改造为多分类-ovr-ovo"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#七-将二分类改造为多分类-ovr-ovo"}},[s._v("#")]),s._v(" "),e("code",[s._v("(七)将二分类改造为多分类(OvR, OvO)")])]),s._v(" "),e("ul",[e("li",[s._v("1：OvR")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t假设有四个类别，那么逻辑回归只能解决二分类问题，如何使用逻辑回归解决呢？\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t可以选择其中一个类别，将其他的类别融合为一体，这样就将4分类问题转换为了2分类问题\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t那么有4个类别就可以形成4个二分类问题\n那么n个类别就进行n次分类，选择分类得分最高的分类，那么我们就可以将这个样本归类于这个类别上。\n\t对于逻辑回归来说，就是判断类别的概率是多少\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br")])]),e("ul",[e("li",[s._v("2：OvO")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t也就是一对一进行比较， 例如只选出红色和蓝色两个类别，判断新的样本属于红色类别还是蓝色类别。\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br")])]),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\t那么有四个这样的类别的话就可以形成6个对，如果有n个类别就要进行C(n,2)次分类，选择赢数最高的分类，作为分类结果\n\tOvO的耗时较长，但是准确度较高\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br")])]),e("ul",[e("li",[s._v("加载数据集, 使用ovr进行多分类")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data[:, :2]\ny = iris.target\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)\n\n# sklearn的逻辑回归算法，已经添加了OVR和OVO功能\nfrom sklearn.linear_model import LogisticRegression\n\n# 使用ovr进行多分类\nlog_reg = LogisticRegression(multi_class='ovr', solver='liblinear')\nlog_reg.fit(X_train, y_train)\nlog_reg.score(X_test, y_test)\n输出结果：\n0.6578947368421053\n\n\ndef plot_decision_boundary(model, axis):\n    \n    x0, x1 = np.meshgrid(\n        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),\n        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)\n    )\n    X_new = np.c_[x0.ravel(), x1.ravel()]\n    \n    y_predict = model.predict(X_new)\n    zz = y_predict.reshape(x0.shape)\n    \n    from matplotlib.colors import ListedColormap\n    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])\n    \n    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)\n\n# 查看决策边界\nplot_decision_boundary(log_reg, axis=[4, 8.5, 1.5, 4.5])\nplt.scatter(X[y==0, 0], X[y==0, 1])\nplt.scatter(X[y==1, 0], X[y==1, 1])\nplt.scatter(X[y==2, 0], X[y==2, 1])\nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br"),e("span",{staticClass:"line-number"},[s._v("15")]),e("br"),e("span",{staticClass:"line-number"},[s._v("16")]),e("br"),e("span",{staticClass:"line-number"},[s._v("17")]),e("br"),e("span",{staticClass:"line-number"},[s._v("18")]),e("br"),e("span",{staticClass:"line-number"},[s._v("19")]),e("br"),e("span",{staticClass:"line-number"},[s._v("20")]),e("br"),e("span",{staticClass:"line-number"},[s._v("21")]),e("br"),e("span",{staticClass:"line-number"},[s._v("22")]),e("br"),e("span",{staticClass:"line-number"},[s._v("23")]),e("br"),e("span",{staticClass:"line-number"},[s._v("24")]),e("br"),e("span",{staticClass:"line-number"},[s._v("25")]),e("br"),e("span",{staticClass:"line-number"},[s._v("26")]),e("br"),e("span",{staticClass:"line-number"},[s._v("27")]),e("br"),e("span",{staticClass:"line-number"},[s._v("28")]),e("br"),e("span",{staticClass:"line-number"},[s._v("29")]),e("br"),e("span",{staticClass:"line-number"},[s._v("30")]),e("br"),e("span",{staticClass:"line-number"},[s._v("31")]),e("br"),e("span",{staticClass:"line-number"},[s._v("32")]),e("br"),e("span",{staticClass:"line-number"},[s._v("33")]),e("br"),e("span",{staticClass:"line-number"},[s._v("34")]),e("br"),e("span",{staticClass:"line-number"},[s._v("35")]),e("br"),e("span",{staticClass:"line-number"},[s._v("36")]),e("br"),e("span",{staticClass:"line-number"},[s._v("37")]),e("br"),e("span",{staticClass:"line-number"},[s._v("38")]),e("br"),e("span",{staticClass:"line-number"},[s._v("39")]),e("br"),e("span",{staticClass:"line-number"},[s._v("40")]),e("br"),e("span",{staticClass:"line-number"},[s._v("41")]),e("br"),e("span",{staticClass:"line-number"},[s._v("42")]),e("br"),e("span",{staticClass:"line-number"},[s._v("43")]),e("br"),e("span",{staticClass:"line-number"},[s._v("44")]),e("br"),e("span",{staticClass:"line-number"},[s._v("45")]),e("br")])]),e("ul",[e("li",[s._v("调用ovo进行多分类")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("# newton-cg：牛顿矩阵算法\nlog_reg2 = LogisticRegression(multi_class='multinomial', solver='newton-cg')\nlog_reg2.fit(X_train,y_train)\nlog_reg2.score(X_test, y_test)\n输出结果：\n0.7894736842105263\n\nplot_decision_boundary(log_reg2, axis=[4, 8.5, 1.5, 4.5])\nplt.scatter(X[y==0, 0], X[y==0, 1])\nplt.scatter(X[y==1, 0], X[y==1, 1])\nplt.scatter(X[y==2, 0], X[y==2, 1])\nplt.show()\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br")])]),e("ul",[e("li",[s._v("使用所有数据进行多分类(ovr和ovo)")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("X = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)\nlog_reg = LogisticRegression(multi_class='ovr', solver='liblinear')\nlog_reg.fit(X_train, y_train)\nlog_reg.score(X_test, y_test)\n输出结果：\n0.9473684210526315\n\nlog_reg = LogisticRegression(multi_class='multinomial', solver='newton-cg')\nlog_reg.fit(X_train, y_train)\nlog_reg.score(X_test, y_test)\n输出结果：\n1.0\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br"),e("span",{staticClass:"line-number"},[s._v("13")]),e("br"),e("span",{staticClass:"line-number"},[s._v("14")]),e("br")])]),e("ul",[e("li",[s._v("其实sklearn还提供了两个类，可以使用这个两个类 + 任何二分类算法完成多分类任务")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\novr = OneVsRestClassifier(log_reg)\novr.fit(X_train, y_train)\novr.score(X_test, y_test)\n输出结果：\n0.9736842105263158\n\novo = OneVsOneClassifier(log_reg)\novo.fit(X_train, y_train)\novo.score(X_test, y_test)\n输出结果：\n1.0\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br"),e("span",{staticClass:"line-number"},[s._v("10")]),e("br"),e("span",{staticClass:"line-number"},[s._v("11")]),e("br"),e("span",{staticClass:"line-number"},[s._v("12")]),e("br")])]),e("ul",[e("li",[s._v("作业")])]),s._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[s._v("\n可以自己完成一个OneVsRestClassifier，和 OneVsOneClassifier\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br")])])])}),[],!1,null,null,null);n.default=t.exports},521:function(s,n,a){s.exports=a.p+"assets/img/1.6af7b8ba.png"}}]);