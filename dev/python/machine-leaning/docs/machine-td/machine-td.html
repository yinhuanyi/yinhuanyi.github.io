<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>梯度下降算法解决多元线性回归问题 | Robby·爱编程</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/img/logo.png">
    <link rel="stylesheet" href="/css/style.css">
    <script charset="utf-8" src="/js/main.js"></script>
    <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?e5c9806c714901413f461d5df50f2512";
          var s = document.getElementsByTagName("script")[0]; 
          s.parentNode.insertBefore(hm, s);
        })();
        </script>
    <meta name="description" content=" ">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/assets/css/0.styles.793b538b.css" as="style"><link rel="preload" href="/assets/js/app.ef2ae1d4.js" as="script"><link rel="preload" href="/assets/js/7.c49e7131.js" as="script"><link rel="preload" href="/assets/js/1.d009776b.js" as="script"><link rel="preload" href="/assets/js/11.452c26e6.js" as="script"><link rel="prefetch" href="/assets/js/10.256d2d9e.js"><link rel="prefetch" href="/assets/js/100.183e29a7.js"><link rel="prefetch" href="/assets/js/101.199d8e46.js"><link rel="prefetch" href="/assets/js/102.b17181c3.js"><link rel="prefetch" href="/assets/js/103.f8859f28.js"><link rel="prefetch" href="/assets/js/104.9fab75ef.js"><link rel="prefetch" href="/assets/js/105.25712a6b.js"><link rel="prefetch" href="/assets/js/106.321faf0e.js"><link rel="prefetch" href="/assets/js/107.edbc3950.js"><link rel="prefetch" href="/assets/js/108.0922d38e.js"><link rel="prefetch" href="/assets/js/109.451e04b9.js"><link rel="prefetch" href="/assets/js/110.f7745add.js"><link rel="prefetch" href="/assets/js/111.82f49a62.js"><link rel="prefetch" href="/assets/js/112.3eca9556.js"><link rel="prefetch" href="/assets/js/113.ef756b43.js"><link rel="prefetch" href="/assets/js/114.a1ffa028.js"><link rel="prefetch" href="/assets/js/115.3ebc5b5f.js"><link rel="prefetch" href="/assets/js/116.68f73bc2.js"><link rel="prefetch" href="/assets/js/117.1913339f.js"><link rel="prefetch" href="/assets/js/118.691d7c30.js"><link rel="prefetch" href="/assets/js/119.b9e4d8c9.js"><link rel="prefetch" href="/assets/js/12.7bb0c6eb.js"><link rel="prefetch" href="/assets/js/120.033a7ce9.js"><link rel="prefetch" href="/assets/js/121.4e17b9cc.js"><link rel="prefetch" href="/assets/js/122.f6342712.js"><link rel="prefetch" href="/assets/js/123.3cc58446.js"><link rel="prefetch" href="/assets/js/124.c19c22e8.js"><link rel="prefetch" href="/assets/js/125.e7ca9035.js"><link rel="prefetch" href="/assets/js/126.95b05ce9.js"><link rel="prefetch" href="/assets/js/127.914d6fb9.js"><link rel="prefetch" href="/assets/js/128.a036f05c.js"><link rel="prefetch" href="/assets/js/129.acd254e3.js"><link rel="prefetch" href="/assets/js/13.742bd49b.js"><link rel="prefetch" href="/assets/js/130.56b0744c.js"><link rel="prefetch" href="/assets/js/131.4ef95683.js"><link rel="prefetch" href="/assets/js/132.fcf72314.js"><link rel="prefetch" href="/assets/js/133.566bd1f7.js"><link rel="prefetch" href="/assets/js/134.a93be345.js"><link rel="prefetch" href="/assets/js/135.a79f211c.js"><link rel="prefetch" href="/assets/js/136.2a6f9b42.js"><link rel="prefetch" href="/assets/js/137.a1acadf1.js"><link rel="prefetch" href="/assets/js/138.624fc52f.js"><link rel="prefetch" href="/assets/js/139.ca7fb473.js"><link rel="prefetch" href="/assets/js/14.5e9f76b2.js"><link rel="prefetch" href="/assets/js/140.696be263.js"><link rel="prefetch" href="/assets/js/141.bb08404f.js"><link rel="prefetch" href="/assets/js/142.b99f910e.js"><link rel="prefetch" href="/assets/js/143.c4bd878c.js"><link rel="prefetch" href="/assets/js/144.21df22c7.js"><link rel="prefetch" href="/assets/js/145.7027e2a4.js"><link rel="prefetch" href="/assets/js/146.2ee3be79.js"><link rel="prefetch" href="/assets/js/147.65727197.js"><link rel="prefetch" href="/assets/js/148.19e9eaee.js"><link rel="prefetch" href="/assets/js/149.c6118e48.js"><link rel="prefetch" href="/assets/js/15.6f107f59.js"><link rel="prefetch" href="/assets/js/150.e2dac6a4.js"><link rel="prefetch" href="/assets/js/151.20292438.js"><link rel="prefetch" href="/assets/js/152.3aa4d6ef.js"><link rel="prefetch" href="/assets/js/153.882d8a36.js"><link rel="prefetch" href="/assets/js/154.f0ab3594.js"><link rel="prefetch" href="/assets/js/155.e2ee68b1.js"><link rel="prefetch" href="/assets/js/156.365860d6.js"><link rel="prefetch" href="/assets/js/157.ece42361.js"><link rel="prefetch" href="/assets/js/158.d809568d.js"><link rel="prefetch" href="/assets/js/159.b5706a52.js"><link rel="prefetch" href="/assets/js/16.39276800.js"><link rel="prefetch" href="/assets/js/160.86946675.js"><link rel="prefetch" href="/assets/js/161.68382dc7.js"><link rel="prefetch" href="/assets/js/162.db9c0697.js"><link rel="prefetch" href="/assets/js/163.cd59e69c.js"><link rel="prefetch" href="/assets/js/164.f439c6b3.js"><link rel="prefetch" href="/assets/js/165.c51b79ba.js"><link rel="prefetch" href="/assets/js/166.a5599c72.js"><link rel="prefetch" href="/assets/js/167.d93d6d09.js"><link rel="prefetch" href="/assets/js/168.c09992d2.js"><link rel="prefetch" href="/assets/js/169.c5fbab48.js"><link rel="prefetch" href="/assets/js/17.f6207881.js"><link rel="prefetch" href="/assets/js/170.e5dde466.js"><link rel="prefetch" href="/assets/js/171.897a5989.js"><link rel="prefetch" href="/assets/js/172.a3fa92f8.js"><link rel="prefetch" href="/assets/js/173.e8417345.js"><link rel="prefetch" href="/assets/js/174.c3e01b8b.js"><link rel="prefetch" href="/assets/js/175.a3cdba2f.js"><link rel="prefetch" href="/assets/js/176.d72805d0.js"><link rel="prefetch" href="/assets/js/177.ebbd2633.js"><link rel="prefetch" href="/assets/js/178.2a27768f.js"><link rel="prefetch" href="/assets/js/179.9286be73.js"><link rel="prefetch" href="/assets/js/18.4ee3074e.js"><link rel="prefetch" href="/assets/js/180.f18cbd55.js"><link rel="prefetch" href="/assets/js/181.a5136d64.js"><link rel="prefetch" href="/assets/js/182.08ce6859.js"><link rel="prefetch" href="/assets/js/183.1dc25b0f.js"><link rel="prefetch" href="/assets/js/184.3c961f5c.js"><link rel="prefetch" href="/assets/js/185.f9f79dff.js"><link rel="prefetch" href="/assets/js/186.28f91be3.js"><link rel="prefetch" href="/assets/js/187.88fa2b8f.js"><link rel="prefetch" href="/assets/js/188.5df2da19.js"><link rel="prefetch" href="/assets/js/189.a794c794.js"><link rel="prefetch" href="/assets/js/19.118ab13e.js"><link rel="prefetch" href="/assets/js/190.7cdec2de.js"><link rel="prefetch" href="/assets/js/191.0297e6cf.js"><link rel="prefetch" href="/assets/js/192.697228d1.js"><link rel="prefetch" href="/assets/js/193.5cb8d43d.js"><link rel="prefetch" href="/assets/js/194.04976a6f.js"><link rel="prefetch" href="/assets/js/195.8d606920.js"><link rel="prefetch" href="/assets/js/196.c37afe55.js"><link rel="prefetch" href="/assets/js/197.2c6d1226.js"><link rel="prefetch" href="/assets/js/198.038c24f2.js"><link rel="prefetch" href="/assets/js/199.ae91bccb.js"><link rel="prefetch" href="/assets/js/20.eb4595e8.js"><link rel="prefetch" href="/assets/js/200.f772507e.js"><link rel="prefetch" href="/assets/js/201.f1d6cf9b.js"><link rel="prefetch" href="/assets/js/202.2ea1d1b9.js"><link rel="prefetch" href="/assets/js/203.125a1a52.js"><link rel="prefetch" href="/assets/js/204.da0a6eb7.js"><link rel="prefetch" href="/assets/js/205.3a31f1d9.js"><link rel="prefetch" href="/assets/js/206.29121567.js"><link rel="prefetch" href="/assets/js/207.2d8fe84f.js"><link rel="prefetch" href="/assets/js/208.2b32fee1.js"><link rel="prefetch" href="/assets/js/209.b95ecdfa.js"><link rel="prefetch" href="/assets/js/21.11c481e1.js"><link rel="prefetch" href="/assets/js/210.b71de55d.js"><link rel="prefetch" href="/assets/js/211.c0b9c70f.js"><link rel="prefetch" href="/assets/js/212.26c1ebdf.js"><link rel="prefetch" href="/assets/js/213.ef52073b.js"><link rel="prefetch" href="/assets/js/214.356048bc.js"><link rel="prefetch" href="/assets/js/215.80bd52a8.js"><link rel="prefetch" href="/assets/js/216.a693de49.js"><link rel="prefetch" href="/assets/js/217.d3f0cb2d.js"><link rel="prefetch" href="/assets/js/218.781c8b07.js"><link rel="prefetch" href="/assets/js/219.4fc65310.js"><link rel="prefetch" href="/assets/js/22.09e52363.js"><link rel="prefetch" href="/assets/js/220.dc918507.js"><link rel="prefetch" href="/assets/js/221.37a9bc26.js"><link rel="prefetch" href="/assets/js/222.14c4ce9a.js"><link rel="prefetch" href="/assets/js/223.79200d95.js"><link rel="prefetch" href="/assets/js/224.b633ba46.js"><link rel="prefetch" href="/assets/js/225.eb4316ed.js"><link rel="prefetch" href="/assets/js/226.f5def5b6.js"><link rel="prefetch" href="/assets/js/227.a30097c7.js"><link rel="prefetch" href="/assets/js/228.08ddff61.js"><link rel="prefetch" href="/assets/js/229.cdf68885.js"><link rel="prefetch" href="/assets/js/23.ad800700.js"><link rel="prefetch" href="/assets/js/230.26d4f9a9.js"><link rel="prefetch" href="/assets/js/231.23da383d.js"><link rel="prefetch" href="/assets/js/232.3de50670.js"><link rel="prefetch" href="/assets/js/233.dcbeedc1.js"><link rel="prefetch" href="/assets/js/234.cedc03a0.js"><link rel="prefetch" href="/assets/js/235.6b91d63a.js"><link rel="prefetch" href="/assets/js/236.082930c3.js"><link rel="prefetch" href="/assets/js/237.71a0d31d.js"><link rel="prefetch" href="/assets/js/238.3b347296.js"><link rel="prefetch" href="/assets/js/239.199b5800.js"><link rel="prefetch" href="/assets/js/24.1a0ec854.js"><link rel="prefetch" href="/assets/js/240.5e7fe51a.js"><link rel="prefetch" href="/assets/js/241.fdccefb6.js"><link rel="prefetch" href="/assets/js/242.1a263fef.js"><link rel="prefetch" href="/assets/js/243.9b76c980.js"><link rel="prefetch" href="/assets/js/244.71a48ed2.js"><link rel="prefetch" href="/assets/js/245.0ca047be.js"><link rel="prefetch" href="/assets/js/246.f492f6e3.js"><link rel="prefetch" href="/assets/js/247.ee9d9380.js"><link rel="prefetch" href="/assets/js/248.dfdf517b.js"><link rel="prefetch" href="/assets/js/249.f3abb8d4.js"><link rel="prefetch" href="/assets/js/25.ebd0fb73.js"><link rel="prefetch" href="/assets/js/250.6ac4cc3a.js"><link rel="prefetch" href="/assets/js/251.6b64e464.js"><link rel="prefetch" href="/assets/js/252.ee2fa61e.js"><link rel="prefetch" href="/assets/js/253.ff810602.js"><link rel="prefetch" href="/assets/js/254.0bde9601.js"><link rel="prefetch" href="/assets/js/255.36b98bd1.js"><link rel="prefetch" href="/assets/js/256.5bcd6250.js"><link rel="prefetch" href="/assets/js/257.c7b560b0.js"><link rel="prefetch" href="/assets/js/258.781f4f39.js"><link rel="prefetch" href="/assets/js/259.4e700996.js"><link rel="prefetch" href="/assets/js/26.bbfde0f8.js"><link rel="prefetch" href="/assets/js/260.f4becfcc.js"><link rel="prefetch" href="/assets/js/261.824eccad.js"><link rel="prefetch" href="/assets/js/262.43d14785.js"><link rel="prefetch" href="/assets/js/263.b69c03ad.js"><link rel="prefetch" href="/assets/js/264.dbc22129.js"><link rel="prefetch" href="/assets/js/265.b5cb48f6.js"><link rel="prefetch" href="/assets/js/266.34356500.js"><link rel="prefetch" href="/assets/js/267.14e0a079.js"><link rel="prefetch" href="/assets/js/268.9ea7c90f.js"><link rel="prefetch" href="/assets/js/269.ff5ce018.js"><link rel="prefetch" href="/assets/js/27.9675606e.js"><link rel="prefetch" href="/assets/js/270.ef200fe2.js"><link rel="prefetch" href="/assets/js/271.4ac62df5.js"><link rel="prefetch" href="/assets/js/272.4d27d446.js"><link rel="prefetch" href="/assets/js/273.9c6efddc.js"><link rel="prefetch" href="/assets/js/274.a3a9c76d.js"><link rel="prefetch" href="/assets/js/275.85c008bc.js"><link rel="prefetch" href="/assets/js/276.ca9b96a1.js"><link rel="prefetch" href="/assets/js/277.ccecfc0f.js"><link rel="prefetch" href="/assets/js/278.c342c4bb.js"><link rel="prefetch" href="/assets/js/279.8ac0e1dc.js"><link rel="prefetch" href="/assets/js/28.01207ecc.js"><link rel="prefetch" href="/assets/js/280.4a8eddcd.js"><link rel="prefetch" href="/assets/js/281.b3bdd051.js"><link rel="prefetch" href="/assets/js/282.291be060.js"><link rel="prefetch" href="/assets/js/283.7065411b.js"><link rel="prefetch" href="/assets/js/284.e93a4e73.js"><link rel="prefetch" href="/assets/js/285.009d797f.js"><link rel="prefetch" href="/assets/js/286.ec26c975.js"><link rel="prefetch" href="/assets/js/287.448073e1.js"><link rel="prefetch" href="/assets/js/288.4207e20b.js"><link rel="prefetch" href="/assets/js/289.53bc96e2.js"><link rel="prefetch" href="/assets/js/29.281eb37f.js"><link rel="prefetch" href="/assets/js/290.30a4fd5c.js"><link rel="prefetch" href="/assets/js/291.2c87a450.js"><link rel="prefetch" href="/assets/js/292.7391f420.js"><link rel="prefetch" href="/assets/js/293.e2c07d98.js"><link rel="prefetch" href="/assets/js/294.69a39f91.js"><link rel="prefetch" href="/assets/js/295.c625045b.js"><link rel="prefetch" href="/assets/js/296.34d89b4e.js"><link rel="prefetch" href="/assets/js/297.064983a6.js"><link rel="prefetch" href="/assets/js/298.023e5d86.js"><link rel="prefetch" href="/assets/js/299.e220fd12.js"><link rel="prefetch" href="/assets/js/3.0616cd46.js"><link rel="prefetch" href="/assets/js/30.e39632a0.js"><link rel="prefetch" href="/assets/js/300.28529b60.js"><link rel="prefetch" href="/assets/js/301.d1438c52.js"><link rel="prefetch" href="/assets/js/302.49a73ebb.js"><link rel="prefetch" href="/assets/js/303.1d69c976.js"><link rel="prefetch" href="/assets/js/304.ba1906c2.js"><link rel="prefetch" href="/assets/js/305.3dc675ac.js"><link rel="prefetch" href="/assets/js/306.4393c939.js"><link rel="prefetch" href="/assets/js/307.8368fb0a.js"><link rel="prefetch" href="/assets/js/308.3115a540.js"><link rel="prefetch" href="/assets/js/309.5833b544.js"><link rel="prefetch" href="/assets/js/31.38b56d14.js"><link rel="prefetch" href="/assets/js/310.53dcfa26.js"><link rel="prefetch" href="/assets/js/311.78c0bfab.js"><link rel="prefetch" href="/assets/js/312.08f0d836.js"><link rel="prefetch" href="/assets/js/313.5519c581.js"><link rel="prefetch" href="/assets/js/314.89323fa8.js"><link rel="prefetch" href="/assets/js/315.5c7d3885.js"><link rel="prefetch" href="/assets/js/316.a9f25bd7.js"><link rel="prefetch" href="/assets/js/317.e2d7a191.js"><link rel="prefetch" href="/assets/js/318.fc1ea4dc.js"><link rel="prefetch" href="/assets/js/319.4b4eaa24.js"><link rel="prefetch" href="/assets/js/32.6f8f7735.js"><link rel="prefetch" href="/assets/js/320.26cadf73.js"><link rel="prefetch" href="/assets/js/321.da66552d.js"><link rel="prefetch" href="/assets/js/322.d1b7e27b.js"><link rel="prefetch" href="/assets/js/323.9518f075.js"><link rel="prefetch" href="/assets/js/324.1bed89f2.js"><link rel="prefetch" href="/assets/js/325.74a10b7f.js"><link rel="prefetch" href="/assets/js/326.6733614c.js"><link rel="prefetch" href="/assets/js/327.2ddf9fe2.js"><link rel="prefetch" href="/assets/js/328.79dc0b15.js"><link rel="prefetch" href="/assets/js/329.d39f716c.js"><link rel="prefetch" href="/assets/js/33.c1604104.js"><link rel="prefetch" href="/assets/js/330.32c96058.js"><link rel="prefetch" href="/assets/js/331.405b6757.js"><link rel="prefetch" href="/assets/js/332.537134ee.js"><link rel="prefetch" href="/assets/js/333.81689199.js"><link rel="prefetch" href="/assets/js/334.f4f1e806.js"><link rel="prefetch" href="/assets/js/335.51f4a3fd.js"><link rel="prefetch" href="/assets/js/336.360d22f4.js"><link rel="prefetch" href="/assets/js/337.95c52f93.js"><link rel="prefetch" href="/assets/js/338.824f01ee.js"><link rel="prefetch" href="/assets/js/339.8e0c18d7.js"><link rel="prefetch" href="/assets/js/34.61042921.js"><link rel="prefetch" href="/assets/js/340.a666422a.js"><link rel="prefetch" href="/assets/js/341.8061f90b.js"><link rel="prefetch" href="/assets/js/342.5ecffedd.js"><link rel="prefetch" href="/assets/js/343.d67c1d54.js"><link rel="prefetch" href="/assets/js/344.4da6841e.js"><link rel="prefetch" href="/assets/js/345.87c72db5.js"><link rel="prefetch" href="/assets/js/346.2796487d.js"><link rel="prefetch" href="/assets/js/347.57b5c6a5.js"><link rel="prefetch" href="/assets/js/348.0cffac81.js"><link rel="prefetch" href="/assets/js/349.8e908833.js"><link rel="prefetch" href="/assets/js/35.1d574a74.js"><link rel="prefetch" href="/assets/js/350.1f5e32d7.js"><link rel="prefetch" href="/assets/js/351.4a655e93.js"><link rel="prefetch" href="/assets/js/352.2ffd5646.js"><link rel="prefetch" href="/assets/js/353.4ffbf66f.js"><link rel="prefetch" href="/assets/js/354.8fffb5f9.js"><link rel="prefetch" href="/assets/js/355.21de33fa.js"><link rel="prefetch" href="/assets/js/356.3d20f95a.js"><link rel="prefetch" href="/assets/js/357.2612cca5.js"><link rel="prefetch" href="/assets/js/358.7cce50cb.js"><link rel="prefetch" href="/assets/js/359.c23b92e5.js"><link rel="prefetch" href="/assets/js/36.66b33e93.js"><link rel="prefetch" href="/assets/js/360.6e33c30f.js"><link rel="prefetch" href="/assets/js/361.6db53217.js"><link rel="prefetch" href="/assets/js/362.28e15494.js"><link rel="prefetch" href="/assets/js/363.5316eb9a.js"><link rel="prefetch" href="/assets/js/364.ae04b75f.js"><link rel="prefetch" href="/assets/js/365.64c1e88f.js"><link rel="prefetch" href="/assets/js/366.066b98bd.js"><link rel="prefetch" href="/assets/js/367.06e05293.js"><link rel="prefetch" href="/assets/js/368.82209659.js"><link rel="prefetch" href="/assets/js/369.d687a4ee.js"><link rel="prefetch" href="/assets/js/37.18a4caf1.js"><link rel="prefetch" href="/assets/js/370.02d4e333.js"><link rel="prefetch" href="/assets/js/371.9e35695d.js"><link rel="prefetch" href="/assets/js/372.de59ddf8.js"><link rel="prefetch" href="/assets/js/373.7cfca530.js"><link rel="prefetch" href="/assets/js/374.d466d79e.js"><link rel="prefetch" href="/assets/js/375.07a95ecc.js"><link rel="prefetch" href="/assets/js/376.0859df9f.js"><link rel="prefetch" href="/assets/js/377.241c4721.js"><link rel="prefetch" href="/assets/js/378.a2d485d0.js"><link rel="prefetch" href="/assets/js/379.bfe54af9.js"><link rel="prefetch" href="/assets/js/38.c7cc8ff1.js"><link rel="prefetch" href="/assets/js/380.886e2c5c.js"><link rel="prefetch" href="/assets/js/381.514ea971.js"><link rel="prefetch" href="/assets/js/382.9bd440e8.js"><link rel="prefetch" href="/assets/js/383.cda3a324.js"><link rel="prefetch" href="/assets/js/384.6cfe10eb.js"><link rel="prefetch" href="/assets/js/385.3ebff401.js"><link rel="prefetch" href="/assets/js/386.7125d75d.js"><link rel="prefetch" href="/assets/js/387.472cd823.js"><link rel="prefetch" href="/assets/js/388.235ad655.js"><link rel="prefetch" href="/assets/js/389.cb8229cf.js"><link rel="prefetch" href="/assets/js/39.c156642a.js"><link rel="prefetch" href="/assets/js/390.b25b016f.js"><link rel="prefetch" href="/assets/js/391.956c8e39.js"><link rel="prefetch" href="/assets/js/392.86acebaf.js"><link rel="prefetch" href="/assets/js/393.f5e26c18.js"><link rel="prefetch" href="/assets/js/394.ceb6bc2c.js"><link rel="prefetch" href="/assets/js/395.8478f3f6.js"><link rel="prefetch" href="/assets/js/396.ee8abf24.js"><link rel="prefetch" href="/assets/js/397.0a467034.js"><link rel="prefetch" href="/assets/js/398.d83e363b.js"><link rel="prefetch" href="/assets/js/399.a800f7df.js"><link rel="prefetch" href="/assets/js/4.ff0d0983.js"><link rel="prefetch" href="/assets/js/40.d21ba833.js"><link rel="prefetch" href="/assets/js/400.567a684b.js"><link rel="prefetch" href="/assets/js/401.04724ff3.js"><link rel="prefetch" href="/assets/js/402.e2a01c0d.js"><link rel="prefetch" href="/assets/js/403.4c059b3b.js"><link rel="prefetch" href="/assets/js/404.0b33c4fc.js"><link rel="prefetch" href="/assets/js/405.31f9f4d4.js"><link rel="prefetch" href="/assets/js/406.0ea31eb3.js"><link rel="prefetch" href="/assets/js/407.f8aeaf6a.js"><link rel="prefetch" href="/assets/js/408.e31d8e6d.js"><link rel="prefetch" href="/assets/js/409.527b15a8.js"><link rel="prefetch" href="/assets/js/41.e327929c.js"><link rel="prefetch" href="/assets/js/410.ab3891e5.js"><link rel="prefetch" href="/assets/js/411.cabd9e6f.js"><link rel="prefetch" href="/assets/js/412.c00feda1.js"><link rel="prefetch" href="/assets/js/413.da9d8d0d.js"><link rel="prefetch" href="/assets/js/414.86e019be.js"><link rel="prefetch" href="/assets/js/415.024f200c.js"><link rel="prefetch" href="/assets/js/416.7a7f201a.js"><link rel="prefetch" href="/assets/js/417.9039f0f5.js"><link rel="prefetch" href="/assets/js/418.d0af7314.js"><link rel="prefetch" href="/assets/js/419.a7880b08.js"><link rel="prefetch" href="/assets/js/42.766a1ebb.js"><link rel="prefetch" href="/assets/js/420.e143149b.js"><link rel="prefetch" href="/assets/js/421.ce227e9b.js"><link rel="prefetch" href="/assets/js/422.5af358c3.js"><link rel="prefetch" href="/assets/js/423.b72620fc.js"><link rel="prefetch" href="/assets/js/424.0fffeb28.js"><link rel="prefetch" href="/assets/js/425.40613664.js"><link rel="prefetch" href="/assets/js/426.2e22eebf.js"><link rel="prefetch" href="/assets/js/427.a553f7d4.js"><link rel="prefetch" href="/assets/js/428.76aab144.js"><link rel="prefetch" href="/assets/js/429.3f7541a4.js"><link rel="prefetch" href="/assets/js/43.bd29747c.js"><link rel="prefetch" href="/assets/js/430.499095ce.js"><link rel="prefetch" href="/assets/js/431.8ab3c62f.js"><link rel="prefetch" href="/assets/js/432.ba138351.js"><link rel="prefetch" href="/assets/js/433.17cccc55.js"><link rel="prefetch" href="/assets/js/434.048e0058.js"><link rel="prefetch" href="/assets/js/435.b7f62b08.js"><link rel="prefetch" href="/assets/js/436.ddd5bc6a.js"><link rel="prefetch" href="/assets/js/437.7fc374a2.js"><link rel="prefetch" href="/assets/js/438.91febbb0.js"><link rel="prefetch" href="/assets/js/439.3bdc32c2.js"><link rel="prefetch" href="/assets/js/44.159635cf.js"><link rel="prefetch" href="/assets/js/440.4c502b0c.js"><link rel="prefetch" href="/assets/js/441.f720ea4c.js"><link rel="prefetch" href="/assets/js/442.a0317b88.js"><link rel="prefetch" href="/assets/js/443.b9c86fb4.js"><link rel="prefetch" href="/assets/js/444.991c700e.js"><link rel="prefetch" href="/assets/js/445.eebf3c94.js"><link rel="prefetch" href="/assets/js/446.baf1e6b5.js"><link rel="prefetch" href="/assets/js/447.36441c47.js"><link rel="prefetch" href="/assets/js/448.3757ac1c.js"><link rel="prefetch" href="/assets/js/449.a62b518b.js"><link rel="prefetch" href="/assets/js/45.8cb3fc60.js"><link rel="prefetch" href="/assets/js/450.88b22b7d.js"><link rel="prefetch" href="/assets/js/451.e543e021.js"><link rel="prefetch" href="/assets/js/452.23d29581.js"><link rel="prefetch" href="/assets/js/453.4cefab78.js"><link rel="prefetch" href="/assets/js/454.505893ff.js"><link rel="prefetch" href="/assets/js/455.4be47b6a.js"><link rel="prefetch" href="/assets/js/456.34067054.js"><link rel="prefetch" href="/assets/js/457.f0bac5b8.js"><link rel="prefetch" href="/assets/js/458.9f72dafe.js"><link rel="prefetch" href="/assets/js/459.298e4e26.js"><link rel="prefetch" href="/assets/js/46.54e218d9.js"><link rel="prefetch" href="/assets/js/460.875ebcb2.js"><link rel="prefetch" href="/assets/js/461.2e995131.js"><link rel="prefetch" href="/assets/js/462.93690745.js"><link rel="prefetch" href="/assets/js/463.c13b2bec.js"><link rel="prefetch" href="/assets/js/464.bf750fb8.js"><link rel="prefetch" href="/assets/js/465.81c60d5d.js"><link rel="prefetch" href="/assets/js/466.a61d596e.js"><link rel="prefetch" href="/assets/js/467.3842176d.js"><link rel="prefetch" href="/assets/js/468.63c7bcdd.js"><link rel="prefetch" href="/assets/js/469.75d9181a.js"><link rel="prefetch" href="/assets/js/47.f17d1df2.js"><link rel="prefetch" href="/assets/js/470.e5676a3b.js"><link rel="prefetch" href="/assets/js/471.6277f541.js"><link rel="prefetch" href="/assets/js/472.125e562f.js"><link rel="prefetch" href="/assets/js/473.39c8df5e.js"><link rel="prefetch" href="/assets/js/474.92a7814b.js"><link rel="prefetch" href="/assets/js/475.783a6e1b.js"><link rel="prefetch" href="/assets/js/476.13ab38fd.js"><link rel="prefetch" href="/assets/js/477.5c6cbe44.js"><link rel="prefetch" href="/assets/js/478.4dfe6494.js"><link rel="prefetch" href="/assets/js/479.4c41ba37.js"><link rel="prefetch" href="/assets/js/48.7e42f465.js"><link rel="prefetch" href="/assets/js/480.fc824c74.js"><link rel="prefetch" href="/assets/js/481.c15ba6c9.js"><link rel="prefetch" href="/assets/js/482.e930790a.js"><link rel="prefetch" href="/assets/js/483.0f102a04.js"><link rel="prefetch" href="/assets/js/484.4717857e.js"><link rel="prefetch" href="/assets/js/485.a935adb5.js"><link rel="prefetch" href="/assets/js/486.6984e85f.js"><link rel="prefetch" href="/assets/js/487.39c3b9b4.js"><link rel="prefetch" href="/assets/js/488.50f3044f.js"><link rel="prefetch" href="/assets/js/489.a95486d4.js"><link rel="prefetch" href="/assets/js/49.f67f18f2.js"><link rel="prefetch" href="/assets/js/490.b3795263.js"><link rel="prefetch" href="/assets/js/491.2863d479.js"><link rel="prefetch" href="/assets/js/492.fb97316c.js"><link rel="prefetch" href="/assets/js/493.fbd42d21.js"><link rel="prefetch" href="/assets/js/494.d49dbddc.js"><link rel="prefetch" href="/assets/js/495.960d7a7a.js"><link rel="prefetch" href="/assets/js/496.067e2f8a.js"><link rel="prefetch" href="/assets/js/497.8892200d.js"><link rel="prefetch" href="/assets/js/498.f370eeef.js"><link rel="prefetch" href="/assets/js/499.9a73037b.js"><link rel="prefetch" href="/assets/js/5.abc9632b.js"><link rel="prefetch" href="/assets/js/50.f653d3eb.js"><link rel="prefetch" href="/assets/js/500.3908db61.js"><link rel="prefetch" href="/assets/js/501.268d26f2.js"><link rel="prefetch" href="/assets/js/502.a0b06ceb.js"><link rel="prefetch" href="/assets/js/503.e58b9748.js"><link rel="prefetch" href="/assets/js/504.1677e8f8.js"><link rel="prefetch" href="/assets/js/505.ea1a1845.js"><link rel="prefetch" href="/assets/js/506.148ab5d9.js"><link rel="prefetch" href="/assets/js/507.757d384a.js"><link rel="prefetch" href="/assets/js/508.3233323c.js"><link rel="prefetch" href="/assets/js/509.cd881c95.js"><link rel="prefetch" href="/assets/js/51.60ef697c.js"><link rel="prefetch" href="/assets/js/510.693e3154.js"><link rel="prefetch" href="/assets/js/511.0726668e.js"><link rel="prefetch" href="/assets/js/512.544f9351.js"><link rel="prefetch" href="/assets/js/513.6b74fe01.js"><link rel="prefetch" href="/assets/js/514.1ee3b99e.js"><link rel="prefetch" href="/assets/js/515.ae2cfef3.js"><link rel="prefetch" href="/assets/js/516.332ac4f7.js"><link rel="prefetch" href="/assets/js/517.e29023be.js"><link rel="prefetch" href="/assets/js/518.925058af.js"><link rel="prefetch" href="/assets/js/519.ce2bd6da.js"><link rel="prefetch" href="/assets/js/52.037d8e07.js"><link rel="prefetch" href="/assets/js/520.017b325c.js"><link rel="prefetch" href="/assets/js/521.58a6306c.js"><link rel="prefetch" href="/assets/js/522.a40f2287.js"><link rel="prefetch" href="/assets/js/523.edb81dd7.js"><link rel="prefetch" href="/assets/js/524.16258545.js"><link rel="prefetch" href="/assets/js/525.4bcc1c02.js"><link rel="prefetch" href="/assets/js/526.c355d6fc.js"><link rel="prefetch" href="/assets/js/527.7c3b7161.js"><link rel="prefetch" href="/assets/js/528.ac47d7da.js"><link rel="prefetch" href="/assets/js/529.3e5e6bc0.js"><link rel="prefetch" href="/assets/js/53.d65bef48.js"><link rel="prefetch" href="/assets/js/530.13d1b1c6.js"><link rel="prefetch" href="/assets/js/531.04e2e897.js"><link rel="prefetch" href="/assets/js/532.533d810f.js"><link rel="prefetch" href="/assets/js/533.2bd83146.js"><link rel="prefetch" href="/assets/js/534.cd16ff61.js"><link rel="prefetch" href="/assets/js/535.f75b00e3.js"><link rel="prefetch" href="/assets/js/536.7824cd53.js"><link rel="prefetch" href="/assets/js/537.c578cfb6.js"><link rel="prefetch" href="/assets/js/538.911cc914.js"><link rel="prefetch" href="/assets/js/539.2de30ddc.js"><link rel="prefetch" href="/assets/js/54.16da8f26.js"><link rel="prefetch" href="/assets/js/540.1cdaa436.js"><link rel="prefetch" href="/assets/js/541.1046697c.js"><link rel="prefetch" href="/assets/js/542.ef1092c7.js"><link rel="prefetch" href="/assets/js/543.54235171.js"><link rel="prefetch" href="/assets/js/544.e4d63122.js"><link rel="prefetch" href="/assets/js/545.6983d416.js"><link rel="prefetch" href="/assets/js/546.98a57513.js"><link rel="prefetch" href="/assets/js/547.8c0ea7a4.js"><link rel="prefetch" href="/assets/js/548.b2e5147f.js"><link rel="prefetch" href="/assets/js/549.110d8ccc.js"><link rel="prefetch" href="/assets/js/55.994be3d0.js"><link rel="prefetch" href="/assets/js/550.b30b0650.js"><link rel="prefetch" href="/assets/js/551.4038a02e.js"><link rel="prefetch" href="/assets/js/552.4ce5afbb.js"><link rel="prefetch" href="/assets/js/553.6156a64c.js"><link rel="prefetch" href="/assets/js/554.0058fb25.js"><link rel="prefetch" href="/assets/js/555.5e22fac0.js"><link rel="prefetch" href="/assets/js/556.e37be811.js"><link rel="prefetch" href="/assets/js/557.9ef5c296.js"><link rel="prefetch" href="/assets/js/558.117e05db.js"><link rel="prefetch" href="/assets/js/559.78e2830c.js"><link rel="prefetch" href="/assets/js/56.a07a348f.js"><link rel="prefetch" href="/assets/js/560.f1afe080.js"><link rel="prefetch" href="/assets/js/561.4b1b4b8c.js"><link rel="prefetch" href="/assets/js/562.6ff741ad.js"><link rel="prefetch" href="/assets/js/563.10fe6323.js"><link rel="prefetch" href="/assets/js/564.23bc7ae3.js"><link rel="prefetch" href="/assets/js/565.99045d53.js"><link rel="prefetch" href="/assets/js/566.e0e8a181.js"><link rel="prefetch" href="/assets/js/567.72cd3a82.js"><link rel="prefetch" href="/assets/js/568.cc37a022.js"><link rel="prefetch" href="/assets/js/569.0b2d095f.js"><link rel="prefetch" href="/assets/js/57.d8914610.js"><link rel="prefetch" href="/assets/js/570.c076c67b.js"><link rel="prefetch" href="/assets/js/571.8e5c59b6.js"><link rel="prefetch" href="/assets/js/572.5a60007c.js"><link rel="prefetch" href="/assets/js/573.f0e2a0f5.js"><link rel="prefetch" href="/assets/js/574.fffad53b.js"><link rel="prefetch" href="/assets/js/575.4297f78b.js"><link rel="prefetch" href="/assets/js/576.cb9de798.js"><link rel="prefetch" href="/assets/js/577.c6c660be.js"><link rel="prefetch" href="/assets/js/578.22177af2.js"><link rel="prefetch" href="/assets/js/579.3824ee1c.js"><link rel="prefetch" href="/assets/js/58.02da9451.js"><link rel="prefetch" href="/assets/js/580.7e4d3cb7.js"><link rel="prefetch" href="/assets/js/581.21bed6e7.js"><link rel="prefetch" href="/assets/js/582.c94227c4.js"><link rel="prefetch" href="/assets/js/583.f0611b7e.js"><link rel="prefetch" href="/assets/js/584.809f0bbb.js"><link rel="prefetch" href="/assets/js/585.ed41b413.js"><link rel="prefetch" href="/assets/js/586.957fe496.js"><link rel="prefetch" href="/assets/js/587.ff5d8d3c.js"><link rel="prefetch" href="/assets/js/588.c1f2b2fa.js"><link rel="prefetch" href="/assets/js/589.7feb7eb3.js"><link rel="prefetch" href="/assets/js/59.275b0293.js"><link rel="prefetch" href="/assets/js/590.6ee9e42d.js"><link rel="prefetch" href="/assets/js/591.d2c873ee.js"><link rel="prefetch" href="/assets/js/592.63d5541c.js"><link rel="prefetch" href="/assets/js/593.1b5910fa.js"><link rel="prefetch" href="/assets/js/594.a9e7a407.js"><link rel="prefetch" href="/assets/js/595.8ae0a32e.js"><link rel="prefetch" href="/assets/js/596.93454a11.js"><link rel="prefetch" href="/assets/js/597.e92d65a6.js"><link rel="prefetch" href="/assets/js/598.05057c9f.js"><link rel="prefetch" href="/assets/js/599.534ff18f.js"><link rel="prefetch" href="/assets/js/6.0c96ad77.js"><link rel="prefetch" href="/assets/js/60.77f5b873.js"><link rel="prefetch" href="/assets/js/600.222db31a.js"><link rel="prefetch" href="/assets/js/601.23dcd251.js"><link rel="prefetch" href="/assets/js/602.64cf1e80.js"><link rel="prefetch" href="/assets/js/603.3d7ae82f.js"><link rel="prefetch" href="/assets/js/604.f3055453.js"><link rel="prefetch" href="/assets/js/605.af0c8252.js"><link rel="prefetch" href="/assets/js/606.fad28edc.js"><link rel="prefetch" href="/assets/js/607.974b78f2.js"><link rel="prefetch" href="/assets/js/608.26462e83.js"><link rel="prefetch" href="/assets/js/609.dd27e631.js"><link rel="prefetch" href="/assets/js/61.eaf63186.js"><link rel="prefetch" href="/assets/js/610.000c1e9d.js"><link rel="prefetch" href="/assets/js/611.44ff14d4.js"><link rel="prefetch" href="/assets/js/612.5b4036a0.js"><link rel="prefetch" href="/assets/js/613.3b71b5a2.js"><link rel="prefetch" href="/assets/js/614.28925cbf.js"><link rel="prefetch" href="/assets/js/615.a9e93388.js"><link rel="prefetch" href="/assets/js/616.f6f1bae0.js"><link rel="prefetch" href="/assets/js/617.31662991.js"><link rel="prefetch" href="/assets/js/618.6d9dcf9c.js"><link rel="prefetch" href="/assets/js/619.d2833531.js"><link rel="prefetch" href="/assets/js/62.7460f830.js"><link rel="prefetch" href="/assets/js/620.3ccbca1a.js"><link rel="prefetch" href="/assets/js/621.8109ad4e.js"><link rel="prefetch" href="/assets/js/622.06a5babf.js"><link rel="prefetch" href="/assets/js/623.ccf364a6.js"><link rel="prefetch" href="/assets/js/624.18209fc2.js"><link rel="prefetch" href="/assets/js/625.eb3f309d.js"><link rel="prefetch" href="/assets/js/626.73701a4d.js"><link rel="prefetch" href="/assets/js/627.aa3ae061.js"><link rel="prefetch" href="/assets/js/628.af3ee1ec.js"><link rel="prefetch" href="/assets/js/629.db9ed29f.js"><link rel="prefetch" href="/assets/js/63.18441910.js"><link rel="prefetch" href="/assets/js/630.f8552446.js"><link rel="prefetch" href="/assets/js/631.e9b80c2c.js"><link rel="prefetch" href="/assets/js/632.ca268728.js"><link rel="prefetch" href="/assets/js/633.e5bcf026.js"><link rel="prefetch" href="/assets/js/634.8ec52643.js"><link rel="prefetch" href="/assets/js/635.cae0b43a.js"><link rel="prefetch" href="/assets/js/636.eb23b5c5.js"><link rel="prefetch" href="/assets/js/637.eee47171.js"><link rel="prefetch" href="/assets/js/638.8dbb86f2.js"><link rel="prefetch" href="/assets/js/639.f3dc3d54.js"><link rel="prefetch" href="/assets/js/64.9bcd2f77.js"><link rel="prefetch" href="/assets/js/640.a02db32d.js"><link rel="prefetch" href="/assets/js/641.25990053.js"><link rel="prefetch" href="/assets/js/642.caab5dfd.js"><link rel="prefetch" href="/assets/js/643.becf6b9f.js"><link rel="prefetch" href="/assets/js/644.ac752ef6.js"><link rel="prefetch" href="/assets/js/645.cf92857c.js"><link rel="prefetch" href="/assets/js/646.d4646b84.js"><link rel="prefetch" href="/assets/js/647.b1895af0.js"><link rel="prefetch" href="/assets/js/648.c26381da.js"><link rel="prefetch" href="/assets/js/649.c0818833.js"><link rel="prefetch" href="/assets/js/65.5ee94aae.js"><link rel="prefetch" href="/assets/js/650.8d4d92d6.js"><link rel="prefetch" href="/assets/js/651.4ff5f780.js"><link rel="prefetch" href="/assets/js/66.f1b27e87.js"><link rel="prefetch" href="/assets/js/67.47082498.js"><link rel="prefetch" href="/assets/js/68.502aa70c.js"><link rel="prefetch" href="/assets/js/69.9a3a9a86.js"><link rel="prefetch" href="/assets/js/70.1a8828f1.js"><link rel="prefetch" href="/assets/js/71.d32d3acb.js"><link rel="prefetch" href="/assets/js/72.0d13e6d5.js"><link rel="prefetch" href="/assets/js/73.be72ee5e.js"><link rel="prefetch" href="/assets/js/74.00d2de22.js"><link rel="prefetch" href="/assets/js/75.1450e671.js"><link rel="prefetch" href="/assets/js/76.f41837bd.js"><link rel="prefetch" href="/assets/js/77.d5222458.js"><link rel="prefetch" href="/assets/js/78.630fb592.js"><link rel="prefetch" href="/assets/js/79.e5277cab.js"><link rel="prefetch" href="/assets/js/8.9176dd4b.js"><link rel="prefetch" href="/assets/js/80.d33b4c24.js"><link rel="prefetch" href="/assets/js/81.e3349db5.js"><link rel="prefetch" href="/assets/js/82.304e56e7.js"><link rel="prefetch" href="/assets/js/83.886030fc.js"><link rel="prefetch" href="/assets/js/84.3d1dd526.js"><link rel="prefetch" href="/assets/js/85.9f2d1ef1.js"><link rel="prefetch" href="/assets/js/86.af375a15.js"><link rel="prefetch" href="/assets/js/87.0d02b425.js"><link rel="prefetch" href="/assets/js/88.8e3d0899.js"><link rel="prefetch" href="/assets/js/89.414af620.js"><link rel="prefetch" href="/assets/js/9.5a6081e3.js"><link rel="prefetch" href="/assets/js/90.e20539a2.js"><link rel="prefetch" href="/assets/js/91.e9e57bd4.js"><link rel="prefetch" href="/assets/js/92.9b5cf84f.js"><link rel="prefetch" href="/assets/js/93.366136f2.js"><link rel="prefetch" href="/assets/js/94.524b1442.js"><link rel="prefetch" href="/assets/js/95.8b4907f1.js"><link rel="prefetch" href="/assets/js/96.eb5ca57b.js"><link rel="prefetch" href="/assets/js/97.912c0f6a.js"><link rel="prefetch" href="/assets/js/98.825483b2.js"><link rel="prefetch" href="/assets/js/99.ae4d5edd.js">
    <link rel="stylesheet" href="/assets/css/0.styles.793b538b.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-1aefc0b4><div data-v-1aefc0b4><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1aefc0b4 data-v-1aefc0b4><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-25ba6db2 data-v-1aefc0b4 data-v-1aefc0b4><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Robby·爱编程</h3> <p class="description" data-v-25ba6db2 data-v-25ba6db2> </p> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>尹欢一的技术博客</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div class="hide" data-v-1aefc0b4><header class="navbar" data-v-1aefc0b4><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/nav-logo.png" alt="Robby·爱编程" class="logo"> <span class="site-name">Robby·爱编程</span></a> <div class="links"><!----> <div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-menu"></i>
      内功心法
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/theory/algorithm/" class="nav-link"><i class="undefined"></i>
  算法与数据结构(Golang版本)
</a></li><li class="dropdown-item"><!----> <a href="/theory/algorithmic-thought/" class="nav-link"><i class="undefined"></i>
  算法思想分类
</a></li><li class="dropdown-item"><!----> <a href="/theory/network/" class="nav-link"><i class="undefined"></i>
  计算机网络
</a></li><li class="dropdown-item"><!----> <a href="/theory/system/" class="nav-link"><i class="undefined"></i>
  操作系统
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-document"></i>
      编程开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>Python开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/python/basic/" class="nav-link"><i class="undefined"></i>
  Python开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/python/machine-leaning/" class="nav-link router-link-active"><i class="undefined"></i>
  Python机器学习
</a></li><li class="dropdown-subitem"><a href="/dev/python/third-lib/" class="nav-link"><i class="undefined"></i>
  Python第三方库
</a></li><li class="dropdown-subitem"><a href="/dev/python/scrapy/" class="nav-link"><i class="undefined"></i>
  Python爬虫
</a></li><li class="dropdown-subitem"><a href="/dev/python/deefuture/" class="nav-link"><i class="undefined"></i>
  Python Deefuture教育平台
</a></li><li class="dropdown-subitem"><a href="/dev/python/shop/" class="nav-link"><i class="undefined"></i>
  Python电商平台
</a></li><li class="dropdown-subitem"><a href="/dev/python/coroutine/" class="nav-link"><i class="undefined"></i>
  Python协程开发
</a></li><li class="dropdown-subitem"><a href="/dev/python/tornado/" class="nav-link"><i class="undefined"></i>
  Python Tornado开发
</a></li><li class="dropdown-subitem"><a href="/dev/python/third/" class="nav-link"><i class="undefined"></i>
  Python第三方包
</a></li><li class="dropdown-subitem"><a href="/dev/python/sqlalchemy/" class="nav-link"><i class="undefined"></i>
  Python SQLAlchemy
</a></li></ul></li><li class="dropdown-item"><h4>Golang开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/golang/basic/" class="nav-link"><i class="undefined"></i>
  Golang开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/golang/package/" class="nav-link"><i class="undefined"></i>
  Golang第三方包
</a></li><li class="dropdown-subitem"><a href="/dev/golang/source/" class="nav-link"><i class="undefined"></i>
  Golang源码分析
</a></li><li class="dropdown-subitem"><a href="/dev/golang/kill/" class="nav-link"><i class="undefined"></i>
  Golang秒杀系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/task/" class="nav-link"><i class="undefined"></i>
  Golang任务系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/post/" class="nav-link"><i class="undefined"></i>
  Golang帖子系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/network/" class="nav-link"><i class="undefined"></i>
  Golang网关系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/advance/" class="nav-link"><i class="undefined"></i>
  Golang进阶开发
</a></li><li class="dropdown-subitem"><a href="/dev/golang/problem/" class="nav-link"><i class="undefined"></i>
  Golang问题集合
</a></li></ul></li><li class="dropdown-item"><h4>Shell开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/shell/basic/" class="nav-link"><i class="undefined"></i>
  Shell开发基础
</a></li></ul></li><li class="dropdown-item"><h4>Vue开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/vue/layui/" class="nav-link"><i class="undefined"></i>
  Layui开发
</a></li><li class="dropdown-subitem"><a href="/dev/vue/official/" class="nav-link"><i class="undefined"></i>
  Vue2.x官方文档
</a></li><li class="dropdown-subitem"><a href="/dev/vue/ebook/" class="nav-link"><i class="undefined"></i>
  Vue2.x电子书系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/monitor/" class="nav-link"><i class="undefined"></i>
  Vue2.x监控系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/sell/" class="nav-link"><i class="undefined"></i>
  Vue2.x电商系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/vue-element-admin/" class="nav-link"><i class="undefined"></i>
  Vue3.x vue-element-admin开发
</a></li><li class="dropdown-subitem"><a href="/dev/vue/vuepress/" class="nav-link"><i class="undefined"></i>
  VuePress安装配置与发布
</a></li></ul></li><li class="dropdown-item"><h4>Java开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/java/basic/" class="nav-link"><i class="undefined"></i>
  Java开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/java/web/" class="nav-link"><i class="undefined"></i>
  Java Web开发
</a></li></ul></li><li class="dropdown-item"><h4>Scala开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/scala/basic/" class="nav-link"><i class="undefined"></i>
  Scala开发基础
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-home"></i>
      SRE
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>数据库系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/database/sql/" class="nav-link"><i class="undefined"></i>
  SQL语言
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-theory/" class="nav-link"><i class="undefined"></i>
  MySQL理论
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-basic/" class="nav-link"><i class="undefined"></i>
  MySQL基础
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-high/" class="nav-link"><i class="undefined"></i>
  MySQL高可用
</a></li><li class="dropdown-subitem"><a href="/sre/database/redis/" class="nav-link"><i class="undefined"></i>
  Redis
</a></li><li class="dropdown-subitem"><a href="/sre/database/elasticsearch/" class="nav-link"><i class="undefined"></i>
  Elasticsearch
</a></li><li class="dropdown-subitem"><a href="/sre/database/mongodb/" class="nav-link"><i class="undefined"></i>
  MongoDB
</a></li></ul></li><li class="dropdown-item"><h4>接入层系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/interface/nginx/" class="nav-link"><i class="undefined"></i>
  Nginx
</a></li></ul></li><li class="dropdown-item"><h4>监控系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/monitor/zabbix/" class="nav-link"><i class="undefined"></i>
  Zabbix
</a></li><li class="dropdown-subitem"><a href="/sre/monitor/grafana/" class="nav-link"><i class="undefined"></i>
  Prometheus、Influxdb、Grafana
</a></li></ul></li><li class="dropdown-item"><h4>日志系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/log/elk/" class="nav-link"><i class="undefined"></i>
  ELK
</a></li><li class="dropdown-subitem"><a href="/sre/log/elastalert/" class="nav-link"><i class="undefined"></i>
  ElastAlert
</a></li></ul></li><li class="dropdown-item"><h4>发布系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/deploy/ansible/" class="nav-link"><i class="undefined"></i>
  Ansible
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/exec-engine/" class="nav-link"><i class="undefined"></i>
  Exec-Engine
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/jenkins/" class="nav-link"><i class="undefined"></i>
  Jenkins
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/gitlab-ci/" class="nav-link"><i class="undefined"></i>
  Gitlab-CI
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/git/" class="nav-link"><i class="undefined"></i>
  Git
</a></li></ul></li><li class="dropdown-item"><h4>虚拟化系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/virtual/docker/" class="nav-link"><i class="undefined"></i>
  Docker
</a></li><li class="dropdown-subitem"><a href="/sre/virtual/k8s/" class="nav-link"><i class="undefined"></i>
  K8S
</a></li></ul></li><li class="dropdown-item"><h4>数据处理系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/data/spark/" class="nav-link"><i class="undefined"></i>
  Spark
</a></li><li class="dropdown-subitem"><a href="/sre/data/flink/" class="nav-link"><i class="undefined"></i>
  Flink
</a></li><li class="dropdown-subitem"><a href="/sre/data/pyflink/" class="nav-link"><i class="undefined"></i>
  PyFlink
</a></li></ul></li><li class="dropdown-item"><h4>消息队列</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/queue/rabbitmq/" class="nav-link"><i class="undefined"></i>
  RabbitMQ
</a></li><li class="dropdown-subitem"><a href="/sre/queue/kafka/" class="nav-link"><i class="undefined"></i>
  Kafka
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-suggestion"></i>
      系统架构
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/arch/micro/grpc/" class="nav-link"><i class="undefined"></i>
  gRPC微服务
</a></li><li class="dropdown-item"><!----> <a href="/arch/micro/thrift/" class="nav-link"><i class="undefined"></i>
  Thrift微服务
</a></li><li class="dropdown-item"><!----> <a href="/arch/sso/oauth2/" class="nav-link"><i class="undefined"></i>
  SSO单点登录系统
</a></li></ul></div></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  时间轴
</a></div><div class="nav-item"><a href="/me/" class="nav-link"><i class="iconfont reco-user"></i>
  关于
</a></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1aefc0b4></div> <aside class="sidebar" data-v-1aefc0b4><div class="personal-info-wrapper" data-v-39576ba9 data-v-1aefc0b4><img src="/img/user-profile.jpeg" alt="author-avatar" class="personal-img" data-v-39576ba9> <h3 class="name" data-v-39576ba9>
    尹欢一的技术博客
  </h3> <div class="num" data-v-39576ba9><div data-v-39576ba9><h3 data-v-39576ba9>641</h3> <h6 data-v-39576ba9>文章</h6></div> <div data-v-39576ba9><h3 data-v-39576ba9>0</h3> <h6 data-v-39576ba9>标签</h6></div></div> <ul class="social-links" data-v-39576ba9></ul> <hr data-v-39576ba9></div> <nav class="nav-links"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-menu"></i>
      内功心法
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/theory/algorithm/" class="nav-link"><i class="undefined"></i>
  算法与数据结构(Golang版本)
</a></li><li class="dropdown-item"><!----> <a href="/theory/algorithmic-thought/" class="nav-link"><i class="undefined"></i>
  算法思想分类
</a></li><li class="dropdown-item"><!----> <a href="/theory/network/" class="nav-link"><i class="undefined"></i>
  计算机网络
</a></li><li class="dropdown-item"><!----> <a href="/theory/system/" class="nav-link"><i class="undefined"></i>
  操作系统
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-document"></i>
      编程开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>Python开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/python/basic/" class="nav-link"><i class="undefined"></i>
  Python开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/python/machine-leaning/" class="nav-link router-link-active"><i class="undefined"></i>
  Python机器学习
</a></li><li class="dropdown-subitem"><a href="/dev/python/third-lib/" class="nav-link"><i class="undefined"></i>
  Python第三方库
</a></li><li class="dropdown-subitem"><a href="/dev/python/scrapy/" class="nav-link"><i class="undefined"></i>
  Python爬虫
</a></li><li class="dropdown-subitem"><a href="/dev/python/deefuture/" class="nav-link"><i class="undefined"></i>
  Python Deefuture教育平台
</a></li><li class="dropdown-subitem"><a href="/dev/python/shop/" class="nav-link"><i class="undefined"></i>
  Python电商平台
</a></li><li class="dropdown-subitem"><a href="/dev/python/coroutine/" class="nav-link"><i class="undefined"></i>
  Python协程开发
</a></li><li class="dropdown-subitem"><a href="/dev/python/tornado/" class="nav-link"><i class="undefined"></i>
  Python Tornado开发
</a></li><li class="dropdown-subitem"><a href="/dev/python/third/" class="nav-link"><i class="undefined"></i>
  Python第三方包
</a></li><li class="dropdown-subitem"><a href="/dev/python/sqlalchemy/" class="nav-link"><i class="undefined"></i>
  Python SQLAlchemy
</a></li></ul></li><li class="dropdown-item"><h4>Golang开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/golang/basic/" class="nav-link"><i class="undefined"></i>
  Golang开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/golang/package/" class="nav-link"><i class="undefined"></i>
  Golang第三方包
</a></li><li class="dropdown-subitem"><a href="/dev/golang/source/" class="nav-link"><i class="undefined"></i>
  Golang源码分析
</a></li><li class="dropdown-subitem"><a href="/dev/golang/kill/" class="nav-link"><i class="undefined"></i>
  Golang秒杀系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/task/" class="nav-link"><i class="undefined"></i>
  Golang任务系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/post/" class="nav-link"><i class="undefined"></i>
  Golang帖子系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/network/" class="nav-link"><i class="undefined"></i>
  Golang网关系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/advance/" class="nav-link"><i class="undefined"></i>
  Golang进阶开发
</a></li><li class="dropdown-subitem"><a href="/dev/golang/problem/" class="nav-link"><i class="undefined"></i>
  Golang问题集合
</a></li></ul></li><li class="dropdown-item"><h4>Shell开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/shell/basic/" class="nav-link"><i class="undefined"></i>
  Shell开发基础
</a></li></ul></li><li class="dropdown-item"><h4>Vue开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/vue/layui/" class="nav-link"><i class="undefined"></i>
  Layui开发
</a></li><li class="dropdown-subitem"><a href="/dev/vue/official/" class="nav-link"><i class="undefined"></i>
  Vue2.x官方文档
</a></li><li class="dropdown-subitem"><a href="/dev/vue/ebook/" class="nav-link"><i class="undefined"></i>
  Vue2.x电子书系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/monitor/" class="nav-link"><i class="undefined"></i>
  Vue2.x监控系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/sell/" class="nav-link"><i class="undefined"></i>
  Vue2.x电商系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/vue-element-admin/" class="nav-link"><i class="undefined"></i>
  Vue3.x vue-element-admin开发
</a></li><li class="dropdown-subitem"><a href="/dev/vue/vuepress/" class="nav-link"><i class="undefined"></i>
  VuePress安装配置与发布
</a></li></ul></li><li class="dropdown-item"><h4>Java开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/java/basic/" class="nav-link"><i class="undefined"></i>
  Java开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/java/web/" class="nav-link"><i class="undefined"></i>
  Java Web开发
</a></li></ul></li><li class="dropdown-item"><h4>Scala开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/scala/basic/" class="nav-link"><i class="undefined"></i>
  Scala开发基础
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-home"></i>
      SRE
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>数据库系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/database/sql/" class="nav-link"><i class="undefined"></i>
  SQL语言
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-theory/" class="nav-link"><i class="undefined"></i>
  MySQL理论
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-basic/" class="nav-link"><i class="undefined"></i>
  MySQL基础
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-high/" class="nav-link"><i class="undefined"></i>
  MySQL高可用
</a></li><li class="dropdown-subitem"><a href="/sre/database/redis/" class="nav-link"><i class="undefined"></i>
  Redis
</a></li><li class="dropdown-subitem"><a href="/sre/database/elasticsearch/" class="nav-link"><i class="undefined"></i>
  Elasticsearch
</a></li><li class="dropdown-subitem"><a href="/sre/database/mongodb/" class="nav-link"><i class="undefined"></i>
  MongoDB
</a></li></ul></li><li class="dropdown-item"><h4>接入层系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/interface/nginx/" class="nav-link"><i class="undefined"></i>
  Nginx
</a></li></ul></li><li class="dropdown-item"><h4>监控系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/monitor/zabbix/" class="nav-link"><i class="undefined"></i>
  Zabbix
</a></li><li class="dropdown-subitem"><a href="/sre/monitor/grafana/" class="nav-link"><i class="undefined"></i>
  Prometheus、Influxdb、Grafana
</a></li></ul></li><li class="dropdown-item"><h4>日志系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/log/elk/" class="nav-link"><i class="undefined"></i>
  ELK
</a></li><li class="dropdown-subitem"><a href="/sre/log/elastalert/" class="nav-link"><i class="undefined"></i>
  ElastAlert
</a></li></ul></li><li class="dropdown-item"><h4>发布系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/deploy/ansible/" class="nav-link"><i class="undefined"></i>
  Ansible
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/exec-engine/" class="nav-link"><i class="undefined"></i>
  Exec-Engine
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/jenkins/" class="nav-link"><i class="undefined"></i>
  Jenkins
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/gitlab-ci/" class="nav-link"><i class="undefined"></i>
  Gitlab-CI
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/git/" class="nav-link"><i class="undefined"></i>
  Git
</a></li></ul></li><li class="dropdown-item"><h4>虚拟化系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/virtual/docker/" class="nav-link"><i class="undefined"></i>
  Docker
</a></li><li class="dropdown-subitem"><a href="/sre/virtual/k8s/" class="nav-link"><i class="undefined"></i>
  K8S
</a></li></ul></li><li class="dropdown-item"><h4>数据处理系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/data/spark/" class="nav-link"><i class="undefined"></i>
  Spark
</a></li><li class="dropdown-subitem"><a href="/sre/data/flink/" class="nav-link"><i class="undefined"></i>
  Flink
</a></li><li class="dropdown-subitem"><a href="/sre/data/pyflink/" class="nav-link"><i class="undefined"></i>
  PyFlink
</a></li></ul></li><li class="dropdown-item"><h4>消息队列</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/queue/rabbitmq/" class="nav-link"><i class="undefined"></i>
  RabbitMQ
</a></li><li class="dropdown-subitem"><a href="/sre/queue/kafka/" class="nav-link"><i class="undefined"></i>
  Kafka
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-suggestion"></i>
      系统架构
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/arch/micro/grpc/" class="nav-link"><i class="undefined"></i>
  gRPC微服务
</a></li><li class="dropdown-item"><!----> <a href="/arch/micro/thrift/" class="nav-link"><i class="undefined"></i>
  Thrift微服务
</a></li><li class="dropdown-item"><!----> <a href="/arch/sso/oauth2/" class="nav-link"><i class="undefined"></i>
  SSO单点登录系统
</a></li></ul></div></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  时间轴
</a></div><div class="nav-item"><a href="/me/" class="nav-link"><i class="iconfont reco-user"></i>
  关于
</a></div> <!----></nav> <ul class="sidebar-links"><li><a href="/dev/python/machine-leaning/docs/concept/concept.html" class="sidebar-link">机器学习概念</a></li><li><a href="/dev/python/machine-leaning/docs/machine-method1/machine-method1.html" class="sidebar-link">机器学习的方法分类1</a></li><li><a href="/dev/python/machine-leaning/docs/machine-method2/machine-method2.html" class="sidebar-link">机器学习的方法分类2</a></li><li><a href="/dev/python/machine-leaning/docs/machine-thinking/machine-thinking.html" class="sidebar-link">机器学习的哲学思考</a></li><li><a href="/dev/python/machine-leaning/docs/machine-use/machine-use.html" class="sidebar-link">Numpy模块的使用</a></li><li><a href="/dev/python/machine-leaning/docs/machine-cal/machine-cal.html" class="sidebar-link">Numpy模块的运算</a></li><li><a href="/dev/python/machine-leaning/docs/machine-together-cal/machine-together-cal.html" class="sidebar-link">Numpy中的聚合运算</a></li><li><a href="/dev/python/machine-leaning/docs/machine-index/machine-index.html" class="sidebar-link">Numpy中的索引</a></li><li><a href="/dev/python/machine-leaning/docs/machine-fancy-index/machine-fancy-index.html" class="sidebar-link">Numpy中的Fancy Indexing</a></li><li><a href="/dev/python/machine-leaning/docs/machine-matplotlib/machine-matplotlib.html" class="sidebar-link">Matplotlib模块</a></li><li><a href="/dev/python/machine-leaning/docs/machine-read-data/machine-read-data.html" class="sidebar-link">读取数据和简单的数据探索</a></li><li><a href="/dev/python/machine-leaning/docs/machine-k/machine-k.html" class="sidebar-link">K-Nearest Neighbors</a></li><li><a href="/dev/python/machine-leaning/docs/machine-k-p/machine-k-p.html" class="sidebar-link">判断机器学习kNN算法的性能和精确度</a></li><li><a href="/dev/python/machine-leaning/docs/machine-k-params/machine-k-params.html" class="sidebar-link">K-Nearest Neighbors 中超参数的问题</a></li><li><a href="/dev/python/machine-leaning/docs/machine-feature-scaling/machine-feature-scaling.html" class="sidebar-link">数据归一化 Feature Scaling</a></li><li><a href="/dev/python/machine-leaning/docs/machine-k-summary/machine-k-summary.html" class="sidebar-link">关于K近邻算法的总结</a></li><li><a href="/dev/python/machine-leaning/docs/machine-leaner/machine-leaner.html" class="sidebar-link">简单线性回归算法</a></li><li><a href="/dev/python/machine-leaning/docs/machine-zg/machine-zg.html" class="sidebar-link">正规方程求解多元线性回归问题</a></li><li><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html" aria-current="page" class="active sidebar-link">梯度下降算法解决多元线性回归问题</a></li><li><a href="/dev/python/machine-leaning/docs/machine-principal/machine-principal.html" class="sidebar-link">梯度下降算法解决多元线性回归问题</a></li><li><a href="/dev/python/machine-leaning/docs/machine-hg/machine-hg.html" class="sidebar-link">多项式回归</a></li><li><a href="/dev/python/machine-leaning/docs/machine-logic/machine-logic.html" class="sidebar-link">逻辑回归 Logistic Regression</a></li><li><a href="/dev/python/machine-leaning/docs/machine-category/machine-category.html" class="sidebar-link">分类结果的评价策略</a></li><li><a href="/dev/python/machine-leaning/docs/machine-svm/machine-svm.html" class="sidebar-link">SVM支持向量机</a></li><li><a href="/dev/python/machine-leaning/docs/machine-decision/machine-decision.html" class="sidebar-link">决策树</a></li><li><a href="/dev/python/machine-leaning/docs/machine-jc/machine-jc.html" class="sidebar-link">集成学习和随机森林</a></li></ul> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-25ba6db2 data-v-1aefc0b4><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>梯度下降算法解决多元线性回归问题</h3> <!----> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>尹欢一的技术博客</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2022
        </a></span></div></div> <div data-v-1aefc0b4><main class="page"><section><div class="page-title"><h1 class="title">梯度下降算法解决多元线性回归问题</h1> <div data-v-f875f3fc><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>尹欢一的技术博客</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>2017/5/1</span></i> <!----> <!----></div></div> <div class="theme-reco-content content__default"><h3 id="一-梯度下降算法"><a href="#一-梯度下降算法" class="header-anchor">#</a> <code>(一)梯度下降算法</code></h3> <ul><li>梯度下降算法概念</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>①：它不是一个机器学习算法
②：是一种基于搜索的最优化方法
③：作用：最小化一个损失函数
④：梯度上升法：最大化一个效用函数
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>定义一个损失函数</li></ul> <p><img src="/assets/img/2019-06-222.44.18.af6d5137.png" alt="Alt text"></p> <p><img src="/assets/img/2019-06-222.52.14.e6d4bf71.png" alt="Alt text"></p> <ul><li>那么θ值递降的速度决定于η的大小和dJ的大小，计算公式如下：左边的θ就是每次迭代递降后的新θ值</li></ul> <p>$$θ = θ - η * dJ$$</p> <h3 id="二-梯度下降算法解决一元二次方程最小值"><a href="#二-梯度下降算法解决一元二次方程最小值" class="header-anchor">#</a> <code>(二)梯度下降算法解决一元二次方程最小值</code></h3> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np 
import matplotlib.pyplot as plt

# 初始化一个140个等间距元素的向量
plot_x = np.linspace(-1, 6, 141)
# 一元二次方程
plot_y = (plot_x - 2.5)**2 - 1

# 绘图
plt.plot(plot_x, plot_y)
plt.show()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><img src="/assets/img/des.dcf6b2ca.png" alt="Alt text"></p> <div class="language- line-numbers-mode"><pre class="language-text"><code># 对二次方程的theta求导, plot_x就是theta
def dJ(theta):
    return 2*(theta - 2.5)

# 损失函数值
def J(theta):
    return (theta-2.5)**2 -1

# 设置theta, eta, epsilon初始值
theta = 0
eta=0.1
epsilon = 1e-8

while True:
    # 获取斜率
    gradient = dJ(theta)
    # 保留之前一次的theta值
    last_theta = theta
    # 获取新的theta值
    theta = theta - eta*gradient
    # 这是循环退出的条件，比较最近两次损失函数的值小于给定的epsilon，就退出循环
    if abs(J(theta) - J(last_theta)) &lt; epsilon:
        break

print(theta)
print(J(theta))
输出结果：
2.499891109642585
-0.99999998814289
# 小结：当theta = 2.499891109642585时候，损失值最小
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><ul><li>画图描述</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>theta = 0
eta=0.1
epsilon = 1e-8

# 记录theta的取值过程
theta_history = [theta]

while True:
    # 获取斜率
    gradient = dJ(theta)  # -5
    # 保留之前一次的theta值
    last_theta = theta  # 0
    # 获取新的theta值
    theta = theta - eta*gradient # 0.5 gradient是负数，减去一个负数得正数
    theta_history.append(theta)
    
    # 这是循环退出的条件，比较最近两次损失函数的值小于给定的epsilon，就退出循环
    if abs(J(theta) - J(last_theta)) &lt; epsilon:
        break

# 看看theta_history每次的取值走向图

# 先画出原一元二次方程图
plt.plot(plot_x, J(plot_x), color='g')
# 再通过theta_history的取值，查看对应的J损失函数的值
plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='x')
plt.show()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><p><img src="/assets/img/sd.0f5f9041.png" alt="Alt text"></p> <div class="language- line-numbers-mode"><pre class="language-text"><code># 查看theta取了多少次
len(theta_history)
输出结果：
46
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>将代码进行封装</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 将上面的计算封装为函数
# 给梯度下降设置梯度下降的次数
def gradient_descent(initial_theta, eta, n_iters=1e4,epsilon=1e-8):

    theta = initial_theta
    theta_history.append(initial_theta)
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta*gradient
        theta_history.append(theta)
    
	    # 当y的差值足够小了就认为是到了极致点，类似于斜率为0的点
        if abs(J(theta) - J(last_theta)) &lt; epsilon:
            break
        
        i_iter += 1
        
        
def plot_theta_history():
    plt.plot(plot_x, J(plot_x), color='g')
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='x')
    plt.show()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><ul><li>修改eta的值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>eta = 0.01
initial_theta = 0
theta_history = []

gradient_descent(initial_theta, eta)
plot_theta_history()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p><img src="/assets/img/er.c93beb60.png" alt="Alt text"></p> <ul><li>查看theta_history记录的值的长度</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>len(theta_history)
输出结果：
424
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>给损失函数添加异常捕获</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>def J(theta):
    try:
        return (theta-2.5) ** 2 -1
    except Exception as e:
        return float('inf')
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code># 即使eta是一个比较大的值，也不会进入死循环， 因为有异常捕获
eta = 1.1
initial_theta = 0
theta_history = []

gradient_descent(initial_theta, eta)
len(theta_history)
输出结果：
10001

# 获取theta_history最后一个值, 一定是nan， 这是无穷大-无穷大
theta_history[-1]
输出结果：
nan
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><h3 id="二-多元线性回归中使用梯度下降法"><a href="#二-多元线性回归中使用梯度下降法" class="header-anchor">#</a> <code>（二）多元线性回归中使用梯度下降法</code></h3> <ul><li>问题转换为：</li></ul> <p><img src="/assets/img/2019-06-225.57.59.911cd3ba.png" alt="Alt text"></p> <ul><li><p>此时 : J(θ) = $\sum\limits_{i=1}^m (y^i - θ_0X_0^i$ + $θ_1X_1^i$ + $θ_2X_2^i$ + $θ_3X_3^i$ + ... + $θ_nX_n^i)^2 $</p></li> <li><p><strong>对J(θ)求θ的偏导数</strong></p></li></ul> <p><img src="/assets/img/2019-06-226.24.13.9c603ca1.png" alt="Alt text"></p> <ul><li>但是这里有个问题，当m的值越大，那么▽J(θ)中元素的值也就越大，这不符合实际需要，因此将损失函数除以m</li></ul> <p><img src="/assets/img/2019-06-226.27.36.0d70fdd4.png" alt="Alt text"></p> <ul><li>那么问题再次转换为下面这个式子尽可能小</li></ul> <p><img src="/assets/img/2019-06-226.31.29.2bb90c22.png" alt="Alt text"></p> <ul><li>模拟一组训练数据，让损失函数的值最小（这里的X矩阵和y真值是训练数据, 3是斜率，4为截距， 为了模拟，线性回归方程末尾添加了正态分布噪音）</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = 2 * np.random.random(size=100)
# 这里加了一个正态分布的噪音
y = x * 3. + 4. + np.random.normal(size=100)
X = x.reshape(-1, 1)
print(X.shape)
print(y.shape)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><ul><li>画出散点图</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>plt.scatter(x, y)
plt.show()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><img src="data:image/png;base64,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" alt="Alt text"></p> <ul><li>梯度下降核心思想：找到合适的θ向量，使得J(θ)尽可能的小，也就是找到合适的θ向量，使得▽J(θ) = 0，但是斜率等于0基本上很难找到，于是将斜率等于0，转换为最后一次求得的J(θ)与上一次J(θ)的差值尽可能小，即：$J(θ) - J_l(θ)$ ， 那么这个θ向量就是被找到的向量，θ[0]元素就是截距，θ[1: ]就是系数</li></ul> <p><img src="/assets/img/2019-06-226.27.36.0d70fdd4.png" alt="Alt text"></p> <ul><li>计算<strong>损失函数</strong>、<strong>损失函数倒数</strong>、<strong>梯度下降函数</strong></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 计算损失函数
# theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / X_b.shape[0]
    except Exception as e:
        return float('inf')

# theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量
def dJ(theta, X_b, y):
    res = np.empty(len(theta))
    # 这是对J求偏导后第一个元素
    # 第一个元素有些特殊，得到的是一个向量，
    # 这个向量应该与1的列向量做点积运算， 也可以将向量元素求和，这里选择求和
    res[0] = np.sum(X_b.dot(theta) -y)
    for i in range(1, len(theta)):
        # 从第二项开始，前面的累加符号由于点积运算不需要理会
        res[i] = (X_b.dot(theta) -y).dot(X_b[:, i])
    return res * 2 / X_b.shape[0]

# 这里的initial_theta是一个向量，不在是一个值了, 函数最终返回theta向量
def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
    
    theta = initial_theta
    # theta_history.append(initial_theta)
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta*gradient
        # theta_history.append(theta)
    
        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:
            break
        
        i_iter += 1
    return theta
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><ul><li>调用函数，计算θ</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 拼凑出X_b矩阵, 由于训练的X数据集是只有一列的矩阵，因此合并后是两列的矩阵
X_b = np.hstack([
    np.ones((X.shape[0], 1)), 
    X])

# 让初始化的theta值从0开始, initial_theta只有两个元素
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01

theta = gradient_descent(X_b, y, initial_theta, eta)
theta
输出结果：
array([4.02145786, 3.00706277])
# 小结：预测结果与期望结果类似，斜率为3，截距为4
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><ul><li>修改之前的linearRegression.py模块的LinearRegression类, 添加梯度下降的拟合方法</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np
from metrics import r2_score

class LinearRegression:
    def __init__(self):
        # 这是系数
        self.coef_ = None
        # 这个是截距
        self.interception_ = None
        # 这个就是具体计算出来的θ列向量
        self._theta = None

    # 这里的拟合是使用正规方程求解
    def fit_normal(self, X_train, y_train):
        assert X_train.shape[0] == y_train.shape[0], \
        '训练数据集必须与预测训练一个维度'

        # 构建特征矩阵， np.ones((X_train.shape[0], 1)), 为元素都为1的矩阵，且只有1列
        X_b = np.hstack([
                         np.ones((X_train.shape[0], 1)),
                         X_train,
                        ])

        # 使用θ表达式，计算出θ的值
        self._theta = np.linalg.inv(X_b.T.dot(X_b))\
                      .dot(X_b.T)\
                      .dot(y_train)

        # 那么截距就是第一个元素
        self.interception_ = self._theta[0]
        # 系数就是后面的元素
        self.coef_ = self._theta[1:]

        return self

    # 这里的拟合使用梯度下降方法
    def fit_gd(self, X_train, y_train, eta=0.01, n_iters=1e4):
        assert X_train.shape[0] == y_train.shape[0], \
        '必须同一维度'

        # 计算损失函数
        # theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量
        def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta)) ** 2) / X_b.shape[0]
            except Exception as e:
                return float('inf')

        # theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量
        def dJ(theta, X_b, y):
            res = np.empty(len(theta))
            # 这是对J求偏导后第一个元素
            # 第一个元素有些特殊，得到的是一个向量，
            # 这个向量应该与1的列向量做点积运算， 也可以将向量元素求和，这里选择求和
            res[0] = np.sum(X_b.dot(theta) - y)
            for i in range(1, len(theta)):
                # 从第二项开始，前面的累加符号由于点积运算不需要理会
                res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
            return res * 2 / X_b.shape[0]

        # 这里的initial_theta是一个向量，不在是一个值了, 函数最终返回theta向量
        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

            theta = initial_theta
            i_iter = 0
            while i_iter &lt; n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient

                if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:
                    break

                i_iter += 1
            return theta

        X_b = np.hstack([np.ones((X_train.shape[0], 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])

        # 最终拿到theta， theta中包括截距和系数
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters=n_iters, epsilon=1e-8)

        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self

    # 预测
    def predict(self, X_predict):

        assert self.interception_ is not None and self.coef_ is not None, \
        '必须先调用fit_normal方法'

        assert X_predict.shape[1] == len(self.coef_), \
        '被预测矩阵的列数必须与系数向量的长度相等'

        # 通过传递进来的X_predict计算新的矩阵X_b
        X_b = np.hstack([
            np.ones((X_predict.shape[0], 1)),
            X_predict,
        ])

        return X_b.dot(self._theta)

    # 使用R Squared 评测算法，评测算法的准确度
    def score(self, X_test, y_test):
        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __str__(self):
        return &quot;LinearRegression(coef_={}, &quot; \
               &quot;interception_={}, &quot; \
               &quot;_theta={})&quot;.format(self.coef_, self.interception_, self._theta)
    
    __repr__ = __str__

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br><span class="line-number">110</span><br><span class="line-number">111</span><br><span class="line-number">112</span><br><span class="line-number">113</span><br><span class="line-number">114</span><br><span class="line-number">115</span><br></div></div><ul><li>调用自实现类，计算θ</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>from linearRegression import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit_gd(X_train=X, y_train=y, eta=0.01, n_iters=1e4)
输出结果：
LinearRegression(coef_=[3.00706277], interception_=4.021457858204859, _theta=[4.02145786 3.00706277])

小结：同样的系数是3.00706277， 截距是4.021457858204859，符合预期结果
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><ul><li>总结：目前学习了两种算法，处理损失函数的最小值
<ul><li>①：正规方程求解，求解出θ的表达式(一般很难求解得到)</li> <li>②：梯度下降，找到损失函数J(θ)的极值点对应的θ，就是预期的θ，从而获得损失函数J(θ)的系数和截距，最终通过求得的θ向量，得到多元线性回归方程来预测趋势数据</li></ul></li></ul> <h3 id="二-线性回归使用梯度下降法向量化训练真实数据集"><a href="#二-线性回归使用梯度下降法向量化训练真实数据集" class="header-anchor">#</a> <code>(二)线性回归使用梯度下降法向量化训练真实数据集</code></h3> <ul><li><p>之前的▽J(θ)是一项一项通过循环的方式将列向量每一行元素计算出来求解的，速度比较慢，下面通过矩阵运算的方式进行优化</p></li> <li><p>将▽J(θ)进行化简变形 $X_0^i$表示第i行第0列的值为1， 由于需要拼凑成X_b，第0列的元素都是1</p></li></ul> <p><img src="/assets/img/has.1cb08fce.png" alt="Alt text"></p> <p><img src="/assets/img/hs12.7a0467b3.png" alt="Alt text"></p> <p><img src="/assets/img/zz.c8f4dac4.png" alt="Alt text"></p> <ul><li>最后将梯度的表达式化简为 矩阵运算，结果如下</li></ul> <p><img src="/assets/img/as.6898b398.png" alt="Alt text"></p> <div class="language- line-numbers-mode"><pre class="language-text"><code># 在
# 修改梯度导数函数
def dJ_matrix(theta, X_b, y):
    return 2. / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li><strong>引入scikit-learn的波士顿房价真实数据进行算法测评(第一个是正规方程求解的方式)</strong></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np 
import matplotlib.pyplot as plt
from sklearn import datasets

# 加载波士顿房产数据
boston = datasets.load_boston()

X = boston.data
y = boston.target

# 这里使用的是fancing indexing的方式过滤数据
X = X[y &lt; 50.0]
y = y[y &lt; 50.0]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

from linearRegression import LinearRegression
lin_reg = LinearRegression()
# 使用线性回归方程的方式求解θ
%time lin_reg.fit_normal(X_train, y_train)
lin_reg.score(X_test, y_test)

输出结果：
CPU times: user 3.25 ms, sys: 492 µs, total: 3.74 ms
Wall time: 2.74 ms
0.8009390227580956
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br></div></div><ul><li>使用梯度下降的方式(▽J(θ)计算已经通过矩阵计算的方式进行了优化)</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>lin_reg2 = LinearRegression()
# 使用梯度下降的方式求解θ, eta的取值要非常小，保证移动的步长是收敛的
%time lin_reg.fit_gd(X_train, y_train, eta=0.0000001, n_iters=1e6)
lin_reg.score(X_test, y_test)
输出结果：
CPU times: user 1min 6s, sys: 403 ms, total: 1min 7s
Wall time: 34.5 s
0.5116716158630092
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><ul><li>小结： <strong>上面的运行结果表明，eta、n_iters这两个值都需要调整，因为这两个值决定了测试结果的准确度，但是，这两个值一般都是事先指定好的，解决的方法可以使用数据归一化，将特征数据和真值数据都统一到同一个维度</strong></li></ul> <p><img src="/assets/img/sdf.e7dcabba.png" alt="Alt text"></p> <ul><li>将特征数据X矩阵和真值数据y列向量归一化处理</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 将初始值进行数据归一化处理
from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
# 调用fit方法，目的是先计算出均值和方差
standardScaler.fit(X_train)
# 通过fit方法计算出来的举止和方差将X_train、X_test进行方差均值归一化
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)

lin_reg3 = LinearRegression()
%time lin_reg3.fit_gd(X_train_standard, y_train)

输出结果：
CPU times: user 317 ms, sys: 3.19 ms, total: 320 ms
Wall time: 167 ms

lin_reg3.score(X_test_standard, y_test)
输出结果：
0.800927010538664
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><ul><li>小结：通过上述结果可以得知：正规方程的运算准确度与梯度下降的运算准确度一样, 梯度下降法寻找最优的θ向量的时间比正规矩阵的方法耗时更加长， 但是这里是有原因的, 看下面这个例子</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 初始化样本数
m = 1000
# 初始化特征数
n = 5000
# 初始化符合正态分布的X特征矩阵
big_X = np.random.normal(size=(m, n))
# 从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high
true_theta = np.random.uniform(0.0, 100.0, size=n+1)
# 生成y真值,生成的公式就是y = X*θ + θ[0] + 噪音(m ✖️1的矩阵)
big_y = big_X.dot(true_theta[1:]) + true_theta[0] + np.random.normal(0.0,10.0,size=m)
# 使用正规矩阵的方式，训练的结果耗时6s
big_reg1 = LinearRegression()
%time big_reg1.fit_normal(big_X, big_y)
输出结果：
CPU times: user 11 s, sys: 396 ms, total: 11.4 s
Wall time: 5.86 s
# 使用梯度下降的方式耗时
big_reg2 = LinearRegression()
%time big_reg2.fit_gd(big_X, big_y)
输出结果：
CPU times: user 3.86 s, sys: 32.4 ms, total: 3.9 s
Wall time: 2.14 s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><ul><li>说明：此时，梯度下降算法的时间比正规矩阵算法的时间少， 因为正规方程处理的是一个m x n 的矩阵运算，而梯度下降是根据样本数量规模影响时间的， 这种梯度下降法是批量梯度下降法，那么当样本量大的化可以使用随机梯度下降法来优化</li></ul> <p><img src="/assets/img/ass.3f6ddbaf.png" alt="Alt text"></p> <ul><li>这种批量梯度下降法随着m样本数越大，计算的时间就长</li></ul> <h3 id="三-随机梯度下降法"><a href="#三-随机梯度下降法" class="header-anchor">#</a> <code>(三)随机梯度下降法</code></h3> <ul><li>随机梯度下降法就是将▽J(θ)向量只取一个样本，不再要前面的求和，例如i=3
<ul><li>这种极小值搜索的曲线将不是沿着一个方向下降，可能会因为随机的不同，递降的方向发生变化，为了避免递降的速度在初始的时候更快，η的去取值会被a、b常数调整。这就是模拟退火的思想</li></ul></li></ul> <p><img src="/assets/img/asdf.5a10747a.png" alt="Alt text"></p> <ul><li>随机梯度下降法从公式本身来看，就是对▽J(θ)进行了改造，批量梯度下降法是考虑了特征矩阵的所有行，在随机梯度下降法中只考虑一行即可，因此没有了前面的求和符号和除以m</li></ul> <p><img src="/assets/img/aaa.f62f8335.png" alt="Alt text"></p> <p><img src="/assets/img/sdd.4d5fa1bc.png" alt="Alt text"></p> <ul><li><p>举例：一个一维特征矩阵的线性回归方程，通过X_train、y_train进行训练，计算出θ，并且比较批量梯度下降方法与随机梯度下降方法的耗时时长</p></li> <li><p><strong>①：初始化特征矩阵</strong></p></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 特征矩阵X， 真值y
import numpy as np 
import matplotlib as plt

# 初始化样本个数
m = 10000
# 初始化样本特征个数，这里特征个数为1, 且保证了特征数据为正态分布的数据
x = np.random.normal(size=m)
X = x.reshape(-1, 1)
# 这里模拟的是一个特征的线性回归方程, 注意，这里是小x，如果是X矩阵，那么y的shape就是10000 x 10000
y = 4. * x + 3. + np.random.normal(0, 3, size=m)
print(X.shape)
print(y.shape)
输出结果：
(10000, 1)
(10000,)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><ul><li>②：使用批量梯度下降方法计算θ值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / X_b.shape[0]
    except Exception as e:
        return float('inf')

def dJ_matrix(theta, X_b, y):
    return 2. / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)
    
# 这里的initial_theta是一个向量，不在是一个值了, 函数最终返回theta向量
def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    
    while i_iter &lt; n_iters:
        gradient = dJ_matrix(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:
            break
        i_iter += 1
    return theta
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><ul><li>③：使用批量梯度下降方法计算θ值且获取耗时时长</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>%%time
X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01
theta = gradient_descent(X_b, y, initial_theta, eta)
print(theta)
输出结果：
[3.04923175 3.98437447]
耗时：103ms
CPU times: user 170 ms, sys: 4.44 ms, total: 175 ms
Wall time: 103 ms
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><ul><li>④：使用随机梯度下降法计算θ值且获取耗时时长(由于在随机梯度下降法中，判断循环退出的条件，不在按照批量梯度下降方法计算J(θ)的差值，判断循环退出的条件只由n_iters循环迭代次数决定</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 这里传递进来的特征矩阵，不再是整个的X_b，而是X_b中的一行特征，相当于一个特征行向量
# y不再是列向量，而是一个值
def dJ_sgd(theta, X_b_i, y_i):
    # 根据随机梯度下降的公式可知
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.

# 计算θ值
# 
def stochastic_gradient_descent(X_b, y, initial_theta, n_iters):
    t0 = 5
    t1 = 50
    
    # 这个就是退火的学习率
    def learning_rate(t):
        return t0 / (t + t1)
    
    theta = initial_theta
    
    # 循环获取迭代次数, 这里每次迭代就是为了计算学习率,这里看看是否可以改为while循环
    for cur_iter in range(n_iters):
        # 获取一个数据样本的索引，这里是在样本总长度中随机选择一个样本
        rand_i = np.random.randint(len(X_b))
        # 获取梯度值
        gradient = dJ_sgd(theta, X_b[rand_i], y[rand_i])
        # 获取最新的θ值，这里最新的θ值计算的公式为θ沿导数递降的变化率
        theta = theta - learning_rate(cur_iter) * gradient
        return theta
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><ul><li>在pycharm中封装自己的sgd随机梯度下降算法</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>    def fit_sgd(self, X_train, y_train, n_iters, t0=5, t1=50):
        assert X_train.shape[0] == y_train.shape[0], \
            '样本数必须等于真值的列向量行数'

        assert n_iters &gt;= 1, \
            '迭代所有样本的次数必须大于1'

        def dJ_sgd(theta, X_b_i, y_i):
            # 这里去掉了转置和点乘
            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.

        # 计算θ值
        # 这里的n_iters的意义发生了变化，n_iters指的是在循环迭代过程中，需要把所有的样本计算n_iters遍
        def stochastic_gradient_descent(X_b, y, initial_theta, n_iters, t0=5, t1=50):

            # 这个就是退火的学习率
            def learning_rate(t):
                return t0 / (t + t1)

            theta = initial_theta
            # 获取样本数
            m = len(X_b)

            # 循环获取迭代次数, 迭代次数为样本的n_iters倍
            for cur_iter in range(n_iters):
                # 获取随机排列的索引
                indexes = np.random.permutation(m)
                # 让之前的X_b、y_new的所有样本随机排列,
                X_b_new = X_b[indexes]
                y_new = y[indexes]

                # 对每一个样本都进行遍历
                for i in range(m):
                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])
                    theta = theta - learning_rate(cur_iter * m + i) * gradient

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        # 使用随机梯度下降计算theta,
        self._theta = stochastic_gradient_descent(X_b, y_train, initial_theta, n_iters=n_iters)
        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br></div></div><ul><li>使用自己的SGD计算θ值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 模拟数据
import numpy as np 
import matplotlib as plt

# 初始化样本个数
m = 100000
# 初始化样本特征个数，这里特征个数为1, 且保证了特征数据为正态分布的数据
x = np.random.normal(size=m)
X = x.reshape(-1, 1)
# 这里模拟的是一个特征的线性回归方程, 注意，这里是小x，如果是X矩阵，那么y的shape就是10000 x 10000
y = 4. * x + 3. + np.random.normal(0, 3, size=m)
print(X.shape)
print(y.shape)
输出结果：
(100000, 1)
(100000,)

from linearRegression import LinearRegression

lin_reg4 = LinearRegression()
lin_reg4.fit_sgd(X, y, n_iters=10)
输出结果：
LinearRegression(coef_=[3.99974133], interception_=3.007942096244411, _theta=[3.0079421  3.99974133])

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><ul><li>使用波士顿房价数据验证算法的准确度</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np 
import matplotlib.pyplot as plt
from sklearn import datasets

# 加载波士顿房产数据
boston = datasets.load_boston()

X = boston.data
y = boston.target

# 这里使用的是fancing indexing的方式过滤数据
X = X[y &lt; 50.0]
y = y[y &lt; 50.0]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# 将初始值进行数据归一化处理
from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X_train)
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)

from linearRegression import LinearRegression
lin_reg = LinearRegression()

%time lin_reg.fit_sgd(X_train_standard, y_train, n_iters=1000)
lin_reg.score(X_test_standard, y_test)
输出结果：
CPU times: user 1.97 s, sys: 11.3 ms, total: 1.99 s
Wall time: 2.02 s
0.8009992300659649
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br></div></div><ul><li>使用scikit-learn中的SGD随机梯度下降方法验证准确度</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 这个类只能解决线性模型
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(n_iter=1000)
%time sgd_reg.fit(X_train_standard, y_train)
sgd_reg.score(X_test_standard, y_test)
输出结果：
CPU times: user 35.8 ms, sys: 1.54 ms, total: 37.3 ms
Wall time: 36.5 ms
0.8010109472897677
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><ul><li>小结：从自实现的算法和scikit-learn算法对比，scikit-learn耗时少</li></ul> <h3 id="四-梯度下降法的调试方法"><a href="#四-梯度下降法的调试方法" class="header-anchor">#</a> <code>(四)梯度下降法的调试方法</code></h3> <ul><li><p>对于推导出来的算法公式，如何证明是正确且准确的呢？这就需要梯度下降的调试方法了</p></li> <li><p>当θ是一个值的时候，此时是一维的场景， 那么在θ这点的导数就两个蓝色点的斜率</p></li></ul> <p><img src="/assets/img/2019-06-254.56.56.6f95b167.png" alt="Alt text"></p> <ul><li>当θ是一个向量的时候，此时是二维的场景， 那么在θ这点的导数就等于对各个分量求导</li></ul> <p><img src="/assets/img/2019-06-255.03.37.1f60fa24.png" alt="Alt text"></p> <ul><li>那么对$θ_0$这个点的导数可以表示如下：</li></ul> <p><img src="/assets/img/2019-06-255.04.03.7a7310a4.png" alt="Alt text"></p> <ul><li>那么对$θ_1$这个点的导数可以表示如下：</li></ul> <p><img src="/assets/img/2019-06-255.06.41.55135767.png" alt="Alt text"></p> <ul><li><strong>线性回归方程初始化</strong></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
# 特征矩阵
X = np.random.random(size=(1000, 10))
# theta值， 11个元素
true_theta = np.arange(1,12,dtype=float)
# 拼凑出X_b, 11列
X_b = np.hstack([np.ones((len(X), 1)), X])
# 这个根据数学上的点乘计算，true_theta为列向量，
# 左边一个矩阵.列向量=列向量， 再加上一个列向量，还是等于列向量
y = X_b.dot(true_theta) + np.random.normal(size=1000)
print(X.shape)
print(y.shape)
print(true_theta)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><ul><li>准备损失函数、数学梯度函数、dubug梯度函数、梯度下降函数</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 损失函数
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / X_b.shape[0]
    except Exception as e:
        return float('inf')

# 损失函数导数，数学推导出来的
def dJ_matrix(theta, X_b, y):
    return 2. / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)

# 根据之前的公式，求出debug模式下的导数
def dJ_debug(theta, X_b, y, epsilon=0.01):
    # 初始化一个与theta等长度的向量
    res = np.empty(len(theta))
    for i in range(len(theta)):
        theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        # 计算每个维度的偏导数
        res[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y)) / (2 * epsilon)
    return res

# 这里多加了一个参数，dJ_matrix，这个函数定义了如何求解梯度
def gradient_descent(dJ_matrix, X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ_matrix(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:
            break
        i_iter += 1
    return theta     
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><ul><li>准备X_b、initial_theta、eta</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>使用debug方式计算θ值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>%time theta = gradient_descent(dJ_debug, X_b, y, initial_theta, eta)
print(theta)
输出结果：
CPU times: user 7.32 s, sys: 34.1 ms, total: 7.36 s
Wall time: 3.8 s
[ 1.1251597   2.05312521  2.91522497  4.11895968  5.05002117  5.90494046
  6.97383745  8.00088367  8.86213468  9.98608331 10.90529198]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><ul><li>使用数学推导的方式计算θ值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>%time theta = gradient_descent(dJ_matrix, X_b, y, initial_theta, eta)
print(theta)
输出结果：
CPU times: user 918 ms, sys: 3.64 ms, total: 922 ms
Wall time: 479 ms
[ 1.1251597   2.05312521  2.91522497  4.11895968  5.05002117  5.90494046
  6.97383745  8.00088367  8.86213468  9.98608331 10.90529198]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><ul><li>小结</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>	这个就是使用了使用debug方式计算θ值与数学推导出来的公式进行对比，最后两者的结果一样，表明数学推导正确, dJ_matrix函数只适用线性的梯度下降问题求解梯度, 这个dJ_debug函数，可以用到所有的梯度下降方法求解梯度的运算中， 可以将dJ_debug函数加入到机器学习的工具集中
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h3 id="五-梯度下降方法总结"><a href="#五-梯度下降方法总结" class="header-anchor">#</a> <code>(五)梯度下降方法总结</code></h3> <ul><li>分类</li></ul> <p><img src="/assets/img/2019-06-259.25.50.04916a2c.png" alt="Alt text"></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>1：批量梯度下降法：稳定，但是速度慢
2：随机批量下降法：在批量梯度下降法的基础上，计算1行样本，速度快，但是不稳定
3：小批量梯度下降法：在批量梯度下降法的基础上，计算k行样本
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li><strong>自己实现小批量梯度下降法</strong></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>待实现内容
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>梯度下降法</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>不是机器学习的一种算法，而是一种解决搜索中的最优化的搜索方法
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>梯度上升方法</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>前讲的都是对损失函数求最小值，但是有时候也有对效用函数求最大值，那么就需要使用梯度上升方法
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">2022/3/12 下午5:47:29</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev"><a href="/dev/python/machine-leaning/docs/machine-zg/machine-zg.html" class="prev">
            正规方程求解多元线性回归问题
          </a></span> <span class="next"><a href="/dev/python/machine-leaning/docs/machine-principal/machine-principal.html">
            梯度下降算法解决多元线性回归问题
          </a></span></p></div> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-cb1513f6><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#一-梯度下降算法" class="sidebar-link reco-side-一-梯度下降算法" data-v-cb1513f6>(一)梯度下降算法</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#二-梯度下降算法解决一元二次方程最小值" class="sidebar-link reco-side-二-梯度下降算法解决一元二次方程最小值" data-v-cb1513f6>(二)梯度下降算法解决一元二次方程最小值</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#二-多元线性回归中使用梯度下降法" class="sidebar-link reco-side-二-多元线性回归中使用梯度下降法" data-v-cb1513f6>（二）多元线性回归中使用梯度下降法</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#二-线性回归使用梯度下降法向量化训练真实数据集" class="sidebar-link reco-side-二-线性回归使用梯度下降法向量化训练真实数据集" data-v-cb1513f6>(二)线性回归使用梯度下降法向量化训练真实数据集</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#三-随机梯度下降法" class="sidebar-link reco-side-三-随机梯度下降法" data-v-cb1513f6>(三)随机梯度下降法</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#四-梯度下降法的调试方法" class="sidebar-link reco-side-四-梯度下降法的调试方法" data-v-cb1513f6>(四)梯度下降法的调试方法</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#五-梯度下降方法总结" class="sidebar-link reco-side-五-梯度下降方法总结" data-v-cb1513f6>(五)梯度下降方法总结</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/assets/js/app.ef2ae1d4.js" defer></script><script src="/assets/js/7.c49e7131.js" defer></script><script src="/assets/js/1.d009776b.js" defer></script><script src="/assets/js/11.452c26e6.js" defer></script>
  </body>
</html>
