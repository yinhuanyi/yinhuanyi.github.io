<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>梯度下降算法解决多元线性回归问题 | Robby·爱编程</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/img/logo.png">
    <link rel="stylesheet" href="/css/style.css">
    <script charset="utf-8" src="/js/main.js"></script>
    <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?e5c9806c714901413f461d5df50f2512";
          var s = document.getElementsByTagName("script")[0]; 
          s.parentNode.insertBefore(hm, s);
        })();
        </script>
    <meta name="description" content=" ">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/assets/css/0.styles.793b538b.css" as="style"><link rel="preload" href="/assets/js/app.9a5d1377.js" as="script"><link rel="preload" href="/assets/js/7.c49e7131.js" as="script"><link rel="preload" href="/assets/js/1.d009776b.js" as="script"><link rel="preload" href="/assets/js/11.2d4145aa.js" as="script"><link rel="prefetch" href="/assets/js/10.bf3a4a01.js"><link rel="prefetch" href="/assets/js/100.183e29a7.js"><link rel="prefetch" href="/assets/js/101.199d8e46.js"><link rel="prefetch" href="/assets/js/102.77871772.js"><link rel="prefetch" href="/assets/js/103.b1663661.js"><link rel="prefetch" href="/assets/js/104.e2724712.js"><link rel="prefetch" href="/assets/js/105.66d2fca1.js"><link rel="prefetch" href="/assets/js/106.d13147aa.js"><link rel="prefetch" href="/assets/js/107.6c143695.js"><link rel="prefetch" href="/assets/js/108.38baff0a.js"><link rel="prefetch" href="/assets/js/109.b87f3dae.js"><link rel="prefetch" href="/assets/js/110.ecd312df.js"><link rel="prefetch" href="/assets/js/111.e86d0cf5.js"><link rel="prefetch" href="/assets/js/112.838e7814.js"><link rel="prefetch" href="/assets/js/113.ef756b43.js"><link rel="prefetch" href="/assets/js/114.90eb6e00.js"><link rel="prefetch" href="/assets/js/115.9086d1d7.js"><link rel="prefetch" href="/assets/js/116.fcaba423.js"><link rel="prefetch" href="/assets/js/117.1913339f.js"><link rel="prefetch" href="/assets/js/118.11985c3d.js"><link rel="prefetch" href="/assets/js/119.ab560a50.js"><link rel="prefetch" href="/assets/js/12.7bb0c6eb.js"><link rel="prefetch" href="/assets/js/120.25784945.js"><link rel="prefetch" href="/assets/js/121.03b17d23.js"><link rel="prefetch" href="/assets/js/122.dbb3d982.js"><link rel="prefetch" href="/assets/js/123.e04e1de0.js"><link rel="prefetch" href="/assets/js/124.c19c22e8.js"><link rel="prefetch" href="/assets/js/125.dc1e889c.js"><link rel="prefetch" href="/assets/js/126.4e0be7c0.js"><link rel="prefetch" href="/assets/js/127.2648dc86.js"><link rel="prefetch" href="/assets/js/128.a036f05c.js"><link rel="prefetch" href="/assets/js/129.ddeea161.js"><link rel="prefetch" href="/assets/js/13.794cb86f.js"><link rel="prefetch" href="/assets/js/130.2f51e053.js"><link rel="prefetch" href="/assets/js/131.50ad10f9.js"><link rel="prefetch" href="/assets/js/132.fcf72314.js"><link rel="prefetch" href="/assets/js/133.952404bc.js"><link rel="prefetch" href="/assets/js/134.319691ce.js"><link rel="prefetch" href="/assets/js/135.358e0553.js"><link rel="prefetch" href="/assets/js/136.a5240b23.js"><link rel="prefetch" href="/assets/js/137.a1acadf1.js"><link rel="prefetch" href="/assets/js/138.060a4f8d.js"><link rel="prefetch" href="/assets/js/139.ac0e3354.js"><link rel="prefetch" href="/assets/js/14.8e692555.js"><link rel="prefetch" href="/assets/js/140.d14826df.js"><link rel="prefetch" href="/assets/js/141.855d329f.js"><link rel="prefetch" href="/assets/js/142.707f4edb.js"><link rel="prefetch" href="/assets/js/143.7ac10fb6.js"><link rel="prefetch" href="/assets/js/144.ccbcbe88.js"><link rel="prefetch" href="/assets/js/145.f97912c7.js"><link rel="prefetch" href="/assets/js/146.8564b53a.js"><link rel="prefetch" href="/assets/js/147.65727197.js"><link rel="prefetch" href="/assets/js/148.3b03ba9b.js"><link rel="prefetch" href="/assets/js/149.b099e234.js"><link rel="prefetch" href="/assets/js/15.5a69bb7b.js"><link rel="prefetch" href="/assets/js/150.e2dac6a4.js"><link rel="prefetch" href="/assets/js/151.20292438.js"><link rel="prefetch" href="/assets/js/152.de07d289.js"><link rel="prefetch" href="/assets/js/153.6ed13d07.js"><link rel="prefetch" href="/assets/js/154.e33f8138.js"><link rel="prefetch" href="/assets/js/155.9514ea65.js"><link rel="prefetch" href="/assets/js/156.365860d6.js"><link rel="prefetch" href="/assets/js/157.be5d1989.js"><link rel="prefetch" href="/assets/js/158.d809568d.js"><link rel="prefetch" href="/assets/js/159.7685a5ea.js"><link rel="prefetch" href="/assets/js/16.39276800.js"><link rel="prefetch" href="/assets/js/160.b8b2aa7c.js"><link rel="prefetch" href="/assets/js/161.2be5e8ba.js"><link rel="prefetch" href="/assets/js/162.e39ddda0.js"><link rel="prefetch" href="/assets/js/163.cd59e69c.js"><link rel="prefetch" href="/assets/js/164.f439c6b3.js"><link rel="prefetch" href="/assets/js/165.74e47c48.js"><link rel="prefetch" href="/assets/js/166.8d1656c7.js"><link rel="prefetch" href="/assets/js/167.1a8d36d8.js"><link rel="prefetch" href="/assets/js/168.c09992d2.js"><link rel="prefetch" href="/assets/js/169.ad76d2ad.js"><link rel="prefetch" href="/assets/js/17.f6207881.js"><link rel="prefetch" href="/assets/js/170.86c8539e.js"><link rel="prefetch" href="/assets/js/171.a00bc1de.js"><link rel="prefetch" href="/assets/js/172.4f7cc2b8.js"><link rel="prefetch" href="/assets/js/173.e8417345.js"><link rel="prefetch" href="/assets/js/174.bcbc7ed5.js"><link rel="prefetch" href="/assets/js/175.8c13131f.js"><link rel="prefetch" href="/assets/js/176.a8e0a82f.js"><link rel="prefetch" href="/assets/js/177.cc3e8330.js"><link rel="prefetch" href="/assets/js/178.0f6cf921.js"><link rel="prefetch" href="/assets/js/179.70e295f9.js"><link rel="prefetch" href="/assets/js/18.dfc6a15d.js"><link rel="prefetch" href="/assets/js/180.5e8d67ce.js"><link rel="prefetch" href="/assets/js/181.a5136d64.js"><link rel="prefetch" href="/assets/js/182.0b1bc7fc.js"><link rel="prefetch" href="/assets/js/183.92dd7271.js"><link rel="prefetch" href="/assets/js/184.ce4e0f66.js"><link rel="prefetch" href="/assets/js/185.4b6c844e.js"><link rel="prefetch" href="/assets/js/186.fb4510cf.js"><link rel="prefetch" href="/assets/js/187.88fa2b8f.js"><link rel="prefetch" href="/assets/js/188.c54c9ff8.js"><link rel="prefetch" href="/assets/js/189.cc525573.js"><link rel="prefetch" href="/assets/js/19.c9a4b6df.js"><link rel="prefetch" href="/assets/js/190.0a0fc517.js"><link rel="prefetch" href="/assets/js/191.c0b20f10.js"><link rel="prefetch" href="/assets/js/192.a401c173.js"><link rel="prefetch" href="/assets/js/193.3884a8c4.js"><link rel="prefetch" href="/assets/js/194.2aecb6b4.js"><link rel="prefetch" href="/assets/js/195.d5b973f4.js"><link rel="prefetch" href="/assets/js/196.e8311196.js"><link rel="prefetch" href="/assets/js/197.49170522.js"><link rel="prefetch" href="/assets/js/198.6a0c43cb.js"><link rel="prefetch" href="/assets/js/199.b20ce791.js"><link rel="prefetch" href="/assets/js/20.531ffaef.js"><link rel="prefetch" href="/assets/js/200.f772507e.js"><link rel="prefetch" href="/assets/js/201.aca6d71e.js"><link rel="prefetch" href="/assets/js/202.0bf45c68.js"><link rel="prefetch" href="/assets/js/203.125a1a52.js"><link rel="prefetch" href="/assets/js/204.e9703838.js"><link rel="prefetch" href="/assets/js/205.11be8755.js"><link rel="prefetch" href="/assets/js/206.5df53ca7.js"><link rel="prefetch" href="/assets/js/207.7881dce6.js"><link rel="prefetch" href="/assets/js/208.4f9fee3c.js"><link rel="prefetch" href="/assets/js/209.b029372d.js"><link rel="prefetch" href="/assets/js/21.551fce6d.js"><link rel="prefetch" href="/assets/js/210.53655350.js"><link rel="prefetch" href="/assets/js/211.c0b9c70f.js"><link rel="prefetch" href="/assets/js/212.26c1ebdf.js"><link rel="prefetch" href="/assets/js/213.ef52073b.js"><link rel="prefetch" href="/assets/js/214.ce8c9841.js"><link rel="prefetch" href="/assets/js/215.192a0677.js"><link rel="prefetch" href="/assets/js/216.6cd17926.js"><link rel="prefetch" href="/assets/js/217.00ed48ab.js"><link rel="prefetch" href="/assets/js/218.781c8b07.js"><link rel="prefetch" href="/assets/js/219.c56f8cf4.js"><link rel="prefetch" href="/assets/js/22.986fb9e7.js"><link rel="prefetch" href="/assets/js/220.dc918507.js"><link rel="prefetch" href="/assets/js/221.39b770a4.js"><link rel="prefetch" href="/assets/js/222.9e1df2c8.js"><link rel="prefetch" href="/assets/js/223.5eff1248.js"><link rel="prefetch" href="/assets/js/224.c0cea281.js"><link rel="prefetch" href="/assets/js/225.8ad561aa.js"><link rel="prefetch" href="/assets/js/226.43acb0be.js"><link rel="prefetch" href="/assets/js/227.37ebd361.js"><link rel="prefetch" href="/assets/js/228.ead480ed.js"><link rel="prefetch" href="/assets/js/229.3baad619.js"><link rel="prefetch" href="/assets/js/23.ad800700.js"><link rel="prefetch" href="/assets/js/230.1af99529.js"><link rel="prefetch" href="/assets/js/231.59879531.js"><link rel="prefetch" href="/assets/js/232.3de50670.js"><link rel="prefetch" href="/assets/js/233.f21ede1a.js"><link rel="prefetch" href="/assets/js/234.7918e7e4.js"><link rel="prefetch" href="/assets/js/235.300a10a5.js"><link rel="prefetch" href="/assets/js/236.d35b33b7.js"><link rel="prefetch" href="/assets/js/237.96116ee5.js"><link rel="prefetch" href="/assets/js/238.59a35d89.js"><link rel="prefetch" href="/assets/js/239.9dfc4fbc.js"><link rel="prefetch" href="/assets/js/24.8425bd19.js"><link rel="prefetch" href="/assets/js/240.36bac2f9.js"><link rel="prefetch" href="/assets/js/241.ad0d06e7.js"><link rel="prefetch" href="/assets/js/242.9344b5fa.js"><link rel="prefetch" href="/assets/js/243.facc5ec6.js"><link rel="prefetch" href="/assets/js/244.71a48ed2.js"><link rel="prefetch" href="/assets/js/245.82a78890.js"><link rel="prefetch" href="/assets/js/246.d274961a.js"><link rel="prefetch" href="/assets/js/247.ee9d9380.js"><link rel="prefetch" href="/assets/js/248.8825abef.js"><link rel="prefetch" href="/assets/js/249.5061079a.js"><link rel="prefetch" href="/assets/js/25.ebd0fb73.js"><link rel="prefetch" href="/assets/js/250.cd3b6c4a.js"><link rel="prefetch" href="/assets/js/251.d218096c.js"><link rel="prefetch" href="/assets/js/252.ac828464.js"><link rel="prefetch" href="/assets/js/253.6030735c.js"><link rel="prefetch" href="/assets/js/254.3534a5fb.js"><link rel="prefetch" href="/assets/js/255.36b98bd1.js"><link rel="prefetch" href="/assets/js/256.48ca0db6.js"><link rel="prefetch" href="/assets/js/257.d0ca646c.js"><link rel="prefetch" href="/assets/js/258.781f4f39.js"><link rel="prefetch" href="/assets/js/259.48f6daae.js"><link rel="prefetch" href="/assets/js/26.e0e94a5e.js"><link rel="prefetch" href="/assets/js/260.ce650517.js"><link rel="prefetch" href="/assets/js/261.824eccad.js"><link rel="prefetch" href="/assets/js/262.b976ff4c.js"><link rel="prefetch" href="/assets/js/263.c1daf4e9.js"><link rel="prefetch" href="/assets/js/264.72936fdc.js"><link rel="prefetch" href="/assets/js/265.16ece61e.js"><link rel="prefetch" href="/assets/js/266.b17f1b64.js"><link rel="prefetch" href="/assets/js/267.14e0a079.js"><link rel="prefetch" href="/assets/js/268.9ea7c90f.js"><link rel="prefetch" href="/assets/js/269.158ff25d.js"><link rel="prefetch" href="/assets/js/27.66fdb8f8.js"><link rel="prefetch" href="/assets/js/270.7446d42b.js"><link rel="prefetch" href="/assets/js/271.68bf6f03.js"><link rel="prefetch" href="/assets/js/272.4d27d446.js"><link rel="prefetch" href="/assets/js/273.9c6efddc.js"><link rel="prefetch" href="/assets/js/274.81b9b843.js"><link rel="prefetch" href="/assets/js/275.bd55419d.js"><link rel="prefetch" href="/assets/js/276.3b600307.js"><link rel="prefetch" href="/assets/js/277.19eff568.js"><link rel="prefetch" href="/assets/js/278.f34b0bd3.js"><link rel="prefetch" href="/assets/js/279.8ac0e1dc.js"><link rel="prefetch" href="/assets/js/28.93d4f4ab.js"><link rel="prefetch" href="/assets/js/280.3c4ed74e.js"><link rel="prefetch" href="/assets/js/281.f37d360e.js"><link rel="prefetch" href="/assets/js/282.79b14434.js"><link rel="prefetch" href="/assets/js/283.f30f0408.js"><link rel="prefetch" href="/assets/js/284.d8658140.js"><link rel="prefetch" href="/assets/js/285.be71c641.js"><link rel="prefetch" href="/assets/js/286.579ea834.js"><link rel="prefetch" href="/assets/js/287.0badbfd7.js"><link rel="prefetch" href="/assets/js/288.0a4d6ad8.js"><link rel="prefetch" href="/assets/js/289.b35dc3c7.js"><link rel="prefetch" href="/assets/js/29.79b889e1.js"><link rel="prefetch" href="/assets/js/290.9152baa2.js"><link rel="prefetch" href="/assets/js/291.bb638936.js"><link rel="prefetch" href="/assets/js/292.c0d33103.js"><link rel="prefetch" href="/assets/js/293.36991fc2.js"><link rel="prefetch" href="/assets/js/294.31714e68.js"><link rel="prefetch" href="/assets/js/295.cb156d67.js"><link rel="prefetch" href="/assets/js/296.ade170a2.js"><link rel="prefetch" href="/assets/js/297.a54e7f48.js"><link rel="prefetch" href="/assets/js/298.1625bd2a.js"><link rel="prefetch" href="/assets/js/299.32a89505.js"><link rel="prefetch" href="/assets/js/3.4041ad45.js"><link rel="prefetch" href="/assets/js/30.11c1c4a9.js"><link rel="prefetch" href="/assets/js/300.3018c2a8.js"><link rel="prefetch" href="/assets/js/301.de84e79f.js"><link rel="prefetch" href="/assets/js/302.9240eb69.js"><link rel="prefetch" href="/assets/js/303.7d585f45.js"><link rel="prefetch" href="/assets/js/304.ba1906c2.js"><link rel="prefetch" href="/assets/js/305.f60f1bce.js"><link rel="prefetch" href="/assets/js/306.4393c939.js"><link rel="prefetch" href="/assets/js/307.8368fb0a.js"><link rel="prefetch" href="/assets/js/308.3115a540.js"><link rel="prefetch" href="/assets/js/309.25437db7.js"><link rel="prefetch" href="/assets/js/31.38b56d14.js"><link rel="prefetch" href="/assets/js/310.53dcfa26.js"><link rel="prefetch" href="/assets/js/311.78c0bfab.js"><link rel="prefetch" href="/assets/js/312.26c3bcdb.js"><link rel="prefetch" href="/assets/js/313.dc39f08b.js"><link rel="prefetch" href="/assets/js/314.655c6ac0.js"><link rel="prefetch" href="/assets/js/315.e35fac2d.js"><link rel="prefetch" href="/assets/js/316.a9f25bd7.js"><link rel="prefetch" href="/assets/js/317.8b6a01a8.js"><link rel="prefetch" href="/assets/js/318.b77d87e1.js"><link rel="prefetch" href="/assets/js/319.b2cc794b.js"><link rel="prefetch" href="/assets/js/32.51c932ff.js"><link rel="prefetch" href="/assets/js/320.af5899d0.js"><link rel="prefetch" href="/assets/js/321.2f00f711.js"><link rel="prefetch" href="/assets/js/322.c935286c.js"><link rel="prefetch" href="/assets/js/323.9518f075.js"><link rel="prefetch" href="/assets/js/324.8a0cfed6.js"><link rel="prefetch" href="/assets/js/325.4afeea42.js"><link rel="prefetch" href="/assets/js/326.8c15ef2e.js"><link rel="prefetch" href="/assets/js/327.8d999cbb.js"><link rel="prefetch" href="/assets/js/328.523585ed.js"><link rel="prefetch" href="/assets/js/329.cbffae2d.js"><link rel="prefetch" href="/assets/js/33.a4a24c5c.js"><link rel="prefetch" href="/assets/js/330.089b8bda.js"><link rel="prefetch" href="/assets/js/331.f62e76ea.js"><link rel="prefetch" href="/assets/js/332.41ac44fb.js"><link rel="prefetch" href="/assets/js/333.81689199.js"><link rel="prefetch" href="/assets/js/334.629527a5.js"><link rel="prefetch" href="/assets/js/335.51f4a3fd.js"><link rel="prefetch" href="/assets/js/336.b4ee5265.js"><link rel="prefetch" href="/assets/js/337.a65084e0.js"><link rel="prefetch" href="/assets/js/338.b8c7f9d4.js"><link rel="prefetch" href="/assets/js/339.32dd2a1f.js"><link rel="prefetch" href="/assets/js/34.d5a4cc1c.js"><link rel="prefetch" href="/assets/js/340.181fbac2.js"><link rel="prefetch" href="/assets/js/341.dfbdcd6a.js"><link rel="prefetch" href="/assets/js/342.7e191cd0.js"><link rel="prefetch" href="/assets/js/343.1fc1aa07.js"><link rel="prefetch" href="/assets/js/344.4da6841e.js"><link rel="prefetch" href="/assets/js/345.d9343bc3.js"><link rel="prefetch" href="/assets/js/346.265f5633.js"><link rel="prefetch" href="/assets/js/347.316afdfc.js"><link rel="prefetch" href="/assets/js/348.5b24302c.js"><link rel="prefetch" href="/assets/js/349.be0ea3c0.js"><link rel="prefetch" href="/assets/js/35.9d8a6e6c.js"><link rel="prefetch" href="/assets/js/350.1f5e32d7.js"><link rel="prefetch" href="/assets/js/351.4a655e93.js"><link rel="prefetch" href="/assets/js/352.50b2e64e.js"><link rel="prefetch" href="/assets/js/353.4cc0f899.js"><link rel="prefetch" href="/assets/js/354.8fffb5f9.js"><link rel="prefetch" href="/assets/js/355.b7fd8cb7.js"><link rel="prefetch" href="/assets/js/356.c211c1b8.js"><link rel="prefetch" href="/assets/js/357.d16a2d83.js"><link rel="prefetch" href="/assets/js/358.d7237c49.js"><link rel="prefetch" href="/assets/js/359.6734cad1.js"><link rel="prefetch" href="/assets/js/36.4b6cd3a4.js"><link rel="prefetch" href="/assets/js/360.a33983a6.js"><link rel="prefetch" href="/assets/js/361.6ff246f0.js"><link rel="prefetch" href="/assets/js/362.2f864708.js"><link rel="prefetch" href="/assets/js/363.c392c5e8.js"><link rel="prefetch" href="/assets/js/364.22a6b189.js"><link rel="prefetch" href="/assets/js/365.df1dbb8e.js"><link rel="prefetch" href="/assets/js/366.0d2547a8.js"><link rel="prefetch" href="/assets/js/367.34d64bf7.js"><link rel="prefetch" href="/assets/js/368.5a888040.js"><link rel="prefetch" href="/assets/js/369.9e4cf3f4.js"><link rel="prefetch" href="/assets/js/37.9577c26e.js"><link rel="prefetch" href="/assets/js/370.12a47033.js"><link rel="prefetch" href="/assets/js/371.6d43aa3f.js"><link rel="prefetch" href="/assets/js/372.f52d346f.js"><link rel="prefetch" href="/assets/js/373.f47542f8.js"><link rel="prefetch" href="/assets/js/374.d466d79e.js"><link rel="prefetch" href="/assets/js/375.cd11d66c.js"><link rel="prefetch" href="/assets/js/376.938c07c7.js"><link rel="prefetch" href="/assets/js/377.03fe1608.js"><link rel="prefetch" href="/assets/js/378.ed6d6894.js"><link rel="prefetch" href="/assets/js/379.bfe54af9.js"><link rel="prefetch" href="/assets/js/38.c179026f.js"><link rel="prefetch" href="/assets/js/380.a41a6355.js"><link rel="prefetch" href="/assets/js/381.ae2d3065.js"><link rel="prefetch" href="/assets/js/382.304163b7.js"><link rel="prefetch" href="/assets/js/383.7b17d4a7.js"><link rel="prefetch" href="/assets/js/384.6cfe10eb.js"><link rel="prefetch" href="/assets/js/385.e359cc49.js"><link rel="prefetch" href="/assets/js/386.63eb6322.js"><link rel="prefetch" href="/assets/js/387.d17554a7.js"><link rel="prefetch" href="/assets/js/388.a81f4ee2.js"><link rel="prefetch" href="/assets/js/389.cb8229cf.js"><link rel="prefetch" href="/assets/js/39.147cc35f.js"><link rel="prefetch" href="/assets/js/390.5e754f5e.js"><link rel="prefetch" href="/assets/js/391.956c8e39.js"><link rel="prefetch" href="/assets/js/392.c83337ff.js"><link rel="prefetch" href="/assets/js/393.5d36dd95.js"><link rel="prefetch" href="/assets/js/394.f0167b91.js"><link rel="prefetch" href="/assets/js/395.8478f3f6.js"><link rel="prefetch" href="/assets/js/396.e6fea9c1.js"><link rel="prefetch" href="/assets/js/397.0a467034.js"><link rel="prefetch" href="/assets/js/398.0dd0e255.js"><link rel="prefetch" href="/assets/js/399.f19817b8.js"><link rel="prefetch" href="/assets/js/4.3ac1d119.js"><link rel="prefetch" href="/assets/js/40.39968cf8.js"><link rel="prefetch" href="/assets/js/400.567a684b.js"><link rel="prefetch" href="/assets/js/401.41cf5537.js"><link rel="prefetch" href="/assets/js/402.89091ee3.js"><link rel="prefetch" href="/assets/js/403.d5d2cbaa.js"><link rel="prefetch" href="/assets/js/404.c7fa04e1.js"><link rel="prefetch" href="/assets/js/405.8f210f6c.js"><link rel="prefetch" href="/assets/js/406.543bb325.js"><link rel="prefetch" href="/assets/js/407.738ee9c2.js"><link rel="prefetch" href="/assets/js/408.e32a6f90.js"><link rel="prefetch" href="/assets/js/409.b73ca551.js"><link rel="prefetch" href="/assets/js/41.1d56c643.js"><link rel="prefetch" href="/assets/js/410.284b9e06.js"><link rel="prefetch" href="/assets/js/411.7cca2664.js"><link rel="prefetch" href="/assets/js/412.f05bbf63.js"><link rel="prefetch" href="/assets/js/413.d0550ac2.js"><link rel="prefetch" href="/assets/js/414.dca9673f.js"><link rel="prefetch" href="/assets/js/415.298c5573.js"><link rel="prefetch" href="/assets/js/416.8dff3a66.js"><link rel="prefetch" href="/assets/js/417.8c1a0a4e.js"><link rel="prefetch" href="/assets/js/418.a6b3d607.js"><link rel="prefetch" href="/assets/js/419.a7880b08.js"><link rel="prefetch" href="/assets/js/42.1b306b17.js"><link rel="prefetch" href="/assets/js/420.e17e772d.js"><link rel="prefetch" href="/assets/js/421.64699486.js"><link rel="prefetch" href="/assets/js/422.3f898a85.js"><link rel="prefetch" href="/assets/js/423.aba3c659.js"><link rel="prefetch" href="/assets/js/424.abbcc73e.js"><link rel="prefetch" href="/assets/js/425.3469bdbd.js"><link rel="prefetch" href="/assets/js/426.deb86b4b.js"><link rel="prefetch" href="/assets/js/427.c6faeccf.js"><link rel="prefetch" href="/assets/js/428.ce2143ba.js"><link rel="prefetch" href="/assets/js/429.f6ac264f.js"><link rel="prefetch" href="/assets/js/43.7aca7037.js"><link rel="prefetch" href="/assets/js/430.c5564c52.js"><link rel="prefetch" href="/assets/js/431.8f223d2d.js"><link rel="prefetch" href="/assets/js/432.ba138351.js"><link rel="prefetch" href="/assets/js/433.17cccc55.js"><link rel="prefetch" href="/assets/js/434.313a2983.js"><link rel="prefetch" href="/assets/js/435.4e75644a.js"><link rel="prefetch" href="/assets/js/436.ddd5bc6a.js"><link rel="prefetch" href="/assets/js/437.7fc374a2.js"><link rel="prefetch" href="/assets/js/438.8f3290a7.js"><link rel="prefetch" href="/assets/js/439.0d98dc66.js"><link rel="prefetch" href="/assets/js/44.5bb17146.js"><link rel="prefetch" href="/assets/js/440.0555189f.js"><link rel="prefetch" href="/assets/js/441.8583cfc0.js"><link rel="prefetch" href="/assets/js/442.bf7b236d.js"><link rel="prefetch" href="/assets/js/443.b9c86fb4.js"><link rel="prefetch" href="/assets/js/444.1a388c82.js"><link rel="prefetch" href="/assets/js/445.f0b67036.js"><link rel="prefetch" href="/assets/js/446.8c732c56.js"><link rel="prefetch" href="/assets/js/447.6d1be8d9.js"><link rel="prefetch" href="/assets/js/448.3757ac1c.js"><link rel="prefetch" href="/assets/js/449.a62b518b.js"><link rel="prefetch" href="/assets/js/45.8cb3fc60.js"><link rel="prefetch" href="/assets/js/450.88b22b7d.js"><link rel="prefetch" href="/assets/js/451.9e75dd3b.js"><link rel="prefetch" href="/assets/js/452.73b48879.js"><link rel="prefetch" href="/assets/js/453.932fa75e.js"><link rel="prefetch" href="/assets/js/454.14027c6b.js"><link rel="prefetch" href="/assets/js/455.4be47b6a.js"><link rel="prefetch" href="/assets/js/456.ee0b078b.js"><link rel="prefetch" href="/assets/js/457.f0bac5b8.js"><link rel="prefetch" href="/assets/js/458.27022088.js"><link rel="prefetch" href="/assets/js/459.ab2f4695.js"><link rel="prefetch" href="/assets/js/46.471ff9b5.js"><link rel="prefetch" href="/assets/js/460.11e77a24.js"><link rel="prefetch" href="/assets/js/461.194f79d1.js"><link rel="prefetch" href="/assets/js/462.80a34b65.js"><link rel="prefetch" href="/assets/js/463.f66a897c.js"><link rel="prefetch" href="/assets/js/464.45bb7abf.js"><link rel="prefetch" href="/assets/js/465.c38904d3.js"><link rel="prefetch" href="/assets/js/466.0c8a1201.js"><link rel="prefetch" href="/assets/js/467.c8b86791.js"><link rel="prefetch" href="/assets/js/468.70a2a91e.js"><link rel="prefetch" href="/assets/js/469.8bbe4ac4.js"><link rel="prefetch" href="/assets/js/47.5ce5c3c2.js"><link rel="prefetch" href="/assets/js/470.fde0c73c.js"><link rel="prefetch" href="/assets/js/471.0be5c4ef.js"><link rel="prefetch" href="/assets/js/472.7e1c6165.js"><link rel="prefetch" href="/assets/js/473.39c8df5e.js"><link rel="prefetch" href="/assets/js/474.29c08fbe.js"><link rel="prefetch" href="/assets/js/475.b85cc5be.js"><link rel="prefetch" href="/assets/js/476.d2472e0d.js"><link rel="prefetch" href="/assets/js/477.5c6cbe44.js"><link rel="prefetch" href="/assets/js/478.2c8a78ce.js"><link rel="prefetch" href="/assets/js/479.10db54c8.js"><link rel="prefetch" href="/assets/js/48.63747cc1.js"><link rel="prefetch" href="/assets/js/480.abd30718.js"><link rel="prefetch" href="/assets/js/481.c15ba6c9.js"><link rel="prefetch" href="/assets/js/482.e930790a.js"><link rel="prefetch" href="/assets/js/483.e1b09083.js"><link rel="prefetch" href="/assets/js/484.9b4b5c9c.js"><link rel="prefetch" href="/assets/js/485.36c42239.js"><link rel="prefetch" href="/assets/js/486.73b5ece1.js"><link rel="prefetch" href="/assets/js/487.39c3b9b4.js"><link rel="prefetch" href="/assets/js/488.50f3044f.js"><link rel="prefetch" href="/assets/js/489.5a922985.js"><link rel="prefetch" href="/assets/js/49.c8a3f2ca.js"><link rel="prefetch" href="/assets/js/490.c6845b01.js"><link rel="prefetch" href="/assets/js/491.2863d479.js"><link rel="prefetch" href="/assets/js/492.b03e4127.js"><link rel="prefetch" href="/assets/js/493.db71ed8b.js"><link rel="prefetch" href="/assets/js/494.9608a92e.js"><link rel="prefetch" href="/assets/js/495.ced07f96.js"><link rel="prefetch" href="/assets/js/496.185decf2.js"><link rel="prefetch" href="/assets/js/497.ddc55f9e.js"><link rel="prefetch" href="/assets/js/498.43397afd.js"><link rel="prefetch" href="/assets/js/499.7320fe31.js"><link rel="prefetch" href="/assets/js/5.3134845c.js"><link rel="prefetch" href="/assets/js/50.68b1ba39.js"><link rel="prefetch" href="/assets/js/500.41912532.js"><link rel="prefetch" href="/assets/js/501.5a2c44ad.js"><link rel="prefetch" href="/assets/js/502.3e92b592.js"><link rel="prefetch" href="/assets/js/503.a909f346.js"><link rel="prefetch" href="/assets/js/504.1677e8f8.js"><link rel="prefetch" href="/assets/js/505.ea1a1845.js"><link rel="prefetch" href="/assets/js/506.ea559644.js"><link rel="prefetch" href="/assets/js/507.757d384a.js"><link rel="prefetch" href="/assets/js/508.3233323c.js"><link rel="prefetch" href="/assets/js/509.cd881c95.js"><link rel="prefetch" href="/assets/js/51.840f1c2d.js"><link rel="prefetch" href="/assets/js/510.693e3154.js"><link rel="prefetch" href="/assets/js/511.0726668e.js"><link rel="prefetch" href="/assets/js/512.544f9351.js"><link rel="prefetch" href="/assets/js/513.09e693c1.js"><link rel="prefetch" href="/assets/js/514.9ea29f17.js"><link rel="prefetch" href="/assets/js/515.ae2cfef3.js"><link rel="prefetch" href="/assets/js/516.332ac4f7.js"><link rel="prefetch" href="/assets/js/517.afd3f56e.js"><link rel="prefetch" href="/assets/js/518.925058af.js"><link rel="prefetch" href="/assets/js/519.0e0f3b50.js"><link rel="prefetch" href="/assets/js/52.037d8e07.js"><link rel="prefetch" href="/assets/js/520.91f935f6.js"><link rel="prefetch" href="/assets/js/521.686c1eb6.js"><link rel="prefetch" href="/assets/js/522.554cbfe5.js"><link rel="prefetch" href="/assets/js/523.48e15e4e.js"><link rel="prefetch" href="/assets/js/524.52d7dd1c.js"><link rel="prefetch" href="/assets/js/525.eb2d041a.js"><link rel="prefetch" href="/assets/js/526.18d14b74.js"><link rel="prefetch" href="/assets/js/527.72005be4.js"><link rel="prefetch" href="/assets/js/528.a927bfeb.js"><link rel="prefetch" href="/assets/js/529.577cf1e8.js"><link rel="prefetch" href="/assets/js/53.1fd6d004.js"><link rel="prefetch" href="/assets/js/530.8214546e.js"><link rel="prefetch" href="/assets/js/531.188187ee.js"><link rel="prefetch" href="/assets/js/532.0d33ab06.js"><link rel="prefetch" href="/assets/js/533.713b971d.js"><link rel="prefetch" href="/assets/js/534.4c3f120d.js"><link rel="prefetch" href="/assets/js/535.f75b00e3.js"><link rel="prefetch" href="/assets/js/536.19cd9376.js"><link rel="prefetch" href="/assets/js/537.c578cfb6.js"><link rel="prefetch" href="/assets/js/538.2e3d2af3.js"><link rel="prefetch" href="/assets/js/539.ced32c6c.js"><link rel="prefetch" href="/assets/js/54.16da8f26.js"><link rel="prefetch" href="/assets/js/540.1cdaa436.js"><link rel="prefetch" href="/assets/js/541.79472898.js"><link rel="prefetch" href="/assets/js/542.c19e66bf.js"><link rel="prefetch" href="/assets/js/543.8c7ba033.js"><link rel="prefetch" href="/assets/js/544.e4d63122.js"><link rel="prefetch" href="/assets/js/545.990cf62f.js"><link rel="prefetch" href="/assets/js/546.c450bfe9.js"><link rel="prefetch" href="/assets/js/547.5f9d5d3c.js"><link rel="prefetch" href="/assets/js/548.da38e02f.js"><link rel="prefetch" href="/assets/js/549.4e7ef51b.js"><link rel="prefetch" href="/assets/js/55.889a99a0.js"><link rel="prefetch" href="/assets/js/550.fe38cd54.js"><link rel="prefetch" href="/assets/js/551.a962ba39.js"><link rel="prefetch" href="/assets/js/552.ea9b8c68.js"><link rel="prefetch" href="/assets/js/553.bb05d44b.js"><link rel="prefetch" href="/assets/js/554.0058fb25.js"><link rel="prefetch" href="/assets/js/555.b9c01360.js"><link rel="prefetch" href="/assets/js/556.90c1efbd.js"><link rel="prefetch" href="/assets/js/557.e91bab69.js"><link rel="prefetch" href="/assets/js/558.117e05db.js"><link rel="prefetch" href="/assets/js/559.ffc1daab.js"><link rel="prefetch" href="/assets/js/56.0d815a94.js"><link rel="prefetch" href="/assets/js/560.f1afe080.js"><link rel="prefetch" href="/assets/js/561.58ef4b9a.js"><link rel="prefetch" href="/assets/js/562.2aea2b58.js"><link rel="prefetch" href="/assets/js/563.9fa5d88a.js"><link rel="prefetch" href="/assets/js/564.9009c1a9.js"><link rel="prefetch" href="/assets/js/565.cc151c74.js"><link rel="prefetch" href="/assets/js/566.6d1dc45c.js"><link rel="prefetch" href="/assets/js/567.72cd3a82.js"><link rel="prefetch" href="/assets/js/568.dce9b1bc.js"><link rel="prefetch" href="/assets/js/569.47798dcf.js"><link rel="prefetch" href="/assets/js/57.4af72b89.js"><link rel="prefetch" href="/assets/js/570.c076c67b.js"><link rel="prefetch" href="/assets/js/571.271e5add.js"><link rel="prefetch" href="/assets/js/572.948d49fb.js"><link rel="prefetch" href="/assets/js/573.eede7803.js"><link rel="prefetch" href="/assets/js/574.040dcefc.js"><link rel="prefetch" href="/assets/js/575.d6c8a336.js"><link rel="prefetch" href="/assets/js/576.cb9de798.js"><link rel="prefetch" href="/assets/js/577.c6c660be.js"><link rel="prefetch" href="/assets/js/578.22177af2.js"><link rel="prefetch" href="/assets/js/579.1a6bf5b8.js"><link rel="prefetch" href="/assets/js/58.df984a43.js"><link rel="prefetch" href="/assets/js/580.7e4d3cb7.js"><link rel="prefetch" href="/assets/js/581.c149fae7.js"><link rel="prefetch" href="/assets/js/582.55545a35.js"><link rel="prefetch" href="/assets/js/583.a31c53bf.js"><link rel="prefetch" href="/assets/js/584.36626d3d.js"><link rel="prefetch" href="/assets/js/585.fd277c61.js"><link rel="prefetch" href="/assets/js/586.52204405.js"><link rel="prefetch" href="/assets/js/587.99335ca1.js"><link rel="prefetch" href="/assets/js/588.c1f2b2fa.js"><link rel="prefetch" href="/assets/js/589.7feb7eb3.js"><link rel="prefetch" href="/assets/js/59.c65322cc.js"><link rel="prefetch" href="/assets/js/590.08417ac3.js"><link rel="prefetch" href="/assets/js/591.9cb3207b.js"><link rel="prefetch" href="/assets/js/592.8595a9f4.js"><link rel="prefetch" href="/assets/js/593.605c39df.js"><link rel="prefetch" href="/assets/js/594.cc98eb1f.js"><link rel="prefetch" href="/assets/js/595.8ae0a32e.js"><link rel="prefetch" href="/assets/js/596.b34683c2.js"><link rel="prefetch" href="/assets/js/597.ce917673.js"><link rel="prefetch" href="/assets/js/598.1b31cbf3.js"><link rel="prefetch" href="/assets/js/599.11899b1a.js"><link rel="prefetch" href="/assets/js/6.0c96ad77.js"><link rel="prefetch" href="/assets/js/60.77f5b873.js"><link rel="prefetch" href="/assets/js/600.36a408d5.js"><link rel="prefetch" href="/assets/js/601.23dcd251.js"><link rel="prefetch" href="/assets/js/602.64cf1e80.js"><link rel="prefetch" href="/assets/js/603.502d8c92.js"><link rel="prefetch" href="/assets/js/604.e660e01e.js"><link rel="prefetch" href="/assets/js/605.2403db7a.js"><link rel="prefetch" href="/assets/js/606.fad28edc.js"><link rel="prefetch" href="/assets/js/607.9c6f2afe.js"><link rel="prefetch" href="/assets/js/608.734afab0.js"><link rel="prefetch" href="/assets/js/609.4c1a6347.js"><link rel="prefetch" href="/assets/js/61.2d4e6a1a.js"><link rel="prefetch" href="/assets/js/610.312082cc.js"><link rel="prefetch" href="/assets/js/611.117258e3.js"><link rel="prefetch" href="/assets/js/612.dbff59f2.js"><link rel="prefetch" href="/assets/js/613.07f207a5.js"><link rel="prefetch" href="/assets/js/614.7690add3.js"><link rel="prefetch" href="/assets/js/615.18c81803.js"><link rel="prefetch" href="/assets/js/616.f6f1bae0.js"><link rel="prefetch" href="/assets/js/617.15209f2e.js"><link rel="prefetch" href="/assets/js/618.4a9d5e62.js"><link rel="prefetch" href="/assets/js/619.4507e3c7.js"><link rel="prefetch" href="/assets/js/62.45203677.js"><link rel="prefetch" href="/assets/js/620.3ccbca1a.js"><link rel="prefetch" href="/assets/js/621.d08dbba4.js"><link rel="prefetch" href="/assets/js/622.a46d6f6c.js"><link rel="prefetch" href="/assets/js/623.9820d99d.js"><link rel="prefetch" href="/assets/js/624.60571315.js"><link rel="prefetch" href="/assets/js/625.da6f7154.js"><link rel="prefetch" href="/assets/js/626.73701a4d.js"><link rel="prefetch" href="/assets/js/627.aa3ae061.js"><link rel="prefetch" href="/assets/js/628.d2296bee.js"><link rel="prefetch" href="/assets/js/629.db9ed29f.js"><link rel="prefetch" href="/assets/js/63.77bcb425.js"><link rel="prefetch" href="/assets/js/630.82ed32d5.js"><link rel="prefetch" href="/assets/js/631.e9b80c2c.js"><link rel="prefetch" href="/assets/js/632.92997d70.js"><link rel="prefetch" href="/assets/js/633.e66c3cbc.js"><link rel="prefetch" href="/assets/js/634.b314ab80.js"><link rel="prefetch" href="/assets/js/635.cae0b43a.js"><link rel="prefetch" href="/assets/js/636.5e41c48d.js"><link rel="prefetch" href="/assets/js/637.45799a46.js"><link rel="prefetch" href="/assets/js/638.8dbb86f2.js"><link rel="prefetch" href="/assets/js/639.f3dc3d54.js"><link rel="prefetch" href="/assets/js/64.48a44ac7.js"><link rel="prefetch" href="/assets/js/640.b538e6a5.js"><link rel="prefetch" href="/assets/js/641.291cd95e.js"><link rel="prefetch" href="/assets/js/642.caab5dfd.js"><link rel="prefetch" href="/assets/js/643.6a4e5909.js"><link rel="prefetch" href="/assets/js/644.395e1509.js"><link rel="prefetch" href="/assets/js/645.df897a52.js"><link rel="prefetch" href="/assets/js/646.07bb34ad.js"><link rel="prefetch" href="/assets/js/647.40581df6.js"><link rel="prefetch" href="/assets/js/648.cc78ed85.js"><link rel="prefetch" href="/assets/js/649.6c9bfe91.js"><link rel="prefetch" href="/assets/js/65.594d727c.js"><link rel="prefetch" href="/assets/js/650.6a7966d1.js"><link rel="prefetch" href="/assets/js/651.cdbe3a61.js"><link rel="prefetch" href="/assets/js/66.697e3989.js"><link rel="prefetch" href="/assets/js/67.a3eb57a2.js"><link rel="prefetch" href="/assets/js/68.e0579512.js"><link rel="prefetch" href="/assets/js/69.c53a8af2.js"><link rel="prefetch" href="/assets/js/70.1af2e395.js"><link rel="prefetch" href="/assets/js/71.47eb643b.js"><link rel="prefetch" href="/assets/js/72.5262f104.js"><link rel="prefetch" href="/assets/js/73.d07f6ff2.js"><link rel="prefetch" href="/assets/js/74.00d2de22.js"><link rel="prefetch" href="/assets/js/75.69ac6b6c.js"><link rel="prefetch" href="/assets/js/76.c05cd6a7.js"><link rel="prefetch" href="/assets/js/77.d5222458.js"><link rel="prefetch" href="/assets/js/78.630fb592.js"><link rel="prefetch" href="/assets/js/79.c188f8e3.js"><link rel="prefetch" href="/assets/js/8.9176dd4b.js"><link rel="prefetch" href="/assets/js/80.c0771c09.js"><link rel="prefetch" href="/assets/js/81.5893998e.js"><link rel="prefetch" href="/assets/js/82.6d490909.js"><link rel="prefetch" href="/assets/js/83.9bd41235.js"><link rel="prefetch" href="/assets/js/84.8f0298e2.js"><link rel="prefetch" href="/assets/js/85.150db05f.js"><link rel="prefetch" href="/assets/js/86.af375a15.js"><link rel="prefetch" href="/assets/js/87.07e03e84.js"><link rel="prefetch" href="/assets/js/88.2cc7eabb.js"><link rel="prefetch" href="/assets/js/89.001203fd.js"><link rel="prefetch" href="/assets/js/9.420da603.js"><link rel="prefetch" href="/assets/js/90.4ae35882.js"><link rel="prefetch" href="/assets/js/91.3da12e9b.js"><link rel="prefetch" href="/assets/js/92.d58a0bce.js"><link rel="prefetch" href="/assets/js/93.b354f9d1.js"><link rel="prefetch" href="/assets/js/94.d41623fe.js"><link rel="prefetch" href="/assets/js/95.8b4907f1.js"><link rel="prefetch" href="/assets/js/96.d23892fd.js"><link rel="prefetch" href="/assets/js/97.914b3374.js"><link rel="prefetch" href="/assets/js/98.957d1e13.js"><link rel="prefetch" href="/assets/js/99.c9c3f20a.js">
    <link rel="stylesheet" href="/assets/css/0.styles.793b538b.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-1aefc0b4><div data-v-1aefc0b4><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1aefc0b4 data-v-1aefc0b4><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-25ba6db2 data-v-1aefc0b4 data-v-1aefc0b4><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>Robby·爱编程</h3> <p class="description" data-v-25ba6db2 data-v-25ba6db2> </p> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>尹欢一的技术博客</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2023
        </a></span></div></div> <div class="hide" data-v-1aefc0b4><header class="navbar" data-v-1aefc0b4><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/nav-logo.png" alt="Robby·爱编程" class="logo"> <span class="site-name">Robby·爱编程</span></a> <div class="links"><!----> <div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-menu"></i>
      计算机理论
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/theory/algorithm/" class="nav-link"><i class="undefined"></i>
  算法与数据结构(Golang版本)
</a></li><li class="dropdown-item"><!----> <a href="/theory/algorithmic-thought/" class="nav-link"><i class="undefined"></i>
  算法思想分类
</a></li><li class="dropdown-item"><!----> <a href="/theory/network/" class="nav-link"><i class="undefined"></i>
  计算机网络
</a></li><li class="dropdown-item"><!----> <a href="/theory/system/" class="nav-link"><i class="undefined"></i>
  操作系统
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-document"></i>
      编程开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>Python开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/python/basic/" class="nav-link"><i class="undefined"></i>
  Python开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/python/machine-leaning/" class="nav-link router-link-active"><i class="undefined"></i>
  Python机器学习
</a></li><li class="dropdown-subitem"><a href="/dev/python/third-lib/" class="nav-link"><i class="undefined"></i>
  Python第三方库
</a></li><li class="dropdown-subitem"><a href="/dev/python/scrapy/" class="nav-link"><i class="undefined"></i>
  Python爬虫
</a></li><li class="dropdown-subitem"><a href="/dev/python/deefuture/" class="nav-link"><i class="undefined"></i>
  Python Deefuture教育平台
</a></li><li class="dropdown-subitem"><a href="/dev/python/shop/" class="nav-link"><i class="undefined"></i>
  Python电商平台
</a></li><li class="dropdown-subitem"><a href="/dev/python/coroutine/" class="nav-link"><i class="undefined"></i>
  Python协程开发
</a></li><li class="dropdown-subitem"><a href="/dev/python/tornado/" class="nav-link"><i class="undefined"></i>
  Python Tornado开发
</a></li><li class="dropdown-subitem"><a href="/dev/python/third/" class="nav-link"><i class="undefined"></i>
  Python第三方包
</a></li><li class="dropdown-subitem"><a href="/dev/python/sqlalchemy/" class="nav-link"><i class="undefined"></i>
  Python SQLAlchemy
</a></li></ul></li><li class="dropdown-item"><h4>Golang开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/golang/basic/" class="nav-link"><i class="undefined"></i>
  Golang开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/golang/package/" class="nav-link"><i class="undefined"></i>
  Golang第三方包
</a></li><li class="dropdown-subitem"><a href="/dev/golang/source/" class="nav-link"><i class="undefined"></i>
  Golang源码分析
</a></li><li class="dropdown-subitem"><a href="/dev/golang/kill/" class="nav-link"><i class="undefined"></i>
  Golang秒杀系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/task/" class="nav-link"><i class="undefined"></i>
  Golang任务系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/post/" class="nav-link"><i class="undefined"></i>
  Golang帖子系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/network/" class="nav-link"><i class="undefined"></i>
  Golang网关系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/advance/" class="nav-link"><i class="undefined"></i>
  Golang进阶开发
</a></li><li class="dropdown-subitem"><a href="/dev/golang/problem/" class="nav-link"><i class="undefined"></i>
  Golang问题集合
</a></li></ul></li><li class="dropdown-item"><h4>Shell开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/shell/basic/" class="nav-link"><i class="undefined"></i>
  Shell开发基础
</a></li></ul></li><li class="dropdown-item"><h4>Vue开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/vue/layui/" class="nav-link"><i class="undefined"></i>
  Layui开发
</a></li><li class="dropdown-subitem"><a href="/dev/vue/official/" class="nav-link"><i class="undefined"></i>
  Vue2.x官方文档
</a></li><li class="dropdown-subitem"><a href="/dev/vue/ebook/" class="nav-link"><i class="undefined"></i>
  Vue2.x电子书系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/monitor/" class="nav-link"><i class="undefined"></i>
  Vue2.x监控系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/sell/" class="nav-link"><i class="undefined"></i>
  Vue2.x电商系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/vue-element-admin/" class="nav-link"><i class="undefined"></i>
  Vue3.x vue-element-admin开发
</a></li><li class="dropdown-subitem"><a href="/dev/vue/vuepress/" class="nav-link"><i class="undefined"></i>
  VuePress安装配置与发布
</a></li></ul></li><li class="dropdown-item"><h4>Java开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/java/basic/" class="nav-link"><i class="undefined"></i>
  Java开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/java/web/" class="nav-link"><i class="undefined"></i>
  Java Web开发
</a></li></ul></li><li class="dropdown-item"><h4>Scala开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/scala/basic/" class="nav-link"><i class="undefined"></i>
  Scala开发基础
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-home"></i>
      SRE
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>数据库系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/database/sql/" class="nav-link"><i class="undefined"></i>
  SQL语言
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-theory/" class="nav-link"><i class="undefined"></i>
  MySQL理论
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-basic/" class="nav-link"><i class="undefined"></i>
  MySQL基础
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-high/" class="nav-link"><i class="undefined"></i>
  MySQL高可用
</a></li><li class="dropdown-subitem"><a href="/sre/database/redis/" class="nav-link"><i class="undefined"></i>
  Redis
</a></li><li class="dropdown-subitem"><a href="/sre/database/elasticsearch/" class="nav-link"><i class="undefined"></i>
  Elasticsearch
</a></li><li class="dropdown-subitem"><a href="/sre/database/mongodb/" class="nav-link"><i class="undefined"></i>
  MongoDB
</a></li></ul></li><li class="dropdown-item"><h4>接入层系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/interface/nginx/" class="nav-link"><i class="undefined"></i>
  Nginx
</a></li></ul></li><li class="dropdown-item"><h4>监控系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/monitor/zabbix/" class="nav-link"><i class="undefined"></i>
  Zabbix
</a></li><li class="dropdown-subitem"><a href="/sre/monitor/grafana/" class="nav-link"><i class="undefined"></i>
  Prometheus、Influxdb、Grafana
</a></li></ul></li><li class="dropdown-item"><h4>日志系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/log/elk/" class="nav-link"><i class="undefined"></i>
  ELK
</a></li><li class="dropdown-subitem"><a href="/sre/log/elastalert/" class="nav-link"><i class="undefined"></i>
  ElastAlert
</a></li></ul></li><li class="dropdown-item"><h4>发布系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/deploy/ansible/" class="nav-link"><i class="undefined"></i>
  Ansible
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/exec-engine/" class="nav-link"><i class="undefined"></i>
  Exec-Engine
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/jenkins/" class="nav-link"><i class="undefined"></i>
  Jenkins
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/gitlab-ci/" class="nav-link"><i class="undefined"></i>
  Gitlab-CI
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/git/" class="nav-link"><i class="undefined"></i>
  Git
</a></li></ul></li><li class="dropdown-item"><h4>虚拟化系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/virtual/docker/" class="nav-link"><i class="undefined"></i>
  Docker
</a></li><li class="dropdown-subitem"><a href="/sre/virtual/k8s/" class="nav-link"><i class="undefined"></i>
  K8S
</a></li></ul></li><li class="dropdown-item"><h4>数据处理系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/data/spark/" class="nav-link"><i class="undefined"></i>
  Spark
</a></li><li class="dropdown-subitem"><a href="/sre/data/flink/" class="nav-link"><i class="undefined"></i>
  Flink
</a></li><li class="dropdown-subitem"><a href="/sre/data/pyflink/" class="nav-link"><i class="undefined"></i>
  PyFlink
</a></li></ul></li><li class="dropdown-item"><h4>消息队列</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/queue/rabbitmq/" class="nav-link"><i class="undefined"></i>
  RabbitMQ
</a></li><li class="dropdown-subitem"><a href="/sre/queue/kafka/" class="nav-link"><i class="undefined"></i>
  Kafka
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-suggestion"></i>
      系统架构
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/arch/micro/grpc/" class="nav-link"><i class="undefined"></i>
  gRPC微服务
</a></li><li class="dropdown-item"><!----> <a href="/arch/micro/thrift/" class="nav-link"><i class="undefined"></i>
  Thrift微服务
</a></li><li class="dropdown-item"><!----> <a href="/arch/sso/oauth2/" class="nav-link"><i class="undefined"></i>
  SSO单点登录系统
</a></li></ul></div></div><div class="nav-item"><a href="/me/" class="nav-link"><i class="iconfont reco-user"></i>
  关于
</a></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1aefc0b4></div> <aside class="sidebar" data-v-1aefc0b4><div class="personal-info-wrapper" data-v-39576ba9 data-v-1aefc0b4><img src="/img/user-profile.jpeg" alt="author-avatar" class="personal-img" data-v-39576ba9> <h3 class="name" data-v-39576ba9>
    尹欢一的技术博客
  </h3> <div class="num" data-v-39576ba9><div data-v-39576ba9><h3 data-v-39576ba9>641</h3> <h6 data-v-39576ba9>文章</h6></div> <div data-v-39576ba9><h3 data-v-39576ba9>0</h3> <h6 data-v-39576ba9>标签</h6></div></div> <ul class="social-links" data-v-39576ba9></ul> <hr data-v-39576ba9></div> <nav class="nav-links"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-menu"></i>
      计算机理论
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/theory/algorithm/" class="nav-link"><i class="undefined"></i>
  算法与数据结构(Golang版本)
</a></li><li class="dropdown-item"><!----> <a href="/theory/algorithmic-thought/" class="nav-link"><i class="undefined"></i>
  算法思想分类
</a></li><li class="dropdown-item"><!----> <a href="/theory/network/" class="nav-link"><i class="undefined"></i>
  计算机网络
</a></li><li class="dropdown-item"><!----> <a href="/theory/system/" class="nav-link"><i class="undefined"></i>
  操作系统
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-document"></i>
      编程开发
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>Python开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/python/basic/" class="nav-link"><i class="undefined"></i>
  Python开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/python/machine-leaning/" class="nav-link router-link-active"><i class="undefined"></i>
  Python机器学习
</a></li><li class="dropdown-subitem"><a href="/dev/python/third-lib/" class="nav-link"><i class="undefined"></i>
  Python第三方库
</a></li><li class="dropdown-subitem"><a href="/dev/python/scrapy/" class="nav-link"><i class="undefined"></i>
  Python爬虫
</a></li><li class="dropdown-subitem"><a href="/dev/python/deefuture/" class="nav-link"><i class="undefined"></i>
  Python Deefuture教育平台
</a></li><li class="dropdown-subitem"><a href="/dev/python/shop/" class="nav-link"><i class="undefined"></i>
  Python电商平台
</a></li><li class="dropdown-subitem"><a href="/dev/python/coroutine/" class="nav-link"><i class="undefined"></i>
  Python协程开发
</a></li><li class="dropdown-subitem"><a href="/dev/python/tornado/" class="nav-link"><i class="undefined"></i>
  Python Tornado开发
</a></li><li class="dropdown-subitem"><a href="/dev/python/third/" class="nav-link"><i class="undefined"></i>
  Python第三方包
</a></li><li class="dropdown-subitem"><a href="/dev/python/sqlalchemy/" class="nav-link"><i class="undefined"></i>
  Python SQLAlchemy
</a></li></ul></li><li class="dropdown-item"><h4>Golang开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/golang/basic/" class="nav-link"><i class="undefined"></i>
  Golang开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/golang/package/" class="nav-link"><i class="undefined"></i>
  Golang第三方包
</a></li><li class="dropdown-subitem"><a href="/dev/golang/source/" class="nav-link"><i class="undefined"></i>
  Golang源码分析
</a></li><li class="dropdown-subitem"><a href="/dev/golang/kill/" class="nav-link"><i class="undefined"></i>
  Golang秒杀系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/task/" class="nav-link"><i class="undefined"></i>
  Golang任务系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/post/" class="nav-link"><i class="undefined"></i>
  Golang帖子系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/network/" class="nav-link"><i class="undefined"></i>
  Golang网关系统
</a></li><li class="dropdown-subitem"><a href="/dev/golang/advance/" class="nav-link"><i class="undefined"></i>
  Golang进阶开发
</a></li><li class="dropdown-subitem"><a href="/dev/golang/problem/" class="nav-link"><i class="undefined"></i>
  Golang问题集合
</a></li></ul></li><li class="dropdown-item"><h4>Shell开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/shell/basic/" class="nav-link"><i class="undefined"></i>
  Shell开发基础
</a></li></ul></li><li class="dropdown-item"><h4>Vue开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/vue/layui/" class="nav-link"><i class="undefined"></i>
  Layui开发
</a></li><li class="dropdown-subitem"><a href="/dev/vue/official/" class="nav-link"><i class="undefined"></i>
  Vue2.x官方文档
</a></li><li class="dropdown-subitem"><a href="/dev/vue/ebook/" class="nav-link"><i class="undefined"></i>
  Vue2.x电子书系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/monitor/" class="nav-link"><i class="undefined"></i>
  Vue2.x监控系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/sell/" class="nav-link"><i class="undefined"></i>
  Vue2.x电商系统
</a></li><li class="dropdown-subitem"><a href="/dev/vue/vue-element-admin/" class="nav-link"><i class="undefined"></i>
  Vue3.x vue-element-admin开发
</a></li><li class="dropdown-subitem"><a href="/dev/vue/vuepress/" class="nav-link"><i class="undefined"></i>
  VuePress安装配置与发布
</a></li></ul></li><li class="dropdown-item"><h4>Java开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/java/basic/" class="nav-link"><i class="undefined"></i>
  Java开发基础
</a></li><li class="dropdown-subitem"><a href="/dev/java/web/" class="nav-link"><i class="undefined"></i>
  Java Web开发
</a></li></ul></li><li class="dropdown-item"><h4>Scala开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/dev/scala/basic/" class="nav-link"><i class="undefined"></i>
  Scala开发基础
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-home"></i>
      SRE
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>数据库系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/database/sql/" class="nav-link"><i class="undefined"></i>
  SQL语言
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-theory/" class="nav-link"><i class="undefined"></i>
  MySQL理论
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-basic/" class="nav-link"><i class="undefined"></i>
  MySQL基础
</a></li><li class="dropdown-subitem"><a href="/sre/database/mysql-high/" class="nav-link"><i class="undefined"></i>
  MySQL高可用
</a></li><li class="dropdown-subitem"><a href="/sre/database/redis/" class="nav-link"><i class="undefined"></i>
  Redis
</a></li><li class="dropdown-subitem"><a href="/sre/database/elasticsearch/" class="nav-link"><i class="undefined"></i>
  Elasticsearch
</a></li><li class="dropdown-subitem"><a href="/sre/database/mongodb/" class="nav-link"><i class="undefined"></i>
  MongoDB
</a></li></ul></li><li class="dropdown-item"><h4>接入层系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/interface/nginx/" class="nav-link"><i class="undefined"></i>
  Nginx
</a></li></ul></li><li class="dropdown-item"><h4>监控系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/monitor/zabbix/" class="nav-link"><i class="undefined"></i>
  Zabbix
</a></li><li class="dropdown-subitem"><a href="/sre/monitor/grafana/" class="nav-link"><i class="undefined"></i>
  Prometheus、Influxdb、Grafana
</a></li></ul></li><li class="dropdown-item"><h4>日志系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/log/elk/" class="nav-link"><i class="undefined"></i>
  ELK
</a></li><li class="dropdown-subitem"><a href="/sre/log/elastalert/" class="nav-link"><i class="undefined"></i>
  ElastAlert
</a></li></ul></li><li class="dropdown-item"><h4>发布系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/deploy/ansible/" class="nav-link"><i class="undefined"></i>
  Ansible
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/exec-engine/" class="nav-link"><i class="undefined"></i>
  Exec-Engine
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/jenkins/" class="nav-link"><i class="undefined"></i>
  Jenkins
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/gitlab-ci/" class="nav-link"><i class="undefined"></i>
  Gitlab-CI
</a></li><li class="dropdown-subitem"><a href="/sre/deploy/git/" class="nav-link"><i class="undefined"></i>
  Git
</a></li></ul></li><li class="dropdown-item"><h4>虚拟化系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/virtual/docker/" class="nav-link"><i class="undefined"></i>
  Docker
</a></li><li class="dropdown-subitem"><a href="/sre/virtual/k8s/" class="nav-link"><i class="undefined"></i>
  K8S
</a></li></ul></li><li class="dropdown-item"><h4>数据处理系统</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/data/spark/" class="nav-link"><i class="undefined"></i>
  Spark
</a></li><li class="dropdown-subitem"><a href="/sre/data/flink/" class="nav-link"><i class="undefined"></i>
  Flink
</a></li><li class="dropdown-subitem"><a href="/sre/data/pyflink/" class="nav-link"><i class="undefined"></i>
  PyFlink
</a></li></ul></li><li class="dropdown-item"><h4>消息队列</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/sre/queue/rabbitmq/" class="nav-link"><i class="undefined"></i>
  RabbitMQ
</a></li><li class="dropdown-subitem"><a href="/sre/queue/kafka/" class="nav-link"><i class="undefined"></i>
  Kafka
</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-suggestion"></i>
      系统架构
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/arch/micro/grpc/" class="nav-link"><i class="undefined"></i>
  gRPC微服务
</a></li><li class="dropdown-item"><!----> <a href="/arch/micro/thrift/" class="nav-link"><i class="undefined"></i>
  Thrift微服务
</a></li><li class="dropdown-item"><!----> <a href="/arch/sso/oauth2/" class="nav-link"><i class="undefined"></i>
  SSO单点登录系统
</a></li></ul></div></div><div class="nav-item"><a href="/me/" class="nav-link"><i class="iconfont reco-user"></i>
  关于
</a></div> <!----></nav> <ul class="sidebar-links"><li><a href="/dev/python/machine-leaning/docs/concept/concept.html" class="sidebar-link">机器学习概念</a></li><li><a href="/dev/python/machine-leaning/docs/machine-method1/machine-method1.html" class="sidebar-link">机器学习的方法分类1</a></li><li><a href="/dev/python/machine-leaning/docs/machine-method2/machine-method2.html" class="sidebar-link">机器学习的方法分类2</a></li><li><a href="/dev/python/machine-leaning/docs/machine-thinking/machine-thinking.html" class="sidebar-link">机器学习的哲学思考</a></li><li><a href="/dev/python/machine-leaning/docs/machine-use/machine-use.html" class="sidebar-link">Numpy模块的使用</a></li><li><a href="/dev/python/machine-leaning/docs/machine-cal/machine-cal.html" class="sidebar-link">Numpy模块的运算</a></li><li><a href="/dev/python/machine-leaning/docs/machine-together-cal/machine-together-cal.html" class="sidebar-link">Numpy中的聚合运算</a></li><li><a href="/dev/python/machine-leaning/docs/machine-index/machine-index.html" class="sidebar-link">Numpy中的索引</a></li><li><a href="/dev/python/machine-leaning/docs/machine-fancy-index/machine-fancy-index.html" class="sidebar-link">Numpy中的Fancy Indexing</a></li><li><a href="/dev/python/machine-leaning/docs/machine-matplotlib/machine-matplotlib.html" class="sidebar-link">Matplotlib模块</a></li><li><a href="/dev/python/machine-leaning/docs/machine-read-data/machine-read-data.html" class="sidebar-link">读取数据和简单的数据探索</a></li><li><a href="/dev/python/machine-leaning/docs/machine-k/machine-k.html" class="sidebar-link">K-Nearest Neighbors</a></li><li><a href="/dev/python/machine-leaning/docs/machine-k-p/machine-k-p.html" class="sidebar-link">判断机器学习kNN算法的性能和精确度</a></li><li><a href="/dev/python/machine-leaning/docs/machine-k-params/machine-k-params.html" class="sidebar-link">K-Nearest Neighbors 中超参数的问题</a></li><li><a href="/dev/python/machine-leaning/docs/machine-feature-scaling/machine-feature-scaling.html" class="sidebar-link">数据归一化 Feature Scaling</a></li><li><a href="/dev/python/machine-leaning/docs/machine-k-summary/machine-k-summary.html" class="sidebar-link">关于K近邻算法的总结</a></li><li><a href="/dev/python/machine-leaning/docs/machine-leaner/machine-leaner.html" class="sidebar-link">简单线性回归算法</a></li><li><a href="/dev/python/machine-leaning/docs/machine-zg/machine-zg.html" class="sidebar-link">正规方程求解多元线性回归问题</a></li><li><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html" aria-current="page" class="active sidebar-link">梯度下降算法解决多元线性回归问题</a></li><li><a href="/dev/python/machine-leaning/docs/machine-principal/machine-principal.html" class="sidebar-link">梯度下降算法解决多元线性回归问题</a></li><li><a href="/dev/python/machine-leaning/docs/machine-hg/machine-hg.html" class="sidebar-link">多项式回归</a></li><li><a href="/dev/python/machine-leaning/docs/machine-logic/machine-logic.html" class="sidebar-link">逻辑回归 Logistic Regression</a></li><li><a href="/dev/python/machine-leaning/docs/machine-category/machine-category.html" class="sidebar-link">分类结果的评价策略</a></li><li><a href="/dev/python/machine-leaning/docs/machine-svm/machine-svm.html" class="sidebar-link">SVM支持向量机</a></li><li><a href="/dev/python/machine-leaning/docs/machine-decision/machine-decision.html" class="sidebar-link">决策树</a></li><li><a href="/dev/python/machine-leaning/docs/machine-jc/machine-jc.html" class="sidebar-link">集成学习和随机森林</a></li></ul> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-25ba6db2 data-v-1aefc0b4><h3 class="title" data-v-25ba6db2 data-v-25ba6db2>梯度下降算法解决多元线性回归问题</h3> <!----> <label id="box" class="inputBox" data-v-25ba6db2 data-v-25ba6db2><input type="password" value="" data-v-25ba6db2> <span data-v-25ba6db2>Konck! Knock!</span> <button data-v-25ba6db2>OK</button></label> <div class="footer" data-v-25ba6db2 data-v-25ba6db2><span data-v-25ba6db2><i class="iconfont reco-theme" data-v-25ba6db2></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-25ba6db2>vuePress-theme-reco</a></span> <span data-v-25ba6db2><i class="iconfont reco-copyright" data-v-25ba6db2></i> <a data-v-25ba6db2><span data-v-25ba6db2>尹欢一的技术博客</span>
            
          <span data-v-25ba6db2>2016 - </span>
          2023
        </a></span></div></div> <div data-v-1aefc0b4><main class="page"><section><div class="page-title"><h1 class="title">梯度下降算法解决多元线性回归问题</h1> <div data-v-f875f3fc><i class="iconfont reco-account" data-v-f875f3fc><span data-v-f875f3fc>尹欢一的技术博客</span></i> <i class="iconfont reco-date" data-v-f875f3fc><span data-v-f875f3fc>2017/5/1</span></i> <!----> <!----></div></div> <div class="theme-reco-content content__default"><h3 id="一-梯度下降算法"><a href="#一-梯度下降算法" class="header-anchor">#</a> <code>(一)梯度下降算法</code></h3> <ul><li>梯度下降算法概念</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>①：它不是一个机器学习算法
②：是一种基于搜索的最优化方法
③：作用：最小化一个损失函数
④：梯度上升法：最大化一个效用函数
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>定义一个损失函数</li></ul> <p><img src="/assets/img/2019-06-222.44.18.af6d5137.png" alt="Alt text"></p> <p><img src="/assets/img/2019-06-222.52.14.e6d4bf71.png" alt="Alt text"></p> <ul><li>那么θ值递降的速度决定于η的大小和dJ的大小，计算公式如下：左边的θ就是每次迭代递降后的新θ值</li></ul> <p>$$θ = θ - η * dJ$$</p> <h3 id="二-梯度下降算法解决一元二次方程最小值"><a href="#二-梯度下降算法解决一元二次方程最小值" class="header-anchor">#</a> <code>(二)梯度下降算法解决一元二次方程最小值</code></h3> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np 
import matplotlib.pyplot as plt

# 初始化一个140个等间距元素的向量
plot_x = np.linspace(-1, 6, 141)
# 一元二次方程
plot_y = (plot_x - 2.5)**2 - 1

# 绘图
plt.plot(plot_x, plot_y)
plt.show()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><img src="/assets/img/des.dcf6b2ca.png" alt="Alt text"></p> <div class="language- line-numbers-mode"><pre class="language-text"><code># 对二次方程的theta求导, plot_x就是theta
def dJ(theta):
    return 2*(theta - 2.5)

# 损失函数值
def J(theta):
    return (theta-2.5)**2 -1

# 设置theta, eta, epsilon初始值
theta = 0
eta=0.1
epsilon = 1e-8

while True:
    # 获取斜率
    gradient = dJ(theta)
    # 保留之前一次的theta值
    last_theta = theta
    # 获取新的theta值
    theta = theta - eta*gradient
    # 这是循环退出的条件，比较最近两次损失函数的值小于给定的epsilon，就退出循环
    if abs(J(theta) - J(last_theta)) &lt; epsilon:
        break

print(theta)
print(J(theta))
输出结果：
2.499891109642585
-0.99999998814289
# 小结：当theta = 2.499891109642585时候，损失值最小
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><ul><li>画图描述</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>theta = 0
eta=0.1
epsilon = 1e-8

# 记录theta的取值过程
theta_history = [theta]

while True:
    # 获取斜率
    gradient = dJ(theta)  # -5
    # 保留之前一次的theta值
    last_theta = theta  # 0
    # 获取新的theta值
    theta = theta - eta*gradient # 0.5 gradient是负数，减去一个负数得正数
    theta_history.append(theta)
    
    # 这是循环退出的条件，比较最近两次损失函数的值小于给定的epsilon，就退出循环
    if abs(J(theta) - J(last_theta)) &lt; epsilon:
        break

# 看看theta_history每次的取值走向图

# 先画出原一元二次方程图
plt.plot(plot_x, J(plot_x), color='g')
# 再通过theta_history的取值，查看对应的J损失函数的值
plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='x')
plt.show()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><p><img src="/assets/img/sd.0f5f9041.png" alt="Alt text"></p> <div class="language- line-numbers-mode"><pre class="language-text"><code># 查看theta取了多少次
len(theta_history)
输出结果：
46
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>将代码进行封装</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 将上面的计算封装为函数
# 给梯度下降设置梯度下降的次数
def gradient_descent(initial_theta, eta, n_iters=1e4,epsilon=1e-8):

    theta = initial_theta
    theta_history.append(initial_theta)
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta*gradient
        theta_history.append(theta)
    
	    # 当y的差值足够小了就认为是到了极致点，类似于斜率为0的点
        if abs(J(theta) - J(last_theta)) &lt; epsilon:
            break
        
        i_iter += 1
        
        
def plot_theta_history():
    plt.plot(plot_x, J(plot_x), color='g')
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='x')
    plt.show()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><ul><li>修改eta的值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>eta = 0.01
initial_theta = 0
theta_history = []

gradient_descent(initial_theta, eta)
plot_theta_history()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p><img src="/assets/img/er.c93beb60.png" alt="Alt text"></p> <ul><li>查看theta_history记录的值的长度</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>len(theta_history)
输出结果：
424
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>给损失函数添加异常捕获</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>def J(theta):
    try:
        return (theta-2.5) ** 2 -1
    except Exception as e:
        return float('inf')
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code># 即使eta是一个比较大的值，也不会进入死循环， 因为有异常捕获
eta = 1.1
initial_theta = 0
theta_history = []

gradient_descent(initial_theta, eta)
len(theta_history)
输出结果：
10001

# 获取theta_history最后一个值, 一定是nan， 这是无穷大-无穷大
theta_history[-1]
输出结果：
nan
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><h3 id="二-多元线性回归中使用梯度下降法"><a href="#二-多元线性回归中使用梯度下降法" class="header-anchor">#</a> <code>（二）多元线性回归中使用梯度下降法</code></h3> <ul><li>问题转换为：</li></ul> <p><img src="/assets/img/2019-06-225.57.59.911cd3ba.png" alt="Alt text"></p> <ul><li><p>此时 : J(θ) = $\sum\limits_{i=1}^m (y^i - θ_0X_0^i$ + $θ_1X_1^i$ + $θ_2X_2^i$ + $θ_3X_3^i$ + ... + $θ_nX_n^i)^2 $</p></li> <li><p><strong>对J(θ)求θ的偏导数</strong></p></li></ul> <p><img src="/assets/img/2019-06-226.24.13.9c603ca1.png" alt="Alt text"></p> <ul><li>但是这里有个问题，当m的值越大，那么▽J(θ)中元素的值也就越大，这不符合实际需要，因此将损失函数除以m</li></ul> <p><img src="/assets/img/2019-06-226.27.36.0d70fdd4.png" alt="Alt text"></p> <ul><li>那么问题再次转换为下面这个式子尽可能小</li></ul> <p><img src="/assets/img/2019-06-226.31.29.2bb90c22.png" alt="Alt text"></p> <ul><li>模拟一组训练数据，让损失函数的值最小（这里的X矩阵和y真值是训练数据, 3是斜率，4为截距， 为了模拟，线性回归方程末尾添加了正态分布噪音）</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = 2 * np.random.random(size=100)
# 这里加了一个正态分布的噪音
y = x * 3. + 4. + np.random.normal(size=100)
X = x.reshape(-1, 1)
print(X.shape)
print(y.shape)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><ul><li>画出散点图</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>plt.scatter(x, y)
plt.show()
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><img src="data:image/png;base64,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" alt="Alt text"></p> <ul><li>梯度下降核心思想：找到合适的θ向量，使得J(θ)尽可能的小，也就是找到合适的θ向量，使得▽J(θ) = 0，但是斜率等于0基本上很难找到，于是将斜率等于0，转换为最后一次求得的J(θ)与上一次J(θ)的差值尽可能小，即：$J(θ) - J_l(θ)$ ， 那么这个θ向量就是被找到的向量，θ[0]元素就是截距，θ[1: ]就是系数</li></ul> <p><img src="/assets/img/2019-06-226.27.36.0d70fdd4.png" alt="Alt text"></p> <ul><li>计算<strong>损失函数</strong>、<strong>损失函数倒数</strong>、<strong>梯度下降函数</strong></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 计算损失函数
# theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / X_b.shape[0]
    except Exception as e:
        return float('inf')

# theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量
def dJ(theta, X_b, y):
    res = np.empty(len(theta))
    # 这是对J求偏导后第一个元素
    # 第一个元素有些特殊，得到的是一个向量，
    # 这个向量应该与1的列向量做点积运算， 也可以将向量元素求和，这里选择求和
    res[0] = np.sum(X_b.dot(theta) -y)
    for i in range(1, len(theta)):
        # 从第二项开始，前面的累加符号由于点积运算不需要理会
        res[i] = (X_b.dot(theta) -y).dot(X_b[:, i])
    return res * 2 / X_b.shape[0]

# 这里的initial_theta是一个向量，不在是一个值了, 函数最终返回theta向量
def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
    
    theta = initial_theta
    # theta_history.append(initial_theta)
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta*gradient
        # theta_history.append(theta)
    
        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:
            break
        
        i_iter += 1
    return theta
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><ul><li>调用函数，计算θ</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 拼凑出X_b矩阵, 由于训练的X数据集是只有一列的矩阵，因此合并后是两列的矩阵
X_b = np.hstack([
    np.ones((X.shape[0], 1)), 
    X])

# 让初始化的theta值从0开始, initial_theta只有两个元素
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01

theta = gradient_descent(X_b, y, initial_theta, eta)
theta
输出结果：
array([4.02145786, 3.00706277])
# 小结：预测结果与期望结果类似，斜率为3，截距为4
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><ul><li>修改之前的linearRegression.py模块的LinearRegression类, 添加梯度下降的拟合方法</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np
from metrics import r2_score

class LinearRegression:
    def __init__(self):
        # 这是系数
        self.coef_ = None
        # 这个是截距
        self.interception_ = None
        # 这个就是具体计算出来的θ列向量
        self._theta = None

    # 这里的拟合是使用正规方程求解
    def fit_normal(self, X_train, y_train):
        assert X_train.shape[0] == y_train.shape[0], \
        '训练数据集必须与预测训练一个维度'

        # 构建特征矩阵， np.ones((X_train.shape[0], 1)), 为元素都为1的矩阵，且只有1列
        X_b = np.hstack([
                         np.ones((X_train.shape[0], 1)),
                         X_train,
                        ])

        # 使用θ表达式，计算出θ的值
        self._theta = np.linalg.inv(X_b.T.dot(X_b))\
                      .dot(X_b.T)\
                      .dot(y_train)

        # 那么截距就是第一个元素
        self.interception_ = self._theta[0]
        # 系数就是后面的元素
        self.coef_ = self._theta[1:]

        return self

    # 这里的拟合使用梯度下降方法
    def fit_gd(self, X_train, y_train, eta=0.01, n_iters=1e4):
        assert X_train.shape[0] == y_train.shape[0], \
        '必须同一维度'

        # 计算损失函数
        # theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量
        def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta)) ** 2) / X_b.shape[0]
            except Exception as e:
                return float('inf')

        # theta是系数向量，X_b是加了一列1的特征矩阵，y是真值向量
        def dJ(theta, X_b, y):
            res = np.empty(len(theta))
            # 这是对J求偏导后第一个元素
            # 第一个元素有些特殊，得到的是一个向量，
            # 这个向量应该与1的列向量做点积运算， 也可以将向量元素求和，这里选择求和
            res[0] = np.sum(X_b.dot(theta) - y)
            for i in range(1, len(theta)):
                # 从第二项开始，前面的累加符号由于点积运算不需要理会
                res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
            return res * 2 / X_b.shape[0]

        # 这里的initial_theta是一个向量，不在是一个值了, 函数最终返回theta向量
        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

            theta = initial_theta
            i_iter = 0
            while i_iter &lt; n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient

                if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:
                    break

                i_iter += 1
            return theta

        X_b = np.hstack([np.ones((X_train.shape[0], 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])

        # 最终拿到theta， theta中包括截距和系数
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters=n_iters, epsilon=1e-8)

        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self

    # 预测
    def predict(self, X_predict):

        assert self.interception_ is not None and self.coef_ is not None, \
        '必须先调用fit_normal方法'

        assert X_predict.shape[1] == len(self.coef_), \
        '被预测矩阵的列数必须与系数向量的长度相等'

        # 通过传递进来的X_predict计算新的矩阵X_b
        X_b = np.hstack([
            np.ones((X_predict.shape[0], 1)),
            X_predict,
        ])

        return X_b.dot(self._theta)

    # 使用R Squared 评测算法，评测算法的准确度
    def score(self, X_test, y_test):
        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __str__(self):
        return &quot;LinearRegression(coef_={}, &quot; \
               &quot;interception_={}, &quot; \
               &quot;_theta={})&quot;.format(self.coef_, self.interception_, self._theta)
    
    __repr__ = __str__

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br><span class="line-number">110</span><br><span class="line-number">111</span><br><span class="line-number">112</span><br><span class="line-number">113</span><br><span class="line-number">114</span><br><span class="line-number">115</span><br></div></div><ul><li>调用自实现类，计算θ</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>from linearRegression import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit_gd(X_train=X, y_train=y, eta=0.01, n_iters=1e4)
输出结果：
LinearRegression(coef_=[3.00706277], interception_=4.021457858204859, _theta=[4.02145786 3.00706277])

小结：同样的系数是3.00706277， 截距是4.021457858204859，符合预期结果
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><ul><li>总结：目前学习了两种算法，处理损失函数的最小值
<ul><li>①：正规方程求解，求解出θ的表达式(一般很难求解得到)</li> <li>②：梯度下降，找到损失函数J(θ)的极值点对应的θ，就是预期的θ，从而获得损失函数J(θ)的系数和截距，最终通过求得的θ向量，得到多元线性回归方程来预测趋势数据</li></ul></li></ul> <h3 id="二-线性回归使用梯度下降法向量化训练真实数据集"><a href="#二-线性回归使用梯度下降法向量化训练真实数据集" class="header-anchor">#</a> <code>(二)线性回归使用梯度下降法向量化训练真实数据集</code></h3> <ul><li><p>之前的▽J(θ)是一项一项通过循环的方式将列向量每一行元素计算出来求解的，速度比较慢，下面通过矩阵运算的方式进行优化</p></li> <li><p>将▽J(θ)进行化简变形 $X_0^i$表示第i行第0列的值为1， 由于需要拼凑成X_b，第0列的元素都是1</p></li></ul> <p><img src="/assets/img/has.1cb08fce.png" alt="Alt text"></p> <p><img src="/assets/img/hs12.7a0467b3.png" alt="Alt text"></p> <p><img src="/assets/img/zz.c8f4dac4.png" alt="Alt text"></p> <ul><li>最后将梯度的表达式化简为 矩阵运算，结果如下</li></ul> <p><img src="/assets/img/as.6898b398.png" alt="Alt text"></p> <div class="language- line-numbers-mode"><pre class="language-text"><code># 在
# 修改梯度导数函数
def dJ_matrix(theta, X_b, y):
    return 2. / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li><strong>引入scikit-learn的波士顿房价真实数据进行算法测评(第一个是正规方程求解的方式)</strong></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np 
import matplotlib.pyplot as plt
from sklearn import datasets

# 加载波士顿房产数据
boston = datasets.load_boston()

X = boston.data
y = boston.target

# 这里使用的是fancing indexing的方式过滤数据
X = X[y &lt; 50.0]
y = y[y &lt; 50.0]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

from linearRegression import LinearRegression
lin_reg = LinearRegression()
# 使用线性回归方程的方式求解θ
%time lin_reg.fit_normal(X_train, y_train)
lin_reg.score(X_test, y_test)

输出结果：
CPU times: user 3.25 ms, sys: 492 µs, total: 3.74 ms
Wall time: 2.74 ms
0.8009390227580956
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br></div></div><ul><li>使用梯度下降的方式(▽J(θ)计算已经通过矩阵计算的方式进行了优化)</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>lin_reg2 = LinearRegression()
# 使用梯度下降的方式求解θ, eta的取值要非常小，保证移动的步长是收敛的
%time lin_reg.fit_gd(X_train, y_train, eta=0.0000001, n_iters=1e6)
lin_reg.score(X_test, y_test)
输出结果：
CPU times: user 1min 6s, sys: 403 ms, total: 1min 7s
Wall time: 34.5 s
0.5116716158630092
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><ul><li>小结： <strong>上面的运行结果表明，eta、n_iters这两个值都需要调整，因为这两个值决定了测试结果的准确度，但是，这两个值一般都是事先指定好的，解决的方法可以使用数据归一化，将特征数据和真值数据都统一到同一个维度</strong></li></ul> <p><img src="/assets/img/sdf.e7dcabba.png" alt="Alt text"></p> <ul><li>将特征数据X矩阵和真值数据y列向量归一化处理</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 将初始值进行数据归一化处理
from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
# 调用fit方法，目的是先计算出均值和方差
standardScaler.fit(X_train)
# 通过fit方法计算出来的举止和方差将X_train、X_test进行方差均值归一化
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)

lin_reg3 = LinearRegression()
%time lin_reg3.fit_gd(X_train_standard, y_train)

输出结果：
CPU times: user 317 ms, sys: 3.19 ms, total: 320 ms
Wall time: 167 ms

lin_reg3.score(X_test_standard, y_test)
输出结果：
0.800927010538664
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><ul><li>小结：通过上述结果可以得知：正规方程的运算准确度与梯度下降的运算准确度一样, 梯度下降法寻找最优的θ向量的时间比正规矩阵的方法耗时更加长， 但是这里是有原因的, 看下面这个例子</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 初始化样本数
m = 1000
# 初始化特征数
n = 5000
# 初始化符合正态分布的X特征矩阵
big_X = np.random.normal(size=(m, n))
# 从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high
true_theta = np.random.uniform(0.0, 100.0, size=n+1)
# 生成y真值,生成的公式就是y = X*θ + θ[0] + 噪音(m ✖️1的矩阵)
big_y = big_X.dot(true_theta[1:]) + true_theta[0] + np.random.normal(0.0,10.0,size=m)
# 使用正规矩阵的方式，训练的结果耗时6s
big_reg1 = LinearRegression()
%time big_reg1.fit_normal(big_X, big_y)
输出结果：
CPU times: user 11 s, sys: 396 ms, total: 11.4 s
Wall time: 5.86 s
# 使用梯度下降的方式耗时
big_reg2 = LinearRegression()
%time big_reg2.fit_gd(big_X, big_y)
输出结果：
CPU times: user 3.86 s, sys: 32.4 ms, total: 3.9 s
Wall time: 2.14 s
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><ul><li>说明：此时，梯度下降算法的时间比正规矩阵算法的时间少， 因为正规方程处理的是一个m x n 的矩阵运算，而梯度下降是根据样本数量规模影响时间的， 这种梯度下降法是批量梯度下降法，那么当样本量大的化可以使用随机梯度下降法来优化</li></ul> <p><img src="/assets/img/ass.3f6ddbaf.png" alt="Alt text"></p> <ul><li>这种批量梯度下降法随着m样本数越大，计算的时间就长</li></ul> <h3 id="三-随机梯度下降法"><a href="#三-随机梯度下降法" class="header-anchor">#</a> <code>(三)随机梯度下降法</code></h3> <ul><li>随机梯度下降法就是将▽J(θ)向量只取一个样本，不再要前面的求和，例如i=3
<ul><li>这种极小值搜索的曲线将不是沿着一个方向下降，可能会因为随机的不同，递降的方向发生变化，为了避免递降的速度在初始的时候更快，η的去取值会被a、b常数调整。这就是模拟退火的思想</li></ul></li></ul> <p><img src="/assets/img/asdf.5a10747a.png" alt="Alt text"></p> <ul><li>随机梯度下降法从公式本身来看，就是对▽J(θ)进行了改造，批量梯度下降法是考虑了特征矩阵的所有行，在随机梯度下降法中只考虑一行即可，因此没有了前面的求和符号和除以m</li></ul> <p><img src="/assets/img/aaa.f62f8335.png" alt="Alt text"></p> <p><img src="/assets/img/sdd.4d5fa1bc.png" alt="Alt text"></p> <ul><li><p>举例：一个一维特征矩阵的线性回归方程，通过X_train、y_train进行训练，计算出θ，并且比较批量梯度下降方法与随机梯度下降方法的耗时时长</p></li> <li><p><strong>①：初始化特征矩阵</strong></p></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 特征矩阵X， 真值y
import numpy as np 
import matplotlib as plt

# 初始化样本个数
m = 10000
# 初始化样本特征个数，这里特征个数为1, 且保证了特征数据为正态分布的数据
x = np.random.normal(size=m)
X = x.reshape(-1, 1)
# 这里模拟的是一个特征的线性回归方程, 注意，这里是小x，如果是X矩阵，那么y的shape就是10000 x 10000
y = 4. * x + 3. + np.random.normal(0, 3, size=m)
print(X.shape)
print(y.shape)
输出结果：
(10000, 1)
(10000,)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><ul><li>②：使用批量梯度下降方法计算θ值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / X_b.shape[0]
    except Exception as e:
        return float('inf')

def dJ_matrix(theta, X_b, y):
    return 2. / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)
    
# 这里的initial_theta是一个向量，不在是一个值了, 函数最终返回theta向量
def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    
    while i_iter &lt; n_iters:
        gradient = dJ_matrix(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:
            break
        i_iter += 1
    return theta
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><ul><li>③：使用批量梯度下降方法计算θ值且获取耗时时长</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>%%time
X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01
theta = gradient_descent(X_b, y, initial_theta, eta)
print(theta)
输出结果：
[3.04923175 3.98437447]
耗时：103ms
CPU times: user 170 ms, sys: 4.44 ms, total: 175 ms
Wall time: 103 ms
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><ul><li>④：使用随机梯度下降法计算θ值且获取耗时时长(由于在随机梯度下降法中，判断循环退出的条件，不在按照批量梯度下降方法计算J(θ)的差值，判断循环退出的条件只由n_iters循环迭代次数决定</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 这里传递进来的特征矩阵，不再是整个的X_b，而是X_b中的一行特征，相当于一个特征行向量
# y不再是列向量，而是一个值
def dJ_sgd(theta, X_b_i, y_i):
    # 根据随机梯度下降的公式可知
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.

# 计算θ值
# 
def stochastic_gradient_descent(X_b, y, initial_theta, n_iters):
    t0 = 5
    t1 = 50
    
    # 这个就是退火的学习率
    def learning_rate(t):
        return t0 / (t + t1)
    
    theta = initial_theta
    
    # 循环获取迭代次数, 这里每次迭代就是为了计算学习率,这里看看是否可以改为while循环
    for cur_iter in range(n_iters):
        # 获取一个数据样本的索引，这里是在样本总长度中随机选择一个样本
        rand_i = np.random.randint(len(X_b))
        # 获取梯度值
        gradient = dJ_sgd(theta, X_b[rand_i], y[rand_i])
        # 获取最新的θ值，这里最新的θ值计算的公式为θ沿导数递降的变化率
        theta = theta - learning_rate(cur_iter) * gradient
        return theta
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div><ul><li>在pycharm中封装自己的sgd随机梯度下降算法</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>    def fit_sgd(self, X_train, y_train, n_iters, t0=5, t1=50):
        assert X_train.shape[0] == y_train.shape[0], \
            '样本数必须等于真值的列向量行数'

        assert n_iters &gt;= 1, \
            '迭代所有样本的次数必须大于1'

        def dJ_sgd(theta, X_b_i, y_i):
            # 这里去掉了转置和点乘
            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.

        # 计算θ值
        # 这里的n_iters的意义发生了变化，n_iters指的是在循环迭代过程中，需要把所有的样本计算n_iters遍
        def stochastic_gradient_descent(X_b, y, initial_theta, n_iters, t0=5, t1=50):

            # 这个就是退火的学习率
            def learning_rate(t):
                return t0 / (t + t1)

            theta = initial_theta
            # 获取样本数
            m = len(X_b)

            # 循环获取迭代次数, 迭代次数为样本的n_iters倍
            for cur_iter in range(n_iters):
                # 获取随机排列的索引
                indexes = np.random.permutation(m)
                # 让之前的X_b、y_new的所有样本随机排列,
                X_b_new = X_b[indexes]
                y_new = y[indexes]

                # 对每一个样本都进行遍历
                for i in range(m):
                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])
                    theta = theta - learning_rate(cur_iter * m + i) * gradient

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        # 使用随机梯度下降计算theta,
        self._theta = stochastic_gradient_descent(X_b, y_train, initial_theta, n_iters=n_iters)
        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br></div></div><ul><li>使用自己的SGD计算θ值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 模拟数据
import numpy as np 
import matplotlib as plt

# 初始化样本个数
m = 100000
# 初始化样本特征个数，这里特征个数为1, 且保证了特征数据为正态分布的数据
x = np.random.normal(size=m)
X = x.reshape(-1, 1)
# 这里模拟的是一个特征的线性回归方程, 注意，这里是小x，如果是X矩阵，那么y的shape就是10000 x 10000
y = 4. * x + 3. + np.random.normal(0, 3, size=m)
print(X.shape)
print(y.shape)
输出结果：
(100000, 1)
(100000,)

from linearRegression import LinearRegression

lin_reg4 = LinearRegression()
lin_reg4.fit_sgd(X, y, n_iters=10)
输出结果：
LinearRegression(coef_=[3.99974133], interception_=3.007942096244411, _theta=[3.0079421  3.99974133])

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><ul><li>使用波士顿房价数据验证算法的准确度</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np 
import matplotlib.pyplot as plt
from sklearn import datasets

# 加载波士顿房产数据
boston = datasets.load_boston()

X = boston.data
y = boston.target

# 这里使用的是fancing indexing的方式过滤数据
X = X[y &lt; 50.0]
y = y[y &lt; 50.0]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# 将初始值进行数据归一化处理
from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X_train)
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)

from linearRegression import LinearRegression
lin_reg = LinearRegression()

%time lin_reg.fit_sgd(X_train_standard, y_train, n_iters=1000)
lin_reg.score(X_test_standard, y_test)
输出结果：
CPU times: user 1.97 s, sys: 11.3 ms, total: 1.99 s
Wall time: 2.02 s
0.8009992300659649
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br></div></div><ul><li>使用scikit-learn中的SGD随机梯度下降方法验证准确度</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 这个类只能解决线性模型
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(n_iter=1000)
%time sgd_reg.fit(X_train_standard, y_train)
sgd_reg.score(X_test_standard, y_test)
输出结果：
CPU times: user 35.8 ms, sys: 1.54 ms, total: 37.3 ms
Wall time: 36.5 ms
0.8010109472897677
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><ul><li>小结：从自实现的算法和scikit-learn算法对比，scikit-learn耗时少</li></ul> <h3 id="四-梯度下降法的调试方法"><a href="#四-梯度下降法的调试方法" class="header-anchor">#</a> <code>(四)梯度下降法的调试方法</code></h3> <ul><li><p>对于推导出来的算法公式，如何证明是正确且准确的呢？这就需要梯度下降的调试方法了</p></li> <li><p>当θ是一个值的时候，此时是一维的场景， 那么在θ这点的导数就两个蓝色点的斜率</p></li></ul> <p><img src="/assets/img/2019-06-254.56.56.6f95b167.png" alt="Alt text"></p> <ul><li>当θ是一个向量的时候，此时是二维的场景， 那么在θ这点的导数就等于对各个分量求导</li></ul> <p><img src="/assets/img/2019-06-255.03.37.1f60fa24.png" alt="Alt text"></p> <ul><li>那么对$θ_0$这个点的导数可以表示如下：</li></ul> <p><img src="/assets/img/2019-06-255.04.03.7a7310a4.png" alt="Alt text"></p> <ul><li>那么对$θ_1$这个点的导数可以表示如下：</li></ul> <p><img src="/assets/img/2019-06-255.06.41.55135767.png" alt="Alt text"></p> <ul><li><strong>线性回归方程初始化</strong></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
# 特征矩阵
X = np.random.random(size=(1000, 10))
# theta值， 11个元素
true_theta = np.arange(1,12,dtype=float)
# 拼凑出X_b, 11列
X_b = np.hstack([np.ones((len(X), 1)), X])
# 这个根据数学上的点乘计算，true_theta为列向量，
# 左边一个矩阵.列向量=列向量， 再加上一个列向量，还是等于列向量
y = X_b.dot(true_theta) + np.random.normal(size=1000)
print(X.shape)
print(y.shape)
print(true_theta)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><ul><li>准备损失函数、数学梯度函数、dubug梯度函数、梯度下降函数</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code># 损失函数
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / X_b.shape[0]
    except Exception as e:
        return float('inf')

# 损失函数导数，数学推导出来的
def dJ_matrix(theta, X_b, y):
    return 2. / len(X_b) * X_b.T.dot(X_b.dot(theta) - y)

# 根据之前的公式，求出debug模式下的导数
def dJ_debug(theta, X_b, y, epsilon=0.01):
    # 初始化一个与theta等长度的向量
    res = np.empty(len(theta))
    for i in range(len(theta)):
        theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        # 计算每个维度的偏导数
        res[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y)) / (2 * epsilon)
    return res

# 这里多加了一个参数，dJ_matrix，这个函数定义了如何求解梯度
def gradient_descent(dJ_matrix, X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ_matrix(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon:
            break
        i_iter += 1
    return theta     
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><ul><li>准备X_b、initial_theta、eta</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li>使用debug方式计算θ值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>%time theta = gradient_descent(dJ_debug, X_b, y, initial_theta, eta)
print(theta)
输出结果：
CPU times: user 7.32 s, sys: 34.1 ms, total: 7.36 s
Wall time: 3.8 s
[ 1.1251597   2.05312521  2.91522497  4.11895968  5.05002117  5.90494046
  6.97383745  8.00088367  8.86213468  9.98608331 10.90529198]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><ul><li>使用数学推导的方式计算θ值</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>%time theta = gradient_descent(dJ_matrix, X_b, y, initial_theta, eta)
print(theta)
输出结果：
CPU times: user 918 ms, sys: 3.64 ms, total: 922 ms
Wall time: 479 ms
[ 1.1251597   2.05312521  2.91522497  4.11895968  5.05002117  5.90494046
  6.97383745  8.00088367  8.86213468  9.98608331 10.90529198]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><ul><li>小结</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>	这个就是使用了使用debug方式计算θ值与数学推导出来的公式进行对比，最后两者的结果一样，表明数学推导正确, dJ_matrix函数只适用线性的梯度下降问题求解梯度, 这个dJ_debug函数，可以用到所有的梯度下降方法求解梯度的运算中， 可以将dJ_debug函数加入到机器学习的工具集中
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h3 id="五-梯度下降方法总结"><a href="#五-梯度下降方法总结" class="header-anchor">#</a> <code>(五)梯度下降方法总结</code></h3> <ul><li>分类</li></ul> <p><img src="/assets/img/2019-06-259.25.50.04916a2c.png" alt="Alt text"></p> <div class="language- line-numbers-mode"><pre class="language-text"><code>1：批量梯度下降法：稳定，但是速度慢
2：随机批量下降法：在批量梯度下降法的基础上，计算1行样本，速度快，但是不稳定
3：小批量梯度下降法：在批量梯度下降法的基础上，计算k行样本
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><ul><li><strong>自己实现小批量梯度下降法</strong></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>待实现内容
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>梯度下降法</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>不是机器学习的一种算法，而是一种解决搜索中的最优化的搜索方法
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>梯度上升方法</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>前讲的都是对损失函数求最小值，但是有时候也有对效用函数求最大值，那么就需要使用梯度上升方法
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">2022/3/12 下午5:47:29</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev"><a href="/dev/python/machine-leaning/docs/machine-zg/machine-zg.html" class="prev">
            正规方程求解多元线性回归问题
          </a></span> <span class="next"><a href="/dev/python/machine-leaning/docs/machine-principal/machine-principal.html">
            梯度下降算法解决多元线性回归问题
          </a></span></p></div> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-cb1513f6><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#一-梯度下降算法" class="sidebar-link reco-side-一-梯度下降算法" data-v-cb1513f6>(一)梯度下降算法</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#二-梯度下降算法解决一元二次方程最小值" class="sidebar-link reco-side-二-梯度下降算法解决一元二次方程最小值" data-v-cb1513f6>(二)梯度下降算法解决一元二次方程最小值</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#二-多元线性回归中使用梯度下降法" class="sidebar-link reco-side-二-多元线性回归中使用梯度下降法" data-v-cb1513f6>（二）多元线性回归中使用梯度下降法</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#二-线性回归使用梯度下降法向量化训练真实数据集" class="sidebar-link reco-side-二-线性回归使用梯度下降法向量化训练真实数据集" data-v-cb1513f6>(二)线性回归使用梯度下降法向量化训练真实数据集</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#三-随机梯度下降法" class="sidebar-link reco-side-三-随机梯度下降法" data-v-cb1513f6>(三)随机梯度下降法</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#四-梯度下降法的调试方法" class="sidebar-link reco-side-四-梯度下降法的调试方法" data-v-cb1513f6>(四)梯度下降法的调试方法</a></li><li class="level-3" data-v-cb1513f6><a href="/dev/python/machine-leaning/docs/machine-td/machine-td.html#五-梯度下降方法总结" class="sidebar-link reco-side-五-梯度下降方法总结" data-v-cb1513f6>(五)梯度下降方法总结</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/assets/js/app.9a5d1377.js" defer></script><script src="/assets/js/7.c49e7131.js" defer></script><script src="/assets/js/1.d009776b.js" defer></script><script src="/assets/js/11.2d4145aa.js" defer></script>
  </body>
</html>
